{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"\u5173\u4e8e\u672c\u5de5\u7a0b # \u5b66\u4e60\u7b97\u6cd5\uff0c\u9996\u5148\u4ece\u9012\u5f52\u5f00\u59cb\u3002\u5b66\u4e60\u5e38\u7528\u7684\u7b97\u6cd5\u8bbe\u8ba1paradigm\u3002\u5b66\u4e60\u7b97\u6cd5\u5206\u6790\u3002\u5b66\u4e60\u7b97\u6cd5\u7684\u76ee\u7684\u662f\u9ad8\u6548\u5730\u89e3\u51b3\u95ee\u9898\uff0c\u5728\u672c\u9879\u76ee\u4e2d\uff0c\u5c06\u8fd9\u4e9b\u95ee\u9898\u7edf\u79f0\u4e3a Computational problem \uff0c\u5728\u9879\u76ee\u4e2d\uff0c\u6536\u5f55\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u7684\u9ad8\u6548\u7b97\u6cd5\u3002","title":"Home"},{"location":"#_1","text":"\u5b66\u4e60\u7b97\u6cd5\uff0c\u9996\u5148\u4ece\u9012\u5f52\u5f00\u59cb\u3002\u5b66\u4e60\u5e38\u7528\u7684\u7b97\u6cd5\u8bbe\u8ba1paradigm\u3002\u5b66\u4e60\u7b97\u6cd5\u5206\u6790\u3002\u5b66\u4e60\u7b97\u6cd5\u7684\u76ee\u7684\u662f\u9ad8\u6548\u5730\u89e3\u51b3\u95ee\u9898\uff0c\u5728\u672c\u9879\u76ee\u4e2d\uff0c\u5c06\u8fd9\u4e9b\u95ee\u9898\u7edf\u79f0\u4e3a Computational problem \uff0c\u5728\u9879\u76ee\u4e2d\uff0c\u6536\u5f55\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u7684\u9ad8\u6548\u7b97\u6cd5\u3002","title":"\u5173\u4e8e\u672c\u5de5\u7a0b"},{"location":"Algorithm-and-math/","text":"algorithm and math # \u4f7f\u7528\u6570\u5b66\u6765\u63cf\u8ff0\u7b97\u6cd5\uff0c\u6839\u636e\u6570\u5b66\u516c\u5f0f\u5199\u51fa\u7b97\u6cd5\uff1a \u9012\u63a8\u516c\u5f0f \u73b0\u5728\u9700\u8981\u5bf9\u6bcf\u4e2a\u7b97\u6cd5\u90fd\u5199\u51fa\u5176\u9012\u5f52\u516c\u5f0f\u3002 \u8981\u5199\u51fa\u7b97\u6cd5\u7684\u5f62\u5f0f\u5316\u63cf\u8ff0\uff0c\u8fd9\u662f\u975e\u5e38\u91cd\u8981\u7684\u3002 \u4e66\u5199\u9012\u5f52\u516c\u5f0f\u7684\u51e0\u4e2a\u8981\u7d20\uff1a \u521d\u59cb\u6761\u4ef6 \u9012\u5f52\u5173\u7cfb\uff0c\u9012\u5f52\u5173\u7cfb\u5f80\u5f80\u5bf9\u5e94\u8fd9\u5982\u4e0b\u51e0\u79cd\u60c5\u51b5\uff1a \u65b0\u52a0\u4e00\u4e2a\u5143\u7d20 \u300aalgorithm-DP-VS-\u6700\u957f\u516c\u5171\u5b50\u5e8f\u5217-VS-\u77e9\u9635\u8fde\u4e58\u95ee\u9898-VS-\u6700\u5927\u5b50\u6bb5\u548c.md\u300b \u5728 Knuth\u2013Morris\u2013Pratt algorithm \u4e2d\u5bf9\u7b97\u6cd5\u7684\u63cf\u8ff0\uff1a The most straightforward algorithm, known as the \" Brute-force \" or \"Naive\" algorithm, is to look for a word match at each index m , the position in the string being searched, i.e. S[m] . At each position m the algorithm first checks for equality of the first character in the word being searched, i.e. S[m] =? W[0] . If a match is found, the algorithm tests the other characters in the word being searched by checking successive values of the word position index, i . The algorithm retrieves the character W[i] in the word being searched and checks for equality of the expression S[m+i] =? W[i] . If all successive characters match in W at position m , then a match is found at that position in the search string. If the index m reaches the end of the string then there is no match, in which case the search is said to \"fail\". \u9012\u5f52\u516c\u5f0f # n-Cubes # \u300aDiscrete Mathematics and Its Applications\u300b\u768410.2 Graph Terminology and Special Types of Graphs\u4e2d\u6709\u8fd9\u6837\u7684\u63cf\u8ff0\uff1a Note that you can construct the (n + 1)-cube $Q_{n+1}$ from the n-cube $Q_n$ by making two copies of $Q_n$ , prefacing the labels on the vertices with a 0 in one copy of $Q_n$ and with a 1 in the other copy of $Q_n$ , and adding edges connecting two vertices that have labels differing only in the first bit. KMP failure function # KMP failure function\u7684\u8ba1\u7b97\u4e5f\u6709\u8d56\u4e8e\u9012\u5f52\u5173\u7cfb\u7684\u5efa\u7acb\uff0c\u53c2\u89c1 Computing the KMP failure function (f(k)) \u3002 one by one # \u4e00\u6b21\u5904\u7406\u4e00\u4e2a\u5143\u7d20\uff0c\u8fd9\u662f\u5f88\u591a\u7b97\u6cd5\u7684\u8fd0\u4f5c\u65b9\u5f0f\uff1a parsing Knuth\u2013Morris\u2013Pratt algorithm \u8fd9\u80cc\u540e\u662f\u7b97\u6cd5\u548c\u79bb\u6563\u6570\u5b66\u3002 \u5faa\u73af\u6b21\u6570\u7684\u786e\u5b9a # \u6309\u7167one by one\u7684\u601d\u8def\uff0c\u975e\u5e38\u591a\u7684\u7b97\u6cd5\u6700\u7ec8\u90fd\u80fd\u591f\u5f62\u5f0f\u5316\u6210\u5faa\u73af\u7684\u5904\u7406\uff0c\u53ef\u80fd\u4f1a\u5d4c\u5957\u591a\u5c42\u5faa\u73af\u3002\u5f53\u5c06\u7b97\u6cd5\u5f62\u5f0f\u5316\u4e3a\u5faa\u73af\u540e\u8fdb\u884c\u5b9e\u73b0\u7684\u65f6\u5019\uff0c\u4e00\u4e2a\u975e\u5e38\u91cd\u8981\u7684\u95ee\u9898\u5c31\u662f\u5faa\u73af\u8bed\u53e5\u7684\u5b9e\u73b0\uff1a for \u8bed\u53e5\u4e00\u4e2a\u975e\u5e38\u91cd\u8981\u7684\u5c31\u662f\u5faa\u73af\u6b21\u6570\u7684\u786e\u5b9a\uff0c\u53ea\u6709\u5148\u786e\u5b9a\u597d\u5faa\u73af\u6b21\u6570\u540e\uff0c\u624d\u80fd\u591f\u5199\u51fa\u6b63\u786e\u7684for\u5faa\u73af `while \u8bed\u53e5\u53ef\u80fd\u6d89\u53ca\u5230\u7684\u6709\uff1a \u5faa\u73af\u6b21\u6570\u7684\u786e\u5b9a \u7ec8\u6b62\u6761\u4ef6\u7684\u786e\u5b9a \u4e0b\u9762\u4ee5\u7ed3\u5408\u4e00\u4e9b\u5177\u4f53\u7684\u7b97\u6cd5\u6765\u8fdb\u884c\u8bf4\u660e naive string search # Given a text txt and a pattern pat , prints all occurrences of pat in txt . \u8fd9\u4e2a\u95ee\u9898\u6700\u7ec8\u662f\u53ef\u4ee5\u5f62\u5f0f\u5316\u4e3a\u4e24\u6b21 for \u5faa\u73af\u7684\uff0c\u8981\u60f3\u5199\u51fa\u6b63\u786e\u7684 for \u8bed\u53e5\u7b2c\u4e00\u4e2a\u95ee\u9898\u5c31\u662f\uff1a\u5728 txt \u4e00\u5171\u8981 pat \u5c06\u8fdb\u884c\u591a\u5c11\u6b21\u5339\u914d\uff1f \u5176\u5b9e\u56de\u7b54\u8fd9\u4e2a\u95ee\u9898\u5e76\u4e0d\u662f\u975e\u5e38\u96be\uff0c\u4e00\u4e2a\u975e\u5e38\u7b80\u5355\u7684\u6280\u5de7\u662f\u8fdb\u884c \u679a\u4e3e \uff0c\u6211\u4eec\u53ef\u4ee5\u4ece\u6700\u6700\u7b80\u5355\u7684\u60c5\u51b5\u8fdb\u884c\u679a\u4e3e\uff0c\u8bbe txt \u7684\u957f\u5ea6\u662f txt_len \uff0c pat \u7684\u957f\u5ea6\u662f pat_len \uff1a txt_len \u4e3a2\uff0c pat_len \u4e3a1\uff0c\u663e\u7136\u8981\u8fdb\u884c2\u6b21\u5339\u914d txt_len \u4e3a4\uff0c pat_len \u4e3a2\uff0c\u663e\u7136\u8981\u8fdb\u884c3\u6b21\u5339\u914d txt_len \u4e3a4\uff0c pat_len \u4e3a3\uff0c\u663e\u7136\u8981\u8fdb\u884c2\u6b21\u5339\u914d \u663e\u7136\u8981\u8fdb\u884c txt_len - pat_len + 1 \u6b21\u5339\u914d\uff0c\u6240\u4ee5\u7b2c\u4e00\u5c42 for \u5faa\u73af\u5c31\u9700\u8981\u4ea7\u751f txt_len - pat_len + 1 \u6b21\u5faa\u73af\u3002\u6240\u4ee5\u6700\u7ec8\u6211\u4eec\u53ef\u4ee5\u5199\u51fa\u5982\u4e0b\u4ee3\u7801 def brute_force_search(txt, pat): \"\"\" Given a text txt and a pattern pat, prints all occurrences of pat in txt. https://www.geeksforgeeks.org/naive-algorithm-for-pattern-searching/ :param txt: :param pat: :return: \"\"\" txt_len = len(txt) pat_len = len(pat) for i in range(txt_len - pat_len + 1): for j in range(pat_len): if txt[i + j] != pat[j]: break if j == pat_len - 1: print(txt[i:i + pat_len]) j = 0 \u5efa\u7acb\u9012\u5f52\u5173\u7cfb # KMP\u7b97\u6cd5\u4e5f\u5b58\u5728\u9012\u5f52\u5173\u7cfb\u3002","title":"Algorithm-and-math"},{"location":"Algorithm-and-math/#algorithm_and_math","text":"\u4f7f\u7528\u6570\u5b66\u6765\u63cf\u8ff0\u7b97\u6cd5\uff0c\u6839\u636e\u6570\u5b66\u516c\u5f0f\u5199\u51fa\u7b97\u6cd5\uff1a \u9012\u63a8\u516c\u5f0f \u73b0\u5728\u9700\u8981\u5bf9\u6bcf\u4e2a\u7b97\u6cd5\u90fd\u5199\u51fa\u5176\u9012\u5f52\u516c\u5f0f\u3002 \u8981\u5199\u51fa\u7b97\u6cd5\u7684\u5f62\u5f0f\u5316\u63cf\u8ff0\uff0c\u8fd9\u662f\u975e\u5e38\u91cd\u8981\u7684\u3002 \u4e66\u5199\u9012\u5f52\u516c\u5f0f\u7684\u51e0\u4e2a\u8981\u7d20\uff1a \u521d\u59cb\u6761\u4ef6 \u9012\u5f52\u5173\u7cfb\uff0c\u9012\u5f52\u5173\u7cfb\u5f80\u5f80\u5bf9\u5e94\u8fd9\u5982\u4e0b\u51e0\u79cd\u60c5\u51b5\uff1a \u65b0\u52a0\u4e00\u4e2a\u5143\u7d20 \u300aalgorithm-DP-VS-\u6700\u957f\u516c\u5171\u5b50\u5e8f\u5217-VS-\u77e9\u9635\u8fde\u4e58\u95ee\u9898-VS-\u6700\u5927\u5b50\u6bb5\u548c.md\u300b \u5728 Knuth\u2013Morris\u2013Pratt algorithm \u4e2d\u5bf9\u7b97\u6cd5\u7684\u63cf\u8ff0\uff1a The most straightforward algorithm, known as the \" Brute-force \" or \"Naive\" algorithm, is to look for a word match at each index m , the position in the string being searched, i.e. S[m] . At each position m the algorithm first checks for equality of the first character in the word being searched, i.e. S[m] =? W[0] . If a match is found, the algorithm tests the other characters in the word being searched by checking successive values of the word position index, i . The algorithm retrieves the character W[i] in the word being searched and checks for equality of the expression S[m+i] =? W[i] . If all successive characters match in W at position m , then a match is found at that position in the search string. If the index m reaches the end of the string then there is no match, in which case the search is said to \"fail\".","title":"algorithm and math"},{"location":"Algorithm-and-math/#_1","text":"","title":"\u9012\u5f52\u516c\u5f0f"},{"location":"Algorithm-and-math/#n-cubes","text":"\u300aDiscrete Mathematics and Its Applications\u300b\u768410.2 Graph Terminology and Special Types of Graphs\u4e2d\u6709\u8fd9\u6837\u7684\u63cf\u8ff0\uff1a Note that you can construct the (n + 1)-cube $Q_{n+1}$ from the n-cube $Q_n$ by making two copies of $Q_n$ , prefacing the labels on the vertices with a 0 in one copy of $Q_n$ and with a 1 in the other copy of $Q_n$ , and adding edges connecting two vertices that have labels differing only in the first bit.","title":"n-Cubes"},{"location":"Algorithm-and-math/#kmp_failure_function","text":"KMP failure function\u7684\u8ba1\u7b97\u4e5f\u6709\u8d56\u4e8e\u9012\u5f52\u5173\u7cfb\u7684\u5efa\u7acb\uff0c\u53c2\u89c1 Computing the KMP failure function (f(k)) \u3002","title":"KMP failure function"},{"location":"Algorithm-and-math/#one_by_one","text":"\u4e00\u6b21\u5904\u7406\u4e00\u4e2a\u5143\u7d20\uff0c\u8fd9\u662f\u5f88\u591a\u7b97\u6cd5\u7684\u8fd0\u4f5c\u65b9\u5f0f\uff1a parsing Knuth\u2013Morris\u2013Pratt algorithm \u8fd9\u80cc\u540e\u662f\u7b97\u6cd5\u548c\u79bb\u6563\u6570\u5b66\u3002","title":"one by one"},{"location":"Algorithm-and-math/#_2","text":"\u6309\u7167one by one\u7684\u601d\u8def\uff0c\u975e\u5e38\u591a\u7684\u7b97\u6cd5\u6700\u7ec8\u90fd\u80fd\u591f\u5f62\u5f0f\u5316\u6210\u5faa\u73af\u7684\u5904\u7406\uff0c\u53ef\u80fd\u4f1a\u5d4c\u5957\u591a\u5c42\u5faa\u73af\u3002\u5f53\u5c06\u7b97\u6cd5\u5f62\u5f0f\u5316\u4e3a\u5faa\u73af\u540e\u8fdb\u884c\u5b9e\u73b0\u7684\u65f6\u5019\uff0c\u4e00\u4e2a\u975e\u5e38\u91cd\u8981\u7684\u95ee\u9898\u5c31\u662f\u5faa\u73af\u8bed\u53e5\u7684\u5b9e\u73b0\uff1a for \u8bed\u53e5\u4e00\u4e2a\u975e\u5e38\u91cd\u8981\u7684\u5c31\u662f\u5faa\u73af\u6b21\u6570\u7684\u786e\u5b9a\uff0c\u53ea\u6709\u5148\u786e\u5b9a\u597d\u5faa\u73af\u6b21\u6570\u540e\uff0c\u624d\u80fd\u591f\u5199\u51fa\u6b63\u786e\u7684for\u5faa\u73af `while \u8bed\u53e5\u53ef\u80fd\u6d89\u53ca\u5230\u7684\u6709\uff1a \u5faa\u73af\u6b21\u6570\u7684\u786e\u5b9a \u7ec8\u6b62\u6761\u4ef6\u7684\u786e\u5b9a \u4e0b\u9762\u4ee5\u7ed3\u5408\u4e00\u4e9b\u5177\u4f53\u7684\u7b97\u6cd5\u6765\u8fdb\u884c\u8bf4\u660e","title":"\u5faa\u73af\u6b21\u6570\u7684\u786e\u5b9a"},{"location":"Algorithm-and-math/#naive_string_search","text":"Given a text txt and a pattern pat , prints all occurrences of pat in txt . \u8fd9\u4e2a\u95ee\u9898\u6700\u7ec8\u662f\u53ef\u4ee5\u5f62\u5f0f\u5316\u4e3a\u4e24\u6b21 for \u5faa\u73af\u7684\uff0c\u8981\u60f3\u5199\u51fa\u6b63\u786e\u7684 for \u8bed\u53e5\u7b2c\u4e00\u4e2a\u95ee\u9898\u5c31\u662f\uff1a\u5728 txt \u4e00\u5171\u8981 pat \u5c06\u8fdb\u884c\u591a\u5c11\u6b21\u5339\u914d\uff1f \u5176\u5b9e\u56de\u7b54\u8fd9\u4e2a\u95ee\u9898\u5e76\u4e0d\u662f\u975e\u5e38\u96be\uff0c\u4e00\u4e2a\u975e\u5e38\u7b80\u5355\u7684\u6280\u5de7\u662f\u8fdb\u884c \u679a\u4e3e \uff0c\u6211\u4eec\u53ef\u4ee5\u4ece\u6700\u6700\u7b80\u5355\u7684\u60c5\u51b5\u8fdb\u884c\u679a\u4e3e\uff0c\u8bbe txt \u7684\u957f\u5ea6\u662f txt_len \uff0c pat \u7684\u957f\u5ea6\u662f pat_len \uff1a txt_len \u4e3a2\uff0c pat_len \u4e3a1\uff0c\u663e\u7136\u8981\u8fdb\u884c2\u6b21\u5339\u914d txt_len \u4e3a4\uff0c pat_len \u4e3a2\uff0c\u663e\u7136\u8981\u8fdb\u884c3\u6b21\u5339\u914d txt_len \u4e3a4\uff0c pat_len \u4e3a3\uff0c\u663e\u7136\u8981\u8fdb\u884c2\u6b21\u5339\u914d \u663e\u7136\u8981\u8fdb\u884c txt_len - pat_len + 1 \u6b21\u5339\u914d\uff0c\u6240\u4ee5\u7b2c\u4e00\u5c42 for \u5faa\u73af\u5c31\u9700\u8981\u4ea7\u751f txt_len - pat_len + 1 \u6b21\u5faa\u73af\u3002\u6240\u4ee5\u6700\u7ec8\u6211\u4eec\u53ef\u4ee5\u5199\u51fa\u5982\u4e0b\u4ee3\u7801 def brute_force_search(txt, pat): \"\"\" Given a text txt and a pattern pat, prints all occurrences of pat in txt. https://www.geeksforgeeks.org/naive-algorithm-for-pattern-searching/ :param txt: :param pat: :return: \"\"\" txt_len = len(txt) pat_len = len(pat) for i in range(txt_len - pat_len + 1): for j in range(pat_len): if txt[i + j] != pat[j]: break if j == pat_len - 1: print(txt[i:i + pat_len]) j = 0","title":"naive string search"},{"location":"Algorithm-and-math/#_3","text":"KMP\u7b97\u6cd5\u4e5f\u5b58\u5728\u9012\u5f52\u5173\u7cfb\u3002","title":"\u5efa\u7acb\u9012\u5f52\u5173\u7cfb"},{"location":"TODO/","text":"TODO # 20191227 # \u5f88\u591a\u7b97\u6cd5\u6280\u5de7\u90fd\u662f\u5728\u907f\u514d combinatorial explosion \u95ee\u9898\uff0c\u6bd4\u5982 dynamic programming LL(1) parser\uff0c\u4f7f\u7528\u9884\u6d4b\u7684\u65b9\u5f0f\u6765\u907f\u514d \u90a3\u4e48\u5982\u4f55\u6765\u7406\u89e3 combinatorial explosion \u5462\uff1f\u5f53\u6211\u4eec\u4f7f\u7528 backtracking \u7684\u65f6\u5019\uff0c\u5c31\u4f1a\u51fa\u73b0 combinatorial explosion \uff0c\u4ece\u7b97\u6cd5\u5206\u6790\u7684\u89d2\u5ea6\u6765\u770b\uff0c\u6b64\u65f6\u7684\u7b97\u6cd5\u590d\u6742\u5ea6\u5c31\u4e5f\u4f1aexplosion\u3002 backtracking \u7684\u526a\u679d # \u65b9\u5f0f\u4e00\uff1a # https://leetcode-cn.com/problems/permutations-ii/solution/hui-su-suan-fa-python-dai-ma-java-dai-ma-by-liwe-2/ \u65b9\u5f0f\u4e8c\uff1a\u9884\u6d4b # LL(1) parser\uff0c\u4f7f\u7528\u9884\u6d4b\u7684\u65b9\u5f0f\u6765\u907f\u514d find top k elements in an array # https://www.geeksforgeeks.org/k-largestor-smallest-elements-in-an-array/ https://www.geeksforgeeks.org/find-top-k-or-most-frequent-numbers-in-a-stream/ https://leetcode-cn.com/problems/kth-largest-element-in-an-array/?utm_source=LCUS&utm_medium=ip_redirect_q_uns&utm_campaign=transfer2china https://leetcode-cn.com/problems/top-k-frequent-elements/?utm_source=LCUS&utm_medium=ip_redirect_q_uns&utm_campaign=transfer2china https://stackoverflow.com/questions/4956593/optimal-algorithm-for-returning-top-k-values-from-an-array-of-length-n How to handle duplicates in Binary Search Tree? # https://www.geeksforgeeks.org/how-to-handle-duplicates-in-binary-search-tree/ https://stackoverflow.com/questions/300935/are-duplicate-keys-allowed-in-the-definition-of-binary-search-trees \u62ec\u53f7\u751f\u6210\u95ee\u9898 # https://leetcode-cn.com/problems/generate-parentheses/ Balanced Parentheses A # The number expressions containing n pairs of parentheses which are correctly matched can be calculated via Catalan numbers. Quoting the relevant link from Wikipedia: There are many counting problems in combinatorics whose solution is given by the Catalan numbers \u2026 Cn is the number of Dyck words of length 2n. A Dyck word is a string consisting of n X's and n Y's such that no initial segment of the string has more Y's than X's \u2026 Re-interpreting the symbol X as an open parenthesis and Y as a close parenthesis, Cn counts the number of expressions containing n pairs of parentheses which are correctly matched. The nth Catalan number is given directly in terms of binomial coefficients by: https://en.wikipedia.org/wiki/Catalan_number#Applications_in_combinatorics Generate balanced parentheses in java # LeetCode 32. \u6700\u957f\u6709\u6548\u62ec\u53f7 # \u7c7b\u4f3c\u95ee\u9898\uff1a \u6700\u5927\u62ec\u53f7\u6df1\u5ea6 \u6700\u5927\u6811\u6df1\u5ea6\u95ee\u9898 \u5982\u4f55\u4f7f\u7528\u5e76\u53d1\u6280\u672f\u6765\u6539\u8fdb\u5e38\u89c1\u7b97\u6cd5 # \u4e8c\u5206\u95ee\u9898\uff0c \u8fd8\u6709\u5f88\u591a\u5176\u4ed6\u7684\u95ee\u9898\uff0c\u5176\u5b9e\u90fd\u975e\u5e38\u9002\u5408\u4e8e\u4f7f\u7528\u5e76\u53d1\u6280\u672f\u6765\u8fdb\u884c\u6539\u8fdb\uff0c\u6b63\u5982\u5728APUE\u768411.6.8 Barriers\u8282\u4e2d\u6240\u4e3e\u7684\u4f8b\u5b50\uff1b Stack-sortable permutation and catalan number # Merge algorithm and All nearest smaller values # \u6700\u957f\u5b50\u5e8f\u5217\u7684\u4f18\u5316 # \u964d\u4f4e\u7a7a\u95f4\u590d\u6742\u5ea6 expression tree # Wikipedia binary expression tree tree and stack # \u8868\u8fbe\u5f0f\u6811\u5176\u5b9e\u5b83\u662f\u53ef\u4ee5\u4f7f\u7528stack\u6765\u8fdb\u884c\u4fdd\u5b58\u7684\uff0c\u5728 Wikipedia binary expression tree \u4e2d\u5c31\u8fdb\u884c\u4e86\u5c55\u793a stacks in tree constructions Can a tree be used to create a stack? geeksforgeeks expression tree \u6269\u5c55 # Parse tree # Regular tree grammar # Abstract syntax tree # grammar vs syntax in compiler # https://linguistics.stackexchange.com/questions/3484/whats-the-difference-between-syntax-and-grammar https://softwareengineering.stackexchange.com/questions/116982/what-is-the-difference-between-syntax-and-grammar \u7b97\u6cd5\u5206\u7c7b # \u8fd9\u4e2a\u5206\u7c7b\u975e\u5e38\u597d\uff0c\u6211\u975e\u5e38\u559c\u6b22\uff1a http://www3.cs.stonybrook.edu/~algorith/implement/watson/implement.shtml \u4f7f\u7528\u8868\u683c\u9a71\u52a8\u7684\u7b97\u6cd5 # KMP\u7b97\u6cd5 LL(1) parser LR parser \u8868\u683c\u9a71\u52a8\u7684\u7b97\u6cd5\u548c\u52a8\u6001\u89c4\u5212\u4e4b\u95f4\u7684\u5173\u8054 Longest Common Prefix # https://leetcode.com/problems/longest-common-prefix/solution/","title":"TODO"},{"location":"TODO/#todo","text":"","title":"TODO"},{"location":"TODO/#20191227","text":"\u5f88\u591a\u7b97\u6cd5\u6280\u5de7\u90fd\u662f\u5728\u907f\u514d combinatorial explosion \u95ee\u9898\uff0c\u6bd4\u5982 dynamic programming LL(1) parser\uff0c\u4f7f\u7528\u9884\u6d4b\u7684\u65b9\u5f0f\u6765\u907f\u514d \u90a3\u4e48\u5982\u4f55\u6765\u7406\u89e3 combinatorial explosion \u5462\uff1f\u5f53\u6211\u4eec\u4f7f\u7528 backtracking \u7684\u65f6\u5019\uff0c\u5c31\u4f1a\u51fa\u73b0 combinatorial explosion \uff0c\u4ece\u7b97\u6cd5\u5206\u6790\u7684\u89d2\u5ea6\u6765\u770b\uff0c\u6b64\u65f6\u7684\u7b97\u6cd5\u590d\u6742\u5ea6\u5c31\u4e5f\u4f1aexplosion\u3002","title":"20191227"},{"location":"TODO/#backtracking","text":"","title":"backtracking \u7684\u526a\u679d"},{"location":"TODO/#_1","text":"https://leetcode-cn.com/problems/permutations-ii/solution/hui-su-suan-fa-python-dai-ma-java-dai-ma-by-liwe-2/","title":"\u65b9\u5f0f\u4e00\uff1a"},{"location":"TODO/#_2","text":"LL(1) parser\uff0c\u4f7f\u7528\u9884\u6d4b\u7684\u65b9\u5f0f\u6765\u907f\u514d","title":"\u65b9\u5f0f\u4e8c\uff1a\u9884\u6d4b"},{"location":"TODO/#find_top_k_elements_in_an_array","text":"https://www.geeksforgeeks.org/k-largestor-smallest-elements-in-an-array/ https://www.geeksforgeeks.org/find-top-k-or-most-frequent-numbers-in-a-stream/ https://leetcode-cn.com/problems/kth-largest-element-in-an-array/?utm_source=LCUS&utm_medium=ip_redirect_q_uns&utm_campaign=transfer2china https://leetcode-cn.com/problems/top-k-frequent-elements/?utm_source=LCUS&utm_medium=ip_redirect_q_uns&utm_campaign=transfer2china https://stackoverflow.com/questions/4956593/optimal-algorithm-for-returning-top-k-values-from-an-array-of-length-n","title":"find top k elements in an array"},{"location":"TODO/#how_to_handle_duplicates_in_binary_search_tree","text":"https://www.geeksforgeeks.org/how-to-handle-duplicates-in-binary-search-tree/ https://stackoverflow.com/questions/300935/are-duplicate-keys-allowed-in-the-definition-of-binary-search-trees","title":"How to handle duplicates in Binary Search Tree?"},{"location":"TODO/#_3","text":"https://leetcode-cn.com/problems/generate-parentheses/ Balanced Parentheses","title":"\u62ec\u53f7\u751f\u6210\u95ee\u9898"},{"location":"TODO/#a","text":"The number expressions containing n pairs of parentheses which are correctly matched can be calculated via Catalan numbers. Quoting the relevant link from Wikipedia: There are many counting problems in combinatorics whose solution is given by the Catalan numbers \u2026 Cn is the number of Dyck words of length 2n. A Dyck word is a string consisting of n X's and n Y's such that no initial segment of the string has more Y's than X's \u2026 Re-interpreting the symbol X as an open parenthesis and Y as a close parenthesis, Cn counts the number of expressions containing n pairs of parentheses which are correctly matched. The nth Catalan number is given directly in terms of binomial coefficients by: https://en.wikipedia.org/wiki/Catalan_number#Applications_in_combinatorics","title":"A"},{"location":"TODO/#generate_balanced_parentheses_in_java","text":"","title":"Generate balanced parentheses in java"},{"location":"TODO/#leetcode_32","text":"\u7c7b\u4f3c\u95ee\u9898\uff1a \u6700\u5927\u62ec\u53f7\u6df1\u5ea6 \u6700\u5927\u6811\u6df1\u5ea6\u95ee\u9898","title":"LeetCode 32. \u6700\u957f\u6709\u6548\u62ec\u53f7"},{"location":"TODO/#_4","text":"\u4e8c\u5206\u95ee\u9898\uff0c \u8fd8\u6709\u5f88\u591a\u5176\u4ed6\u7684\u95ee\u9898\uff0c\u5176\u5b9e\u90fd\u975e\u5e38\u9002\u5408\u4e8e\u4f7f\u7528\u5e76\u53d1\u6280\u672f\u6765\u8fdb\u884c\u6539\u8fdb\uff0c\u6b63\u5982\u5728APUE\u768411.6.8 Barriers\u8282\u4e2d\u6240\u4e3e\u7684\u4f8b\u5b50\uff1b","title":"\u5982\u4f55\u4f7f\u7528\u5e76\u53d1\u6280\u672f\u6765\u6539\u8fdb\u5e38\u89c1\u7b97\u6cd5"},{"location":"TODO/#stack-sortable_permutation_and_catalan_number","text":"","title":"Stack-sortable permutation and catalan number"},{"location":"TODO/#merge_algorithm_and_all_nearest_smaller_values","text":"","title":"Merge algorithm and All nearest smaller values"},{"location":"TODO/#_5","text":"\u964d\u4f4e\u7a7a\u95f4\u590d\u6742\u5ea6","title":"\u6700\u957f\u5b50\u5e8f\u5217\u7684\u4f18\u5316"},{"location":"TODO/#expression_tree","text":"Wikipedia binary expression tree","title":"expression tree"},{"location":"TODO/#tree_and_stack","text":"\u8868\u8fbe\u5f0f\u6811\u5176\u5b9e\u5b83\u662f\u53ef\u4ee5\u4f7f\u7528stack\u6765\u8fdb\u884c\u4fdd\u5b58\u7684\uff0c\u5728 Wikipedia binary expression tree \u4e2d\u5c31\u8fdb\u884c\u4e86\u5c55\u793a stacks in tree constructions Can a tree be used to create a stack? geeksforgeeks expression tree","title":"tree and stack"},{"location":"TODO/#_6","text":"","title":"\u6269\u5c55"},{"location":"TODO/#parse_tree","text":"","title":"Parse tree"},{"location":"TODO/#regular_tree_grammar","text":"","title":"Regular tree grammar"},{"location":"TODO/#abstract_syntax_tree","text":"","title":"Abstract syntax tree"},{"location":"TODO/#grammar_vs_syntax_in_compiler","text":"https://linguistics.stackexchange.com/questions/3484/whats-the-difference-between-syntax-and-grammar https://softwareengineering.stackexchange.com/questions/116982/what-is-the-difference-between-syntax-and-grammar","title":"grammar vs syntax in compiler"},{"location":"TODO/#_7","text":"\u8fd9\u4e2a\u5206\u7c7b\u975e\u5e38\u597d\uff0c\u6211\u975e\u5e38\u559c\u6b22\uff1a http://www3.cs.stonybrook.edu/~algorith/implement/watson/implement.shtml","title":"\u7b97\u6cd5\u5206\u7c7b"},{"location":"TODO/#_8","text":"KMP\u7b97\u6cd5 LL(1) parser LR parser \u8868\u683c\u9a71\u52a8\u7684\u7b97\u6cd5\u548c\u52a8\u6001\u89c4\u5212\u4e4b\u95f4\u7684\u5173\u8054","title":"\u4f7f\u7528\u8868\u683c\u9a71\u52a8\u7684\u7b97\u6cd5"},{"location":"TODO/#longest_common_prefix","text":"https://leetcode.com/problems/longest-common-prefix/solution/","title":"Longest Common Prefix"},{"location":"library/","text":"","title":"Library"},{"location":"Algorithm-design-paradigm/","text":"\u5173\u4e8e\u672c\u7ae0 # \u672c\u7ae0\u63cf\u8ff0\u4e00\u4e9b\u5e38\u89c1\u7684\u7b97\u6cd5paradigm\u3002 https://online.wlu.ca/news/2019/02/12/how-algorithm-design-applied","title":"Introduction"},{"location":"Algorithm-design-paradigm/#_1","text":"\u672c\u7ae0\u63cf\u8ff0\u4e00\u4e9b\u5e38\u89c1\u7684\u7b97\u6cd5paradigm\u3002 https://online.wlu.ca/news/2019/02/12/how-algorithm-design-applied","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"Algorithm-design-paradigm/Brute-force-VS-backtrack/","text":"\u66b4\u641c\u4e0e\u56de\u6eaf\uff0c\u4e24\u8005\u4e4b\u95f4\u5bc6\u5207\u76f8\u5173\uff1a https://www.cnblogs.com/dusf/p/kmp.html \u66b4\u529b\u641c\u7d22\u4f1a\u7f57\u5217\u6240\u6709\u7684\u53ef\u80fd\u6027\uff0c\u5728\u8fdb\u884c\u4f18\u5316\u7684\u65f6\u5019\uff0c\u9700\u8981\u8003\u8651\u5982\u4f55\u51cf\u5c11\u641c\u7d22\u7a7a\u95f4\u3002 \u5982\u4f55\u907f\u514d\u56de\u6eaf","title":"Brute-force-VS-backtrack"},{"location":"Algorithm-design-paradigm/00-Brute-force/Brute-force-search/","text":"Brute-force search #","title":"Brute-force-search"},{"location":"Algorithm-design-paradigm/00-Brute-force/Brute-force-search/#brute-force_search","text":"","title":"Brute-force search"},{"location":"Algorithm-design-paradigm/01-Divide-and-Conquer/Divide-and-conquer-algorithm/","text":"Divide-and-conquer algorithm #","title":"Divide-and-conquer-algorithm"},{"location":"Algorithm-design-paradigm/01-Divide-and-Conquer/Divide-and-conquer-algorithm/#divide-and-conquer_algorithm","text":"","title":"Divide-and-conquer algorithm"},{"location":"Algorithm-design-paradigm/02-Dynamic-Programming&Greedy-Algorithm/","text":"\u5173\u4e8e\u672c\u7ae0 # \u52a8\u6001\u89c4\u5212\u7b97\u6cd5\u548c\u8d2a\u5fc3\u7b97\u6cd5\u6bd4\u8f83\u7c7b\u4f3c\uff0c\u4e24\u8005\u90fd\u7528\u4e8e\u89e3\u51b3optimization\u95ee\u9898/\u6700\u503c\u95ee\u9898\uff0c\u56e0\u4e3a\u80fd\u591f\u7528\u8fd9\u4e24\u79cd\u7b97\u6cd5\u89e3\u51b3\u7684\u95ee\u9898\u5177\u6709\u76f8\u540c\u7684\u6027\u8d28\uff1a Optimal substructure \u5982\u679c\u95ee\u9898\u5177\u6709 Overlapping subproblems \u6027\u8d28\uff0c\u5219\u53ef\u4ee5\u4f7f\u7528 Dynamic programming \uff0c\u5426\u5219\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528 Greedy algorithm \u3002 TODO # https://stackoverflow.com/questions/tagged/dynamic-programming?tab=Active \u5e8f\u5217\u95ee\u9898\u4e0e\u591a\u6761\u89c4\u5212\u7b97\u6cd5","title":"Introduction"},{"location":"Algorithm-design-paradigm/02-Dynamic-Programming&Greedy-Algorithm/#_1","text":"\u52a8\u6001\u89c4\u5212\u7b97\u6cd5\u548c\u8d2a\u5fc3\u7b97\u6cd5\u6bd4\u8f83\u7c7b\u4f3c\uff0c\u4e24\u8005\u90fd\u7528\u4e8e\u89e3\u51b3optimization\u95ee\u9898/\u6700\u503c\u95ee\u9898\uff0c\u56e0\u4e3a\u80fd\u591f\u7528\u8fd9\u4e24\u79cd\u7b97\u6cd5\u89e3\u51b3\u7684\u95ee\u9898\u5177\u6709\u76f8\u540c\u7684\u6027\u8d28\uff1a Optimal substructure \u5982\u679c\u95ee\u9898\u5177\u6709 Overlapping subproblems \u6027\u8d28\uff0c\u5219\u53ef\u4ee5\u4f7f\u7528 Dynamic programming \uff0c\u5426\u5219\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528 Greedy algorithm \u3002","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"Algorithm-design-paradigm/02-Dynamic-Programming&Greedy-Algorithm/#todo","text":"https://stackoverflow.com/questions/tagged/dynamic-programming?tab=Active \u5e8f\u5217\u95ee\u9898\u4e0e\u591a\u6761\u89c4\u5212\u7b97\u6cd5","title":"TODO"},{"location":"Algorithm-design-paradigm/02-Dynamic-Programming&Greedy-Algorithm/Bellman-equation/","text":"Bellman equation Bellman equation #","title":"Bellman-equation"},{"location":"Algorithm-design-paradigm/02-Dynamic-Programming&Greedy-Algorithm/Bellman-equation/#bellman_equation","text":"","title":"Bellman equation"},{"location":"Algorithm-design-paradigm/02-Dynamic-Programming&Greedy-Algorithm/Dynamic-programming-VS-search-algorithm-VS-greedy-algorithm/","text":"VS # \u52a8\u6001\u89c4\u5212VS\u8d2a\u5fc3\u7b97\u6cd5 # \u4e24\u8005\u4e4b\u95f4\u7684\u5171\u540c\u70b9\u662f Optimal substructure \uff1b\u5982\u679c\u5177\u6709 Overlapping subproblems \uff0c\u5219\u4f7f\u7528 dynamic programming \uff0c\u5426\u5219\u4f7f\u7528 greedy algorithm \uff1b\u8fd9\u4e2a\u89c2\u70b9\u5728 Optimal substructure \u4e2d\u7ed9\u51fa\u4e86\uff1b \u52a8\u6001\u89c4\u5212\u7b97\u6cd5\u4e0e\u641c\u7d22\u7b97\u6cd5\u6bd4\u8f83\u5206\u6790 # \u65e0\u8bba\u662f \u52a8\u6001\u89c4\u5212\u7b97\u6cd5 \u6291\u6216\u662f \u57fa\u4e8e\u9012\u5f52\u7684\u641c\u7d22\u7b97\u6cd5 \uff0c\u4ed6\u4eec\u7684\u5b9e\u73b0\u63a5\u4f9d\u8d56\u4e8e \u9012\u5f52\u5173\u7cfb \u7684\u5efa\u7acb\u56e0\u6b64\u5efa\u7acb\u6b63\u786e\u7684 \u9012\u5f52\u5173\u7cfb \u662f\u89e3\u51b3\u95ee\u9898\u7684\u6838\u5fc3\u6240\u5728\u3002 \u4e0b\u9762\u5206\u6790\u8fd9\u4e24\u79cd\u7b97\u6cd5\u5bf9 \u9012\u5f52\u5173\u7cfb \u7684\u5b9e\u73b0\u65b9\u5f0f\u7684\u4e0d\u540c\u3002\u603b\u7684\u6765\u8bf4\u4ed6\u4eec\u5bf9 \u9012\u5f52\u5173\u7cfb \uff0c\u5b9e\u73b0\u7684\u65b9\u5f0f\u662f \u76f8\u53cd \u7684\uff1a \u641c\u7d22\u7b97\u6cd5 \u5bf9 \u9012\u5f52\u5173\u7cfb \u7684\u5b9e\u73b0\u662f \u81ea\u5de6\u81f3\u53f3 \u7684\uff0c\u56e0\u4e3a\u5b83\u91c7\u7528\u9012\u5f52\u6765\u5b9e\u73b0\uff0c\u56e0\u6b64\u4ed6\u7684\u8ba1\u7b97\u662f \u81ea\u9876\u5411\u4e0b \u8fdb\u884c\u7684\u3002\u4ed6\u501f\u4f4f \u7cfb\u7edf\u6808 \u4e0d\u65ad\u7684\u6309\u7167 \u9012\u5f52\u5173\u7cfb \u81ea\u5de6\u5411\u53f3\u7684\u8fdb\u884c \u5206\u6790 \uff0c\u76f4\u81f3 \u6700\u5c0f\u5b50\u95ee\u9898 \u53ef\u4ee5\u6c42\u89e3\u5f97\u5230\uff1b\u5728 \u641c\u7d22\u7b97\u6cd5 \u4e2d\uff0c \u6700\u5c0f\u5b50\u95ee\u9898 \u662f \u9012\u5f52\u7ec8\u6b62\u6761\u4ef6 \uff1b \u52a8\u6001\u89c4\u5212\u7b97\u6cd5 \u5bf9 \u9012\u5f52\u5173\u7cfb \u7684\u5b9e\u73b0\u662f \u81ea\u53f3\u81f3\u5de6 \u7684\uff0c\u56e0\u6b64\u4ed6\u9996\u5148\u8ba1\u7b97\u7684\u662f \u6700\u5c0f\u5b50\u95ee\u9898 \uff0c\u7136\u540e\u6309\u7167\u9012\u5f52\u5173\u7cfb\uff08 synthetically \u6216\u8005 \u5f52\u7eb3\uff09\u8ba1\u7b97\u51fa\u66f4\u5927\u7684\u95ee\u9898\u76f4\u81f3\u6c42\u89e3\u9664\u76ee\u6807\u95ee\u9898\uff0c\u5373\u66f4\u5927\u95ee\u9898\u7684\u6c42\u89e3\u662f\u4f9d\u8d56\u4e8e\u5c0f\u95ee\u9898\u7684\u89e3\u7684\uff0c\u6240\u4ee5\u5728\u52a8\u6001\u89c4\u5212\u7b97\u6cd5\u4e2d\uff0c\u5fc5\u987b\u8981\u8003\u8651\u7684\u4e00\u4e2a\u95ee\u9898\u662f\u4fdd\u5b58\u5c0f\u95ee\u9898\u7684\u89e3\uff08\u5982 Fibonacci sequence \u4e2d\u9700\u8981\u4fdd\u5b58\u524d\u4e24\u9879\uff0c\u6700\u5927\u5b57\u6bb5\u548c\u95ee\u9898\u4e2d\u9700\u8981\u4fdd\u5b58\u524d\u4e00\u9879 \uff09\u3002\u4fdd\u5b58\u5c0f\u95ee\u9898\u7684\u89e3\u6240\u5e26\u6765\u7684\u53e6\u5916\u4e00\u4e2a\u597d\u5904\u662f\uff1a\u907f\u514d\u4e86\u91cd\u590d\u8ba1\u7b97\u76f8\u540c\u7684\u95ee\u9898\uff0c\u5728\u52a8\u6001\u89c4\u5212\u7b97\u6cd5\u4e2d\u5e38\u5e38\u9700\u8981\u4f7f\u7528\u4e00\u4e2a\u5bb9\u5668\u5c06\u5404\u4e2a\u6b64\u95ee\u9898\u7684\u8ba1\u7b97\u7ed3\u679c\u7ed9\u4fdd\u5b58\u8d77\u6765\uff0c\u8fd9\u6837\u5728\u540e\u7eed\u8ba1\u7b97\u4e2d\u9700\u8981\u88ab\u4f7f\u7528\u65f6\u5c31\u53ef\u76f4\u63a5\u53d6\u5f97\uff08 Overlapping subproblems \uff09\uff1b\u5728 \u52a8\u6001\u89c4\u5212\u7b97\u6cd5 \u4e2d\uff0c \u6700\u5c0f\u5b50\u95ee\u9898 \u662f\u65e0\u9700\u8ba1\u7b97\u7684\uff0c\u5b83\u662f\u5728\u7b97\u6cd5\u5f00\u59cb\u4e4b\u521d\u5c31\u53ef\u4ee5\u76f4\u63a5 \u521d\u59cb\u5316 \u5230\u4fdd\u5b58\u95ee\u9898\u89e3\u7684\u5bb9\u5668\u4e2d\uff1b SUMMARY : \u8fd9\u79cd\u5173\u7cfb\u5728 wikipedia Corecursion \u4e2d\u6709\u975e\u5e38\u597d\u7684\u63cf\u8ff0\uff1b \u641c\u7d22\u7b97\u6cd5 \u5bf9 \u9012\u5f52\u5173\u7cfb \u7684\u5904\u7406\u662f \u7531\u5de6\u5411\u53f3 \u7684\u56e0\u6b64\u4ed6\u662f \u7531\u9876\u5411\u4e0b \u8fdb\u884c\u8ba1\u7b97\u7684\uff1b \u52a8\u6001\u89c4\u5212\u7b97\u6cd5 \u5bf9 \u9012\u5f52\u5173\u7cfb \u7684\u5904\u7406\u662f \u7531\u53f3\u5411\u5de6 \u7684\u56e0\u6b64\u4ed6\u7684\u8ba1\u7b97\u662f \u7531\u5e95\u5411\u4e0a \u7684\uff1b\u65e0\u8bba\u662f \u52a8\u6001\u89c4\u5212\u7b97\u6cd5 \u6291\u6216\u662f \u56de\u6eaf\u6cd5 \u4ed6\u4eec\u63a5\u4f9d\u8d56\u4e8e\u9012\u5f52\u8868\u8fbe\u5f0f\u7684\u5efa\u7acb\u4e24\u79cd\u7b97\u6cd5\u5bf9\u9012\u5f52\u8868\u8fbe\u5f0f\u63d0\u4f9b\u4e86\u4e0d\u540c\u7684\u5b9e\u73b0\u65b9\u5f0f\u663e\u7136\u3002\u662f\u5b9e\u73b0\u91c7\u7528\u7684\u662f\u66f4\u52a0\u7eaf\u7cb9\u7684\u9012\u5f52\u65b9\u5f0f\u4ed6\u501f\u52a9\u4e8e\u7cfb\u7edf\u7ad9\u6765\u9010\u6b65\u5206\u89e3\u76f4\u5230\u6700\u5c0f\u5b50\u95ee\u9898\u800c\u52a8\u6001\u89c4\u5212\u7b97\u6cd5\u5219\u5b8c\u5168\u662f\u7531\u7a0b\u5e8f\u7684\u8bed\u8a00\u6765\u5b89\u6392\u8ba1\u7b97\u6b64\u95ee\u9898\u7684\u6b21\u5e8f\u3002D\u5b89\u6392\u6570\u7ec4\u548c\u53d8\u4eae\u6765\u4fdd\u5b58\u8ba1\u7b97\u7684\u5b50\u95ee\u9898\u7684\u7ed3\u679c\u6211\u731c\u60f3\u5e94\u8be5\u662f\u5148\u6709\u56de\u8083\u53cd\u7136\u540e\u624d\u6709\u4e86\u9488\u5bf9\u56de\u6eaf\u53d1\u8fdb\u884c\u6539\u8fdb\u7684\u66f4\u52a0\u7075\u6d3b\u7684\u52a8\u6001\u89c4\u5212\u7b97\u6cd5\u3002 \u52a8\u6001\u89c4\u5212VS\u56de\u6eaf\u6cd5VS\u5206\u652f\u9650\u754c\u6cd5VS\u8d2a\u5fc3\u7b97\u6cd5 # \u52a8\u6001\u89c4\u5212\u3001\u56de\u6eaf\u6cd5\u3001\u5206\u652f\u9650\u754c\u6cd5\u90fd\u5c1d\u8bd5\u5728\u95ee\u9898\u7684\u89e3\u7a7a\u95f4\u4e2d\u9009\u53d6\u6700\u4f18\u89e3\uff0c\u800c\u8d2a\u5fc3\u7b97\u6cd5\u5219\u4e0d\u540c\uff0c\u5b83\u4e0d\u65ad\u5730\u9009\u53d6\u5f53\u524d\u6700\u4f18\u89e3\u3002","title":"Dynamic-programming-VS-search-algorithm-VS-greedy-algorithm"},{"location":"Algorithm-design-paradigm/02-Dynamic-Programming&Greedy-Algorithm/Dynamic-programming-VS-search-algorithm-VS-greedy-algorithm/#vs","text":"","title":"VS"},{"location":"Algorithm-design-paradigm/02-Dynamic-Programming&Greedy-Algorithm/Dynamic-programming-VS-search-algorithm-VS-greedy-algorithm/#vs_1","text":"\u4e24\u8005\u4e4b\u95f4\u7684\u5171\u540c\u70b9\u662f Optimal substructure \uff1b\u5982\u679c\u5177\u6709 Overlapping subproblems \uff0c\u5219\u4f7f\u7528 dynamic programming \uff0c\u5426\u5219\u4f7f\u7528 greedy algorithm \uff1b\u8fd9\u4e2a\u89c2\u70b9\u5728 Optimal substructure \u4e2d\u7ed9\u51fa\u4e86\uff1b","title":"\u52a8\u6001\u89c4\u5212VS\u8d2a\u5fc3\u7b97\u6cd5"},{"location":"Algorithm-design-paradigm/02-Dynamic-Programming&Greedy-Algorithm/Dynamic-programming-VS-search-algorithm-VS-greedy-algorithm/#_1","text":"\u65e0\u8bba\u662f \u52a8\u6001\u89c4\u5212\u7b97\u6cd5 \u6291\u6216\u662f \u57fa\u4e8e\u9012\u5f52\u7684\u641c\u7d22\u7b97\u6cd5 \uff0c\u4ed6\u4eec\u7684\u5b9e\u73b0\u63a5\u4f9d\u8d56\u4e8e \u9012\u5f52\u5173\u7cfb \u7684\u5efa\u7acb\u56e0\u6b64\u5efa\u7acb\u6b63\u786e\u7684 \u9012\u5f52\u5173\u7cfb \u662f\u89e3\u51b3\u95ee\u9898\u7684\u6838\u5fc3\u6240\u5728\u3002 \u4e0b\u9762\u5206\u6790\u8fd9\u4e24\u79cd\u7b97\u6cd5\u5bf9 \u9012\u5f52\u5173\u7cfb \u7684\u5b9e\u73b0\u65b9\u5f0f\u7684\u4e0d\u540c\u3002\u603b\u7684\u6765\u8bf4\u4ed6\u4eec\u5bf9 \u9012\u5f52\u5173\u7cfb \uff0c\u5b9e\u73b0\u7684\u65b9\u5f0f\u662f \u76f8\u53cd \u7684\uff1a \u641c\u7d22\u7b97\u6cd5 \u5bf9 \u9012\u5f52\u5173\u7cfb \u7684\u5b9e\u73b0\u662f \u81ea\u5de6\u81f3\u53f3 \u7684\uff0c\u56e0\u4e3a\u5b83\u91c7\u7528\u9012\u5f52\u6765\u5b9e\u73b0\uff0c\u56e0\u6b64\u4ed6\u7684\u8ba1\u7b97\u662f \u81ea\u9876\u5411\u4e0b \u8fdb\u884c\u7684\u3002\u4ed6\u501f\u4f4f \u7cfb\u7edf\u6808 \u4e0d\u65ad\u7684\u6309\u7167 \u9012\u5f52\u5173\u7cfb \u81ea\u5de6\u5411\u53f3\u7684\u8fdb\u884c \u5206\u6790 \uff0c\u76f4\u81f3 \u6700\u5c0f\u5b50\u95ee\u9898 \u53ef\u4ee5\u6c42\u89e3\u5f97\u5230\uff1b\u5728 \u641c\u7d22\u7b97\u6cd5 \u4e2d\uff0c \u6700\u5c0f\u5b50\u95ee\u9898 \u662f \u9012\u5f52\u7ec8\u6b62\u6761\u4ef6 \uff1b \u52a8\u6001\u89c4\u5212\u7b97\u6cd5 \u5bf9 \u9012\u5f52\u5173\u7cfb \u7684\u5b9e\u73b0\u662f \u81ea\u53f3\u81f3\u5de6 \u7684\uff0c\u56e0\u6b64\u4ed6\u9996\u5148\u8ba1\u7b97\u7684\u662f \u6700\u5c0f\u5b50\u95ee\u9898 \uff0c\u7136\u540e\u6309\u7167\u9012\u5f52\u5173\u7cfb\uff08 synthetically \u6216\u8005 \u5f52\u7eb3\uff09\u8ba1\u7b97\u51fa\u66f4\u5927\u7684\u95ee\u9898\u76f4\u81f3\u6c42\u89e3\u9664\u76ee\u6807\u95ee\u9898\uff0c\u5373\u66f4\u5927\u95ee\u9898\u7684\u6c42\u89e3\u662f\u4f9d\u8d56\u4e8e\u5c0f\u95ee\u9898\u7684\u89e3\u7684\uff0c\u6240\u4ee5\u5728\u52a8\u6001\u89c4\u5212\u7b97\u6cd5\u4e2d\uff0c\u5fc5\u987b\u8981\u8003\u8651\u7684\u4e00\u4e2a\u95ee\u9898\u662f\u4fdd\u5b58\u5c0f\u95ee\u9898\u7684\u89e3\uff08\u5982 Fibonacci sequence \u4e2d\u9700\u8981\u4fdd\u5b58\u524d\u4e24\u9879\uff0c\u6700\u5927\u5b57\u6bb5\u548c\u95ee\u9898\u4e2d\u9700\u8981\u4fdd\u5b58\u524d\u4e00\u9879 \uff09\u3002\u4fdd\u5b58\u5c0f\u95ee\u9898\u7684\u89e3\u6240\u5e26\u6765\u7684\u53e6\u5916\u4e00\u4e2a\u597d\u5904\u662f\uff1a\u907f\u514d\u4e86\u91cd\u590d\u8ba1\u7b97\u76f8\u540c\u7684\u95ee\u9898\uff0c\u5728\u52a8\u6001\u89c4\u5212\u7b97\u6cd5\u4e2d\u5e38\u5e38\u9700\u8981\u4f7f\u7528\u4e00\u4e2a\u5bb9\u5668\u5c06\u5404\u4e2a\u6b64\u95ee\u9898\u7684\u8ba1\u7b97\u7ed3\u679c\u7ed9\u4fdd\u5b58\u8d77\u6765\uff0c\u8fd9\u6837\u5728\u540e\u7eed\u8ba1\u7b97\u4e2d\u9700\u8981\u88ab\u4f7f\u7528\u65f6\u5c31\u53ef\u76f4\u63a5\u53d6\u5f97\uff08 Overlapping subproblems \uff09\uff1b\u5728 \u52a8\u6001\u89c4\u5212\u7b97\u6cd5 \u4e2d\uff0c \u6700\u5c0f\u5b50\u95ee\u9898 \u662f\u65e0\u9700\u8ba1\u7b97\u7684\uff0c\u5b83\u662f\u5728\u7b97\u6cd5\u5f00\u59cb\u4e4b\u521d\u5c31\u53ef\u4ee5\u76f4\u63a5 \u521d\u59cb\u5316 \u5230\u4fdd\u5b58\u95ee\u9898\u89e3\u7684\u5bb9\u5668\u4e2d\uff1b SUMMARY : \u8fd9\u79cd\u5173\u7cfb\u5728 wikipedia Corecursion \u4e2d\u6709\u975e\u5e38\u597d\u7684\u63cf\u8ff0\uff1b \u641c\u7d22\u7b97\u6cd5 \u5bf9 \u9012\u5f52\u5173\u7cfb \u7684\u5904\u7406\u662f \u7531\u5de6\u5411\u53f3 \u7684\u56e0\u6b64\u4ed6\u662f \u7531\u9876\u5411\u4e0b \u8fdb\u884c\u8ba1\u7b97\u7684\uff1b \u52a8\u6001\u89c4\u5212\u7b97\u6cd5 \u5bf9 \u9012\u5f52\u5173\u7cfb \u7684\u5904\u7406\u662f \u7531\u53f3\u5411\u5de6 \u7684\u56e0\u6b64\u4ed6\u7684\u8ba1\u7b97\u662f \u7531\u5e95\u5411\u4e0a \u7684\uff1b\u65e0\u8bba\u662f \u52a8\u6001\u89c4\u5212\u7b97\u6cd5 \u6291\u6216\u662f \u56de\u6eaf\u6cd5 \u4ed6\u4eec\u63a5\u4f9d\u8d56\u4e8e\u9012\u5f52\u8868\u8fbe\u5f0f\u7684\u5efa\u7acb\u4e24\u79cd\u7b97\u6cd5\u5bf9\u9012\u5f52\u8868\u8fbe\u5f0f\u63d0\u4f9b\u4e86\u4e0d\u540c\u7684\u5b9e\u73b0\u65b9\u5f0f\u663e\u7136\u3002\u662f\u5b9e\u73b0\u91c7\u7528\u7684\u662f\u66f4\u52a0\u7eaf\u7cb9\u7684\u9012\u5f52\u65b9\u5f0f\u4ed6\u501f\u52a9\u4e8e\u7cfb\u7edf\u7ad9\u6765\u9010\u6b65\u5206\u89e3\u76f4\u5230\u6700\u5c0f\u5b50\u95ee\u9898\u800c\u52a8\u6001\u89c4\u5212\u7b97\u6cd5\u5219\u5b8c\u5168\u662f\u7531\u7a0b\u5e8f\u7684\u8bed\u8a00\u6765\u5b89\u6392\u8ba1\u7b97\u6b64\u95ee\u9898\u7684\u6b21\u5e8f\u3002D\u5b89\u6392\u6570\u7ec4\u548c\u53d8\u4eae\u6765\u4fdd\u5b58\u8ba1\u7b97\u7684\u5b50\u95ee\u9898\u7684\u7ed3\u679c\u6211\u731c\u60f3\u5e94\u8be5\u662f\u5148\u6709\u56de\u8083\u53cd\u7136\u540e\u624d\u6709\u4e86\u9488\u5bf9\u56de\u6eaf\u53d1\u8fdb\u884c\u6539\u8fdb\u7684\u66f4\u52a0\u7075\u6d3b\u7684\u52a8\u6001\u89c4\u5212\u7b97\u6cd5\u3002","title":"\u52a8\u6001\u89c4\u5212\u7b97\u6cd5\u4e0e\u641c\u7d22\u7b97\u6cd5\u6bd4\u8f83\u5206\u6790"},{"location":"Algorithm-design-paradigm/02-Dynamic-Programming&Greedy-Algorithm/Dynamic-programming-VS-search-algorithm-VS-greedy-algorithm/#vsvsvs","text":"\u52a8\u6001\u89c4\u5212\u3001\u56de\u6eaf\u6cd5\u3001\u5206\u652f\u9650\u754c\u6cd5\u90fd\u5c1d\u8bd5\u5728\u95ee\u9898\u7684\u89e3\u7a7a\u95f4\u4e2d\u9009\u53d6\u6700\u4f18\u89e3\uff0c\u800c\u8d2a\u5fc3\u7b97\u6cd5\u5219\u4e0d\u540c\uff0c\u5b83\u4e0d\u65ad\u5730\u9009\u53d6\u5f53\u524d\u6700\u4f18\u89e3\u3002","title":"\u52a8\u6001\u89c4\u5212VS\u56de\u6eaf\u6cd5VS\u5206\u652f\u9650\u754c\u6cd5VS\u8d2a\u5fc3\u7b97\u6cd5"},{"location":"Algorithm-design-paradigm/02-Dynamic-Programming&Greedy-Algorithm/Optimal-substructure/","text":"Optimal substructure Problems with optimal substructure Problems without optimal substructure 20191112 Optimal substructure # In computer science , a problem is said to have optimal substructure if an optimal solution can be constructed from optimal solutions of its subproblems. This property is used to determine the usefulness of dynamic programming and greedy algorithms for a problem.[ 1] SUMMARY : optimal substructure\u5176\u5b9e\u4e5f\u4f53\u73b0\u4e86 \u9012\u5f52\u5173\u7cfb \uff1b Typically, a greedy algorithm is used to solve a problem with optimal substructure if it can be proven by induction that this is optimal at each step.[ 1] Otherwise, provided the problem exhibits overlapping subproblems as well, dynamic programming is used. If there are no appropriate greedy algorithms and the problem fails to exhibit overlapping subproblems, often a lengthy but straightforward search of the solution space is the best alternative\uff08\u4f7f\u7528\u641c\u7d22\u7b97\u6cd5\uff09. In the application of dynamic programming to mathematical optimization , Richard Bellman 's Principle of Optimality is based on the idea that in order to solve a dynamic optimization problem from some starting period t to some ending period T , one implicitly has to solve subproblems starting from later dates s , where t<s<T . This is an example of optimal substructure . The Principle of Optimality is used to derive the Bellman equation , which shows how the value of the problem starting from t is related to the value of the problem starting from s . Problems with optimal substructure # Longest common subsequence problem Longest increasing subsequence Longest palindromic substring All-Pairs Shortest Path Any problem that can be solved by dynamic programming . Problems without optimal substructure # Longest path problem Least-cost airline fare. (Using online flight search, we will frequently find that the cheapest flight from airport A to airport B involves a single connection through airport C, but the cheapest flight from airport A to airport C involves a connection through some other airport D.) 20191112 # \u6700\u4f18\u5b50\u7ed3\u6784\u5176\u5b9e\u5c31\u5177\u5907\u9012\u5f52\u6027\u8d28\uff1a\u5168\u95ee\u9898\u7684\u6700\u4f18\u89e3\u8574\u542b\u7740\u5b50\u95ee\u9898\u7684\u6700\u4f18\u89e3\u3002 \u6240\u4ee5\u6211\u4eec\u5373\u53ef\u4ee5\u81ea\u9876\u5411\u4e0b\u6765\u8fd0\u7528\u9012\u5f52\u5173\u7cfb\u4e5f\u53ef\u4ee5\u81ea\u5e95\u5411\u4e0a\u6765\u8fd0\u7528\u9012\u5f52\u5173\u7cfb\u3002\u52a8\u6001\u89c4\u5212\u5c31\u662f\u81ea\u5e95\u5411\u4e0a\u7684\u3002","title":"Optimal-substructure"},{"location":"Algorithm-design-paradigm/02-Dynamic-Programming&Greedy-Algorithm/Optimal-substructure/#optimal_substructure","text":"In computer science , a problem is said to have optimal substructure if an optimal solution can be constructed from optimal solutions of its subproblems. This property is used to determine the usefulness of dynamic programming and greedy algorithms for a problem.[ 1] SUMMARY : optimal substructure\u5176\u5b9e\u4e5f\u4f53\u73b0\u4e86 \u9012\u5f52\u5173\u7cfb \uff1b Typically, a greedy algorithm is used to solve a problem with optimal substructure if it can be proven by induction that this is optimal at each step.[ 1] Otherwise, provided the problem exhibits overlapping subproblems as well, dynamic programming is used. If there are no appropriate greedy algorithms and the problem fails to exhibit overlapping subproblems, often a lengthy but straightforward search of the solution space is the best alternative\uff08\u4f7f\u7528\u641c\u7d22\u7b97\u6cd5\uff09. In the application of dynamic programming to mathematical optimization , Richard Bellman 's Principle of Optimality is based on the idea that in order to solve a dynamic optimization problem from some starting period t to some ending period T , one implicitly has to solve subproblems starting from later dates s , where t<s<T . This is an example of optimal substructure . The Principle of Optimality is used to derive the Bellman equation , which shows how the value of the problem starting from t is related to the value of the problem starting from s .","title":"Optimal substructure"},{"location":"Algorithm-design-paradigm/02-Dynamic-Programming&Greedy-Algorithm/Optimal-substructure/#problems_with_optimal_substructure","text":"Longest common subsequence problem Longest increasing subsequence Longest palindromic substring All-Pairs Shortest Path Any problem that can be solved by dynamic programming .","title":"Problems with optimal substructure"},{"location":"Algorithm-design-paradigm/02-Dynamic-Programming&Greedy-Algorithm/Optimal-substructure/#problems_without_optimal_substructure","text":"Longest path problem Least-cost airline fare. (Using online flight search, we will frequently find that the cheapest flight from airport A to airport B involves a single connection through airport C, but the cheapest flight from airport A to airport C involves a connection through some other airport D.)","title":"Problems without optimal substructure"},{"location":"Algorithm-design-paradigm/02-Dynamic-Programming&Greedy-Algorithm/Optimal-substructure/#20191112","text":"\u6700\u4f18\u5b50\u7ed3\u6784\u5176\u5b9e\u5c31\u5177\u5907\u9012\u5f52\u6027\u8d28\uff1a\u5168\u95ee\u9898\u7684\u6700\u4f18\u89e3\u8574\u542b\u7740\u5b50\u95ee\u9898\u7684\u6700\u4f18\u89e3\u3002 \u6240\u4ee5\u6211\u4eec\u5373\u53ef\u4ee5\u81ea\u9876\u5411\u4e0b\u6765\u8fd0\u7528\u9012\u5f52\u5173\u7cfb\u4e5f\u53ef\u4ee5\u81ea\u5e95\u5411\u4e0a\u6765\u8fd0\u7528\u9012\u5f52\u5173\u7cfb\u3002\u52a8\u6001\u89c4\u5212\u5c31\u662f\u81ea\u5e95\u5411\u4e0a\u7684\u3002","title":"20191112"},{"location":"Algorithm-design-paradigm/02-Dynamic-Programming&Greedy-Algorithm/Optimization-summary/","text":"\u4e00\u822c\u8981\u6211\u4eec\u89e3\u51b3\u7684optimization\u95ee\u9898\u4e2d\u5f80\u5f80\u53ea\u5305\u542b\u4e00\u4e2a\u6700\u503c\uff0c\u5982\uff1a - All nearest smaller values \u7684\u6700\u503c\u662fnearest - Maximum subarray problem \u7684\u6700\u503c\u662flargest \u6240\u4ee5\u5728\u6c42\u89e3\u6700\u503c\u95ee\u9898\u7684\u65f6\u5019\uff0c\u4e00\u4e2a\u975e\u5e38\u91cd\u8981\u7684\u524d\u63d0\u662f\u8981\u641e\u6e05\u695a\u5b83\u7684\u6700\u503c\u662f\u4ec0\u4e48\u3002 \u5176\u6b21\uff0c\u6211\u4eec\u5f80\u5f80\u662f\u57fa\u4e8e \u6bd4\u8f83 \uff08\u6253\u64c2\u53f0\uff09\u6765\u6c42\u89e3\u6700\u503c\uff0c\u4f46\u662f\u6709\u6709\u4e9b\u662f\u53ef\u4ee5\u76f4\u63a5\u8fdb\u884c\u6bd4\u8f83\u7684\uff0c\u6bd4\u5982\u6570\u503c\uff0c\u4f46\u662f\u6709\u4e9b\u662f\u65e0\u6cd5\u76f4\u63a5\u8fdb\u884c\u6bd4\u8f83\u7684\uff0c\u6bd4\u5982\u5728 All nearest smaller values \u4e2d\u6700\u503c\u662fnearest\uff0c\u9664\u975e\u8bb0\u5f55\u6bcf\u4e2a\u5143\u7d20\u7684\u4f4d\u7f6e\uff0c\u5426\u5219\u53ea\u80fd\u591f\u501f\u52a9\u4e00\u4e2astack\u6765\u5b9e\u73b0nearest\uff1b","title":"Optimization-summary"},{"location":"Algorithm-design-paradigm/02-Dynamic-Programming&Greedy-Algorithm/Dynamic-Programming/Catalan/","text":"","title":"Catalan"},{"location":"Algorithm-design-paradigm/02-Dynamic-Programming&Greedy-Algorithm/Dynamic-Programming/Dynamic-programming/","text":"Dynamic programming # Dynamic programming is both a mathematical optimization method and a computer programming method. The method was developed by Richard Bellman in the 1950s and has found applications in numerous fields, from aerospace engineering \uff08\u822a\u5929\u5de5\u7a0b\uff09 to economics . In both contexts it refers to simplifying a complicated problem by breaking it down into simpler sub-problems in a recursive manner. While some decision problems \uff08\u51b3\u7b56\u95ee\u9898\uff09 cannot be taken apart this way, decisions that span several points in time do often break apart recursively. Likewise, in computer science, if a problem can be solved optimally by breaking it into sub-problems and then recursively finding the optimal solutions to the sub-problems , then it is said to have optimal substructure \uff08\u6700\u4f18\u5b50\u7ed3\u6784\uff09. If sub-problems can be nested recursively inside larger problems, so that dynamic programming methods are applicable, then there is a relation between the value of the larger problem and the values of the sub-problems.[ 1] In the optimization literature this relationship is called the Bellman equation . Overview # Mathematical optimization # In terms of mathematical optimization, dynamic programming usually refers to simplifying a decision by breaking it down into a sequence of decision steps over time. This is done by defining a sequence of value functions V 1, V 2, ..., V n taking y* as an argument representing the state of the system at times i from 1 to n . The definition of *V n ( y ) is the value obtained in state y at the last time n . The values V i at earlier times i = n \u22121, n \u2212 2, ..., 2, 1 can be found by working backwards, using a recursive relationship called the Bellman equation . For i = 2, ..., n , V i \u22121 at any state y is calculated from V i by maximizing a simple function (usually the sum) of the gain from a decision at time i \u2212 1 and the function V i at the new state of the system if this decision is made. Since V i has already been calculated for the needed states, the above operation yields V i \u22121 for those states. Finally, V 1 at the initial state of the system is the value of the optimal solution. The optimal values of the decision variables can be recovered, one by one, by tracking back the calculations already performed. Computer programming # There are two key attributes that a problem must have in order for dynamic programming to be applicable: optimal substructure and overlapping sub-problems . If a problem can be solved by combining optimal solutions to non-overlapping sub-problems, the strategy is called \" divide and conquer \" instead.[ 1] This is why merge sort and quick sort are not classified as dynamic programming problems. SUMMARY : sorting\u5e76\u4e0d\u5177\u5907 overlapping sub-problems \u7684\u7279\u6027\uff1b Optimal substructure means that the solution to a given optimization problem can be obtained by the combination of optimal solutions to its sub-problems . Such optimal substructures are usually described by means of recursion . For example, given a graph G=(V,E) , the shortest path p from a vertex u to a vertex v exhibits optimal substructure : take any intermediate vertex w on this shortest path p . If p is truly the shortest path, then it can be split into sub-paths p1 from u to w and p2 from w to v such that these, in turn, are indeed the shortest paths between the corresponding vertices (by the simple cut-and-paste argument described in Introduction to Algorithms ). Hence, one can easily formulate the solution for finding shortest paths in a recursive manner, which is what the Bellman\u2013Ford algorithm or the Floyd\u2013Warshall algorithm does. Overlapping sub-problems means that the space of sub-problems must be small, that is, any recursive algorithm solving the problem should solve the same sub-problems over and over, rather than generating new sub-problems. For example, consider the recursive formulation for generating the Fibonacci series: $F_i = F_{i\u22121} + F_{i\u22122}$, with base case $F_1 = F_2 = 1$. Then $F_{43} = F_{42} + F_{41}$, and $F_{42} = F_{41} + F_{40}$. Now $F_{41}$ is being solved in the recursive sub-trees of both $F_{43}$ as well as $F_{42}$. Even though the total number of sub-problems is actually small (only 43 of them), we end up solving the same problems over and over if we adopt a naive recursive solution such as this. Dynamic programming takes account of this fact and solves each sub-problem only once. Figure 2. This can be achieved in either of two ways:[ citation needed ] Top-down approach : This is the direct fall-out of the recursive formulation of any problem. If the solution to any problem can be formulated recursively using the solution to its sub-problems, and if its sub-problems are overlapping, then one can easily memoize or store the solutions to the sub-problems in a table. Whenever we attempt to solve a new sub-problem, we first check the table to see if it is already solved. If a solution has been recorded, we can use it directly, otherwise we solve the sub-problem and add its solution to the table. Bottom-up approach : Once we formulate the solution to a problem recursively as in terms of its sub-problems, we can try reformulating the problem in a bottom-up fashion: try solving the sub-problems first and use their solutions to build-on and arrive at solutions to bigger sub-problems. This is also usually done in a tabular form by iteratively generating solutions to bigger and bigger sub-problems by using the solutions to small sub-problems. For example, if we already know the values of F 41 and F 40, we can directly calculate the value of F 42. Some programming languages can automatically memoize the result of a function call with a particular set of arguments, in order to speed up call-by-name evaluation (this mechanism is referred to as call-by-need ). Some languages make it possible portably (e.g. Scheme , Common Lisp , Perl or D ). Some languages have automatic memoization built in, such as tabled Prolog and J , which supports memoization with the M. adverb.[ 4] In any case, this is only possible for a referentially transparent function. Memoization is also encountered as an easily accessible design pattern within term-rewrite based languages such as Wolfram Language . Examples: Computer algorithms # Dijkstra's algorithm for the shortest path problem # Fibonacci sequence # A type of balanced 0\u20131 matrix # Checkerboard # Sequence alignment # Tower of Hanoi puzzle # Egg dropping puzzle # Matrix chain multiplication # Main article: Matrix chain multiplication Algorithms that use dynamic programming # Recurrent solutions to lattice models for protein-DNA binding Backward induction as a solution method for finite-horizon discrete-time dynamic optimization problems Method of undetermined coefficients can be used to solve the Bellman equation in infinite-horizon, discrete-time, discounted , time-invariant dynamic optimization problems Many string algorithms including longest common subsequence , longest increasing subsequence , longest common substring , Levenshtein distance (edit distance) Many algorithmic problems on graphs can be solved efficiently for graphs of bounded treewidth or bounded clique-width by using dynamic programming on a tree decomposition of the graph. The Cocke\u2013Younger\u2013Kasami (CYK) algorithm which determines whether and how a given string can be generated by a given context-free grammar Knuth's word wrapping algorithm that minimizes raggedness when word wrapping text The use of transposition tables and refutation tables in computer chess The Viterbi algorithm (used for hidden Markov models , and particularly in part of speech tagging ) The Earley algorithm (a type of chart parser ) The Needleman\u2013Wunsch algorithm and other algorithms used in bioinformatics , including sequence alignment , structural alignment , RNA structure prediction Floyd's all-pairs shortest path algorithm Optimizing the order for chain matrix multiplication Pseudo-polynomial time algorithms for the subset sum , knapsack and partition problems The dynamic time warping algorithm for computing the global distance between two time series The Selinger (a.k.a. System R ) algorithm for relational database query optimization De Boor algorithm for evaluating B-spline curves Duckworth\u2013Lewis method for resolving the problem when games of cricket are interrupted The value iteration method for solving Markov decision processes Some graphic image edge following selection methods such as the \"magnet\" selection tool in Photoshop Some methods for solving interval scheduling problems Some methods for solving the travelling salesman problem , either exactly (in exponential time ) or approximately (e.g. via the bitonic tour ) Recursive least squares method Beat tracking in music information retrieval Adaptive-critic training strategy for artificial neural networks Stereo algorithms for solving the correspondence problem used in stereo vision Seam carving (content-aware image resizing) The Bellman\u2013Ford algorithm for finding the shortest distance in a graph Some approximate solution methods for the linear search problem Kadane's algorithm for the maximum subarray problem Optimization of electric generation expansion plans in the Wein Automatic System Planning (WASP) package See also # Convexity in economics Greedy algorithm Non-convexity (economics) Stochastic programming Stochastic dynamic programming","title":"Dynamic-programming"},{"location":"Algorithm-design-paradigm/02-Dynamic-Programming&Greedy-Algorithm/Dynamic-Programming/Dynamic-programming/#dynamic_programming","text":"Dynamic programming is both a mathematical optimization method and a computer programming method. The method was developed by Richard Bellman in the 1950s and has found applications in numerous fields, from aerospace engineering \uff08\u822a\u5929\u5de5\u7a0b\uff09 to economics . In both contexts it refers to simplifying a complicated problem by breaking it down into simpler sub-problems in a recursive manner. While some decision problems \uff08\u51b3\u7b56\u95ee\u9898\uff09 cannot be taken apart this way, decisions that span several points in time do often break apart recursively. Likewise, in computer science, if a problem can be solved optimally by breaking it into sub-problems and then recursively finding the optimal solutions to the sub-problems , then it is said to have optimal substructure \uff08\u6700\u4f18\u5b50\u7ed3\u6784\uff09. If sub-problems can be nested recursively inside larger problems, so that dynamic programming methods are applicable, then there is a relation between the value of the larger problem and the values of the sub-problems.[ 1] In the optimization literature this relationship is called the Bellman equation .","title":"Dynamic programming"},{"location":"Algorithm-design-paradigm/02-Dynamic-Programming&Greedy-Algorithm/Dynamic-Programming/Dynamic-programming/#overview","text":"","title":"Overview"},{"location":"Algorithm-design-paradigm/02-Dynamic-Programming&Greedy-Algorithm/Dynamic-Programming/Dynamic-programming/#mathematical_optimization","text":"In terms of mathematical optimization, dynamic programming usually refers to simplifying a decision by breaking it down into a sequence of decision steps over time. This is done by defining a sequence of value functions V 1, V 2, ..., V n taking y* as an argument representing the state of the system at times i from 1 to n . The definition of *V n ( y ) is the value obtained in state y at the last time n . The values V i at earlier times i = n \u22121, n \u2212 2, ..., 2, 1 can be found by working backwards, using a recursive relationship called the Bellman equation . For i = 2, ..., n , V i \u22121 at any state y is calculated from V i by maximizing a simple function (usually the sum) of the gain from a decision at time i \u2212 1 and the function V i at the new state of the system if this decision is made. Since V i has already been calculated for the needed states, the above operation yields V i \u22121 for those states. Finally, V 1 at the initial state of the system is the value of the optimal solution. The optimal values of the decision variables can be recovered, one by one, by tracking back the calculations already performed.","title":"Mathematical optimization"},{"location":"Algorithm-design-paradigm/02-Dynamic-Programming&Greedy-Algorithm/Dynamic-Programming/Dynamic-programming/#computer_programming","text":"There are two key attributes that a problem must have in order for dynamic programming to be applicable: optimal substructure and overlapping sub-problems . If a problem can be solved by combining optimal solutions to non-overlapping sub-problems, the strategy is called \" divide and conquer \" instead.[ 1] This is why merge sort and quick sort are not classified as dynamic programming problems. SUMMARY : sorting\u5e76\u4e0d\u5177\u5907 overlapping sub-problems \u7684\u7279\u6027\uff1b Optimal substructure means that the solution to a given optimization problem can be obtained by the combination of optimal solutions to its sub-problems . Such optimal substructures are usually described by means of recursion . For example, given a graph G=(V,E) , the shortest path p from a vertex u to a vertex v exhibits optimal substructure : take any intermediate vertex w on this shortest path p . If p is truly the shortest path, then it can be split into sub-paths p1 from u to w and p2 from w to v such that these, in turn, are indeed the shortest paths between the corresponding vertices (by the simple cut-and-paste argument described in Introduction to Algorithms ). Hence, one can easily formulate the solution for finding shortest paths in a recursive manner, which is what the Bellman\u2013Ford algorithm or the Floyd\u2013Warshall algorithm does. Overlapping sub-problems means that the space of sub-problems must be small, that is, any recursive algorithm solving the problem should solve the same sub-problems over and over, rather than generating new sub-problems. For example, consider the recursive formulation for generating the Fibonacci series: $F_i = F_{i\u22121} + F_{i\u22122}$, with base case $F_1 = F_2 = 1$. Then $F_{43} = F_{42} + F_{41}$, and $F_{42} = F_{41} + F_{40}$. Now $F_{41}$ is being solved in the recursive sub-trees of both $F_{43}$ as well as $F_{42}$. Even though the total number of sub-problems is actually small (only 43 of them), we end up solving the same problems over and over if we adopt a naive recursive solution such as this. Dynamic programming takes account of this fact and solves each sub-problem only once. Figure 2. This can be achieved in either of two ways:[ citation needed ] Top-down approach : This is the direct fall-out of the recursive formulation of any problem. If the solution to any problem can be formulated recursively using the solution to its sub-problems, and if its sub-problems are overlapping, then one can easily memoize or store the solutions to the sub-problems in a table. Whenever we attempt to solve a new sub-problem, we first check the table to see if it is already solved. If a solution has been recorded, we can use it directly, otherwise we solve the sub-problem and add its solution to the table. Bottom-up approach : Once we formulate the solution to a problem recursively as in terms of its sub-problems, we can try reformulating the problem in a bottom-up fashion: try solving the sub-problems first and use their solutions to build-on and arrive at solutions to bigger sub-problems. This is also usually done in a tabular form by iteratively generating solutions to bigger and bigger sub-problems by using the solutions to small sub-problems. For example, if we already know the values of F 41 and F 40, we can directly calculate the value of F 42. Some programming languages can automatically memoize the result of a function call with a particular set of arguments, in order to speed up call-by-name evaluation (this mechanism is referred to as call-by-need ). Some languages make it possible portably (e.g. Scheme , Common Lisp , Perl or D ). Some languages have automatic memoization built in, such as tabled Prolog and J , which supports memoization with the M. adverb.[ 4] In any case, this is only possible for a referentially transparent function. Memoization is also encountered as an easily accessible design pattern within term-rewrite based languages such as Wolfram Language .","title":"Computer programming"},{"location":"Algorithm-design-paradigm/02-Dynamic-Programming&Greedy-Algorithm/Dynamic-Programming/Dynamic-programming/#examples_computer_algorithms","text":"","title":"Examples: Computer algorithms"},{"location":"Algorithm-design-paradigm/02-Dynamic-Programming&Greedy-Algorithm/Dynamic-Programming/Dynamic-programming/#dijkstras_algorithm_for_the_shortest_path_problem","text":"","title":"Dijkstra's algorithm for the shortest path problem"},{"location":"Algorithm-design-paradigm/02-Dynamic-Programming&Greedy-Algorithm/Dynamic-Programming/Dynamic-programming/#fibonacci_sequence","text":"","title":"Fibonacci sequence"},{"location":"Algorithm-design-paradigm/02-Dynamic-Programming&Greedy-Algorithm/Dynamic-Programming/Dynamic-programming/#a_type_of_balanced_01_matrix","text":"","title":"A type of balanced 0\u20131 matrix"},{"location":"Algorithm-design-paradigm/02-Dynamic-Programming&Greedy-Algorithm/Dynamic-Programming/Dynamic-programming/#checkerboard","text":"","title":"Checkerboard"},{"location":"Algorithm-design-paradigm/02-Dynamic-Programming&Greedy-Algorithm/Dynamic-Programming/Dynamic-programming/#sequence_alignment","text":"","title":"Sequence alignment"},{"location":"Algorithm-design-paradigm/02-Dynamic-Programming&Greedy-Algorithm/Dynamic-Programming/Dynamic-programming/#tower_of_hanoi_puzzle","text":"","title":"Tower of Hanoi puzzle"},{"location":"Algorithm-design-paradigm/02-Dynamic-Programming&Greedy-Algorithm/Dynamic-Programming/Dynamic-programming/#egg_dropping_puzzle","text":"","title":"Egg dropping puzzle"},{"location":"Algorithm-design-paradigm/02-Dynamic-Programming&Greedy-Algorithm/Dynamic-Programming/Dynamic-programming/#matrix_chain_multiplication","text":"Main article: Matrix chain multiplication","title":"Matrix chain multiplication"},{"location":"Algorithm-design-paradigm/02-Dynamic-Programming&Greedy-Algorithm/Dynamic-Programming/Dynamic-programming/#algorithms_that_use_dynamic_programming","text":"Recurrent solutions to lattice models for protein-DNA binding Backward induction as a solution method for finite-horizon discrete-time dynamic optimization problems Method of undetermined coefficients can be used to solve the Bellman equation in infinite-horizon, discrete-time, discounted , time-invariant dynamic optimization problems Many string algorithms including longest common subsequence , longest increasing subsequence , longest common substring , Levenshtein distance (edit distance) Many algorithmic problems on graphs can be solved efficiently for graphs of bounded treewidth or bounded clique-width by using dynamic programming on a tree decomposition of the graph. The Cocke\u2013Younger\u2013Kasami (CYK) algorithm which determines whether and how a given string can be generated by a given context-free grammar Knuth's word wrapping algorithm that minimizes raggedness when word wrapping text The use of transposition tables and refutation tables in computer chess The Viterbi algorithm (used for hidden Markov models , and particularly in part of speech tagging ) The Earley algorithm (a type of chart parser ) The Needleman\u2013Wunsch algorithm and other algorithms used in bioinformatics , including sequence alignment , structural alignment , RNA structure prediction Floyd's all-pairs shortest path algorithm Optimizing the order for chain matrix multiplication Pseudo-polynomial time algorithms for the subset sum , knapsack and partition problems The dynamic time warping algorithm for computing the global distance between two time series The Selinger (a.k.a. System R ) algorithm for relational database query optimization De Boor algorithm for evaluating B-spline curves Duckworth\u2013Lewis method for resolving the problem when games of cricket are interrupted The value iteration method for solving Markov decision processes Some graphic image edge following selection methods such as the \"magnet\" selection tool in Photoshop Some methods for solving interval scheduling problems Some methods for solving the travelling salesman problem , either exactly (in exponential time ) or approximately (e.g. via the bitonic tour ) Recursive least squares method Beat tracking in music information retrieval Adaptive-critic training strategy for artificial neural networks Stereo algorithms for solving the correspondence problem used in stereo vision Seam carving (content-aware image resizing) The Bellman\u2013Ford algorithm for finding the shortest distance in a graph Some approximate solution methods for the linear search problem Kadane's algorithm for the maximum subarray problem Optimization of electric generation expansion plans in the Wein Automatic System Planning (WASP) package","title":"Algorithms that use dynamic programming"},{"location":"Algorithm-design-paradigm/02-Dynamic-Programming&Greedy-Algorithm/Dynamic-Programming/Dynamic-programming/#see_also","text":"Convexity in economics Greedy algorithm Non-convexity (economics) Stochastic programming Stochastic dynamic programming","title":"See also"},{"location":"Algorithm-design-paradigm/02-Dynamic-Programming&Greedy-Algorithm/Dynamic-Programming/Overlapping-subproblems/","text":"Overlapping subproblems Overlapping subproblems # In computer science , a problem is said to have overlapping subproblems if the problem can be broken down into subproblems which are reused several times or a recursive algorithm for the problem solves the same subproblem over and over rather than always generating new subproblems.[ 1] [ 2] [ 3] For example, the problem of computing the Fibonacci sequence exhibits overlapping subproblems. The problem of computing the n th Fibonacci number F ( n ), can be broken down into the subproblems of computing F ( n \u2212 1) and F ( n \u2212 2), and then adding the two. The subproblem of computing F ( n \u2212 1) can itself be broken down into a subproblem that involves computing F ( n \u2212 2). Therefore, the computation of F ( n \u2212 2) is reused, and the Fibonacci sequence thus exhibits overlapping subproblems. A naive recursive approach to such a problem generally fails due to an exponential complexity . If the problem also shares an optimal substructure property, dynamic programming is a good way to work it out.","title":"Overlapping-subproblems"},{"location":"Algorithm-design-paradigm/02-Dynamic-Programming&Greedy-Algorithm/Dynamic-Programming/Overlapping-subproblems/#overlapping_subproblems","text":"In computer science , a problem is said to have overlapping subproblems if the problem can be broken down into subproblems which are reused several times or a recursive algorithm for the problem solves the same subproblem over and over rather than always generating new subproblems.[ 1] [ 2] [ 3] For example, the problem of computing the Fibonacci sequence exhibits overlapping subproblems. The problem of computing the n th Fibonacci number F ( n ), can be broken down into the subproblems of computing F ( n \u2212 1) and F ( n \u2212 2), and then adding the two. The subproblem of computing F ( n \u2212 1) can itself be broken down into a subproblem that involves computing F ( n \u2212 2). Therefore, the computation of F ( n \u2212 2) is reused, and the Fibonacci sequence thus exhibits overlapping subproblems. A naive recursive approach to such a problem generally fails due to an exponential complexity . If the problem also shares an optimal substructure property, dynamic programming is a good way to work it out.","title":"Overlapping subproblems"},{"location":"Algorithm-design-paradigm/02-Dynamic-Programming&Greedy-Algorithm/Dynamic-Programming/Reading-list/","text":"Reading list # Chapter 6 Dynamic programming https://www.geeksforgeeks.org/top-20-dynamic-programming-interview-questions/","title":"Reading-list"},{"location":"Algorithm-design-paradigm/02-Dynamic-Programming&Greedy-Algorithm/Dynamic-Programming/Reading-list/#reading_list","text":"Chapter 6 Dynamic programming https://www.geeksforgeeks.org/top-20-dynamic-programming-interview-questions/","title":"Reading list"},{"location":"Algorithm-design-paradigm/02-Dynamic-Programming&Greedy-Algorithm/Dynamic-Programming/VS-\u6700\u957f\u516c\u5171\u5b50\u5e8f\u5217-VS-\u77e9\u9635\u8fde\u4e58\u95ee\u9898-VS-\u6700\u5927\u5b50\u6bb5\u548c/","text":"\u524d\u8a00 \u95ee\u9898\uff1a\u77e9\u9635\u8fde\u4e58\u7684\u6700\u4f18\u8ba1\u7b97\u6b21\u5e8f\u95ee\u9898 \u95ee\u9898\u63cf\u8ff0 step1\u5206\u6790\u6700\u4f18\u89e3\u7684\u7ed3\u6784 step2\u5efa\u7acb\u9012\u5f52\u5173\u7cfb step3\u8ba1\u7b97\u6700\u4f18\u503c step4\u6784\u9020\u6700\u4f18\u89e3 \u95ee\u9898\uff1a\u6700\u957f\u516c\u5171\u5b50\u5e8f\u5217 \u95ee\u9898\u63cf\u8ff0 step1\u6700\u957f\u516c\u5171\u5b50\u5e8f\u5217\u7684\u7ed3\u6784\uff08\u5373\u89e3\u7684\u7ed3\u6784\uff09 step2\u5b50\u95ee\u9898\u7684\u9012\u5f52\u7ed3\u6784\uff08\u5373\u5efa\u7acb\u9012\u5f52\u5173\u7cfb\uff09 step3\u8ba1\u7b97\u6700\u4f18\u503c step4\u6784\u9020\u6700\u4f18\u89e3 Q&A \u95ee\u9898\uff1a\u6700\u5927\u5b57\u6bb5\u548c \u95ee\u9898\u63cf\u8ff0 $O(n^3)$\u89e3\u51b3\u7b97\u6cd5 $O(n^2)$\u89e3\u51b3\u7b97\u6cd5 $O(n \\log n)$\u89e3\u51b3\u7b97\u6cd5-\u4e8c\u5206\u6cd5 $O(n)$\u7b97\u6cd5-\u52a8\u6001\u89c4\u5212\u6cd5 summary \u524d\u8a00 # \u5728\u300a\u8ba1\u7b97\u673a\u7b97\u6cd5\u8bbe\u8ba1\u4e0e\u5206\u6790\u300b\u7b2c\u56db\u7248\u738b\u6653\u4e1c\u8457\u7684chapter 3 dynamic programming\u4e2d\u5c06\u8fd9\u4e09\u4e2a\u95ee\u9898\u4f5c\u4e3a\u4f8b\u5b50\u6765\u8bb2\u8ff0\u52a8\u6001\u89c4\u5212\u7b97\u6cd5\uff0c\u4eca\u5929\u5c06\u8fd9\u4e09\u4e2a\u95ee\u9898\u770b\u4e86\u4e00\u4e0b\uff0c\u53d1\u73b0\u8fd9\u4e09\u4e2a\u95ee\u9898\u5176\u5b9e\u662f\u5b58\u5728\u7740\u8fd9\u6837\u7684\u5171\u6027\uff1a\u5bf9 \u5e8f\u5217 \u8fdb\u884c\u64cd\u4f5c\uff1a \u77e9\u9635\u8fde\u4e58\u95ee\u9898\u662f\u5bf9 \u77e9\u9635\u94fe \u8fdb\u884c\u64cd\u4f5c\uff08\u5c06\u77e9\u9635\u94fe\u5206\u5272\u4e3a\u5b50\u94fe\uff09 \u6700\u957f\u516c\u5171\u5b50\u5e8f\u5217\u662f\u5bf9\u4e24\u4e2a\u5e8f\u5217\u8fdb\u884c\u64cd\u4f5c\uff08\u524d\u7f00\uff09 \u6700\u5927\u5b50\u6bb5\u548c\u95ee\u9898\u662f\u5bf9\u4e00\u4e2a\u6570\u4e32\u8fdb\u884c\u64cd\u4f5c \u8fd9\u4e09\u4e2a\u95ee\u9898\u7684\u5206\u6790\u5982\u4e0b\u3002 \u95ee\u9898\uff1a\u77e9\u9635\u8fde\u4e58\u7684\u6700\u4f18\u8ba1\u7b97\u6b21\u5e8f\u95ee\u9898 # \u95ee\u9898\u63cf\u8ff0 # \u77e9\u9635\u8fde\u4e58\u7684\u6700\u4f18\u6b21\u5e8f\u95ee\u9898 \u89e3\u51b3\u7684\u662f\u591a\u4e2a\u77e9\u9635\u8fde\u4e58\u6b21\u6570\u7684\u95ee\u9898\uff0c\u53ef\u4ee5\u5c06\u8fd9\u4e9b\u77e9\u9635\u770b\u505a\u662f\u4e00\u4e2a \u77e9\u9635\u94fe \uff1b\u5bf9\u5b83\u8fdb\u884c\u9012\u5f52\u5c31\u53ef\u4ee5\u5c06\u539f\u957f\u7684\u77e9\u9635\u94fe\u5207\u5272\u4e3a\u4e24\u4e2a \u5b50\u94fe \uff0c\u7136\u540e\u5c06\u4e24\u4e2a \u5b50\u94fe \u76f8\u4e58\uff1b\u4e0d\u540c\u7684\u5207\u5272\u65b9\u6848\u5bfc\u81f4 \u5b50\u94fe \u7684\u957f\u5ea6\u95ee\u9898\uff0c\u4e0d\u540c\u957f\u5ea6\u7684\u5b50\u94fe\u5b83\u4eec\u7684\u4e58\u6cd5\u6b21\u6570\u4e5f\u662f\u4e0d\u540c\u7684\uff1b\u53ef\u4ee5\u6839\u636e\u5b83\u5f97\u5230\u66f4\u957f\u7684 \u77e9\u9635\u94fe \u7684\u4e58\u6cd5\u6b21\u6570\uff1b step1\u5206\u6790\u6700\u4f18\u89e3\u7684\u7ed3\u6784 # \u5c06 \u77e9\u9635\u94fe \u8bb0\u4e3a$A_iA_{i+1} \\dots A_j$\uff0c\u7b80\u8bb0\u4e3a$A[i:j]$\u3002 \u8003\u5bdf\u8ba1\u7b97$A[1:n]$\u7684\u6700\u4f18\u8ba1\u7b97\u6b21\u5e8f\u95ee\u9898\uff1a\u8bbe\u8fd9\u4e2a\u8ba1\u7b97\u6b21\u5e8f\u5728$A_k$\u548c$A_{k+1}$\u4e4b\u95f4\u5c06 \u77e9\u9635\u94fe \u65ad\u5f00\uff0c$1 \\le k \\lt n$\uff0c\u5219\u5176\u76f8\u5e94\u7684\u5b8c\u5168\u52a0\u62ec\u53f7\u65b9\u5f0f\u4e3a$ (A_1 \\dots A_{k}) (A_{k+1} \\ldots A_n)$\u3002\u4f9d\u7167\u6b64\u6b21\u5e8f\uff0c\u5148\u8ba1\u7b97$A[1:k]$\u548c$A[k+1, n]$\uff0c\u7136\u540e\u5c06\u8ba1\u7b97\u7ed3\u679c\u76f8\u4e58\u5f97\u5230$A[1:n]$\u3002 \u8fd9\u4e2a\u95ee\u9898\u7684\u4e00\u4e2a\u5173\u952e\u7279\u5f81\u662f\uff1a\u8ba1\u7b97$A[1:n]$\u6700\u4f18\u6b21\u5e8f\u6240\u5305\u542b\u7684\u8ba1\u7b97\u77e9\u9635\u5b50\u94fe$A[1:k]$\u548c$A[k+1, n]$\u7684\u6b21\u5e8f\u4e5f\u662f\u6700\u4f18\u7684\u3002\u56e0\u6b64\uff0c\u77e9\u9635\u8fde\u4e58\u8ba1\u7b97\u6b21\u5e8f\u95ee\u9898\u7684\u6700\u4f18\u89e3\u8574\u542b\u7740\u5176\u5b50\u95ee\u9898\u7684\u6700\u4f18\u89e3\uff0c\u8fd9\u79cd\u6027\u8d28\u79f0\u4e3a \u6700\u4f18\u5b50\u7ed3\u6784\u6027\u8d28 \u3002 step2\u5efa\u7acb\u9012\u5f52\u5173\u7cfb # \u8bbe\u8ba1 \u52a8\u6001\u89c4\u5212\u7b97\u6cd5 \u7684\u7b2c2\u6b65\u662f\u9012\u5f52\u5730\u5b9a\u4e49 \u6700\u4f18\u503c \u3002\u8bbe$A[i:j], 1 \\le i\\le j \\le n$\uff0c\u6240\u9700\u7684\u6700\u5c11\u4e58\u6cd5\u6b21\u6570\u662f$m[i][j]$\uff0c\u5219\u539f\u95ee\u9898\u7684 \u6700\u4f18\u503c \u4e3a$m[1][n]$\u3002 $$ m[i][j] = \\begin{cases} 0, & i=j \\ \\min\\limits_{i \\le k \\lt j} { m[i][k] + m[k+i][j] + p_{i-1}p_k p_j } & i \\lt j \\end{cases} $$ $m[i][j]$\u7ed9\u51fa\u4e86\u6700\u4f18\u89e3\uff0c\u540c\u65f6\u8fd8\u786e\u5b9a\u4e86$A[i:j]$\u7684\u6700\u4f18\u6b21\u5e8f\u4e2d\u7684\u65ad\u5f00\u4f4d\u7f6e$k$\uff0c\u82e5\u5c06\u5bf9\u5e94\u4e8e$m[i][j]$\u7684\u65ad\u5f00\u4f4d\u7f6e$k$\u8bb0\u4e3a$s[i][j]$\uff0c\u5728\u8ba1\u7b97\u51fa\u6700\u4f18\u503c$m[i][j]$\u540e\uff0c\u53ef\u9012\u5f52\u5730\u7531$s[i][j]$\u6784\u9020\u51fa\u76f8\u5e94\u7684\u6700\u4f18\u89e3\u3002 SUMMARY : $i=j$\u662f\u9012\u5f52\u5173\u7cfb\u4e2d\u7684base case\uff0c\u662f\u9012\u5f52\u7684\u7ec8\u6b62\u6761\u4ef6\uff1b \u5728\u77e9\u9635\u8fde\u4e58\u95ee\u9898\u4e2d\uff0c\u5982\u4f55\u6765\u5b9a\u4e49\u548c\u4fdd\u5b58\u5b50\u95ee\u9898\u7684\u89e3\u5462\uff1f\u4f7f\u7528\u4e8c\u7ef4\u8868\u6765\u4fdd\u5b58\u7684\u662f\u4e0d\u540c\u957f\u5ea6\u7684\u77e9\u9635\u94fe\u7684\u4e58\u6cd5\u6b21\u6570\uff0c\u4e8c\u7ef4\u8868\u7684\u7b2c\u4e00\u7ef4\u662f\u77e9\u9635\u94fe\u7684\u8d77\u59cb\u4f4d\u7f6e\uff0c\u7b2c\u4e8c\u7ef4\u662f\u77e9\u9635\u94fe\u7684\u7ec8\u6b62\u4f4d\u7f6e\uff1b step3\u8ba1\u7b97\u6700\u4f18\u503c # \u8f93\u5165\u53c2\u6570$p_0, p_1, \\dots , p_n$\u5b58\u50a8\u4e8e\u6570\u7ec4$p$\u4e2d\uff0c\u7b97\u6cd5\u9664\u4e86\u8f93\u51fa\u6700\u4f18\u503c\u6570\u7ec4 m \u5916\uff0c\u8fd8\u8f93\u51fa\u8bb0\u5f55\u6700\u4f18\u65ad\u5f00\u4f4d\u7f6e\u7684\u6570\u7ec4 s \u3002 void MatrixChain(int *p, int n, int **m, int **s){ for(int i=0;i<n;++i)m[i][i] = 0; int chain_length = 2;//\u77e9\u9635\u94fe\u7684\u957f\u5ea6 for(;chain_length<=n;++chain_length){ for(int i=0;i<n-chain_length;++i){ j = i+chain_length-1;//\u5b50\u94fe\u7684\u7ec8\u6b62\u4f4d\u7f6e // \u4e0b\u9762\u4f7f\u7528\u6253\u64c2\u53f0\u7684\u65b9\u5f0f\u9009\u51fa\u6700\u4f18\u89e3 m[i][j] = m[i+1][j] + p[i-1]*p[i]*p[j]; s[i][j] = i; for(int k=i+1;k<j;++k){ int t = m[i][k] + m[k+1][j] + p[k-1]*p[k]*p[j]; if(t<m[i][j]){ m[i][j] = t; s[i][j] = k; } } } } } \u65f6\u95f4\u590d\u6742\u5ea6\uff1a$O(n^3)$ \u7a7a\u95f4\u590d\u6742\u5ea6\uff1a$O(n^2)$ step4\u6784\u9020\u6700\u4f18\u89e3 # $s[i][j]$\u8868\u793a\u7684\u662f\u8ba1\u7b97$A[i:j]$\u7684\u6700\u4f73\u65ad\u5f00\u4f4d\u7f6e\uff1b\u6240\u4ee5\u77e9\u9635\u94fe$A[1:n]$\u7684\u6700\u4f73\u65ad\u5f00\u4f4d\u7f6e\u4e3a$s[1][n]$\uff0c\u4ece\u5b83\u53ef\u4ee5\u5f97\u5230\u4e24\u4e2a\u5b50\u94fe$A[1:s[1][n]]$\u3001$A[s[1][n] + 1:n]$\uff0c\u800c\u8fd9\u4e24\u4e2a\u5b50\u94fe\u7684\u65ad\u5f00\u4f4d\u7f6e\u53c8\u53ef\u4ee5\u901a\u8fc7\u67e5\u8868\u5f97\u5230\uff0c\u663e\u7136\u8fd9\u662f\u4e00\u4e2a\u9012\u5f52\u7684\u8fc7\u7a0b\uff0c\u4f7f\u7528\u5982\u4e0b\u51fd\u6570\u53ef\u4ee5\u5b9e\u73b0\u5c06\u89e3\u8f93\u51fa\uff1a void TraceBack(int i, int j, int **s){ if( i == j)return; TraceBack(i, s[i][j], s); TraceBack(s[i][j] + 1, j, s); cout<<\"Multiply A\"<<i<<\",\"<<s[i][j]; cout<<\"and A\"<<(s[i][j]+1)<<\",\"<<j<<endl; } SUMMARY : \u4e8c\u53c9\u6811\u7684\u6df1\u5ea6\u4f18\u5148\u641c\u7d22 \u95ee\u9898\uff1a\u6700\u957f\u516c\u5171\u5b50\u5e8f\u5217 # \u95ee\u9898\u63cf\u8ff0 # \u6700\u957f\u516c\u5171\u5b50\u5e8f\u5217 \u89e3\u51b3\u7684\u662f \u4e24\u4e2a \u5e8f\u5217\u7684\u95ee\u9898\uff0c\u5bf9\u5b83\u8fdb\u884c \u9012\u5f52 \u5c31\u53ef\u4ee5\u5c06\u539f\u95ee\u9898reduce\u5230\u4e24\u4e2a\u5b50\u5e8f\u5217\uff1b\u7531\u4e8e \u52a8\u6001\u89c4\u5212\u7b97\u6cd5 \u662f\u4ece\u53f3\u81f3\u5de6\uff0c\u81ea\u5e95\u5411\u4e0a\u5730\u8fd0\u7528 \u9012\u5f52\u5173\u7cfb \uff0c\u6240\u4ee5\u5b83\u9700\u8981\u9996\u5148\u8ba1\u7b97\u5b50\u95ee\u9898\uff0c\u7136\u540e\u7531\u5b50\u95ee\u9898\u7684\u89e3 \u63a8\u5bfc \u51fa\u66f4\u5927\u7684\u95ee\u9898\u7684\u89e3\uff1b step1\u6700\u957f\u516c\u5171\u5b50\u5e8f\u5217\u7684\u7ed3\u6784\uff08\u5373\u89e3\u7684\u7ed3\u6784\uff09 # \u8bbe\u5e8f\u5217$X={ x_1, x_2, x_3, \\dots ,x_m }$\u548c$Y={ x_1, y_2, y_3, \\dots ,y_n }$\u7684\u6700\u957f\u516c\u5171\u5b50\u5e8f\u5217\u4e3a$Z={z_1, z_2, z_3, \\dots ,z_k}$\uff0c\u5219 \u82e5$x_m = y_n$\uff0c\u5219$z_k = x_m = y_n$\uff0c\u4e14 $Z_{k-1}$\u662f$X_{m-1}$\u548c$Y_{n-1}$\u7684\u6700\u957f\u516c\u5171\u5b50\u5e8f\u5217\u3002 \u82e5$x_m \\ne y_n$\uff0c\u4e14$z_k \\ne x_m$\uff0c\u5219 $Z$\u662f$X_{m-1}$\u548c$Y$\u7684\u6700\u957f\u516c\u5171\u5b50\u5e8f\u5217\u3002 \u82e5$x_m \\ne y_n$\uff0c\u4e14$z_k \\ne y_n$\uff0c\u5219 $Z$\u662f$X$\u548c$Y_{n-1}$\u7684\u6700\u957f\u516c\u5171\u5b50\u5e8f\u5217\u3002 \u7531\u6b64\u53ef\u89c1\uff1a\u4e24\u4e2a\u5e8f\u5217\u7684\u6700\u957f\u516c\u5171\u5b50\u5e8f\u5217\u8574\u542b\u7740\u8fd9\u4e24\u4e2a\u5e8f\u5217\u7684 \u524d\u7f00 \u7684\u6700\u957f\u516c\u5171\u5b50\u5e8f\u5217\uff0c\u7531\u6b64 \u6700\u957f\u516c\u5171\u5b50\u5e8f\u5217\u95ee\u9898 \u5177\u6709 \u6700\u4f18\u5b50\u7ed3\u6784 \u6027\u8d28\u3002 step2\u5b50\u95ee\u9898\u7684\u9012\u5f52\u7ed3\u6784\uff08\u5373\u5efa\u7acb\u9012\u5f52\u5173\u7cfb\uff09 # \u5efa\u7acb\u5b50\u95ee\u9898\u6700\u4f18\u503c\u7684\u9012\u5f52\u5173\u7cfb\uff1a\u7528$c[i][j]$\u8bb0\u5f55\u5e8f\u5217$X_i$\u548c$Y_j$\u7684\u6700\u957f\u516c\u5171\u5b50\u5e8f\u5217\u7684\u957f\u5ea6\uff0c\u5176\u4e2d$X_i= { x_1, x_2, \\dots , x_i }$\uff1b$Y_j = { y_1, y_2, \\dots , y_j }$\u3002\u5f53$i=0$\u6216$j=0$\u65f6\uff0c\u7a7a\u5e8f\u5217\u662f$X_i$\u548c$Y_j$\u7684 \u6700\u957f\u516c\u5171\u5b50\u5e8f\u5217 \uff0c\u6545\u6b64\u65f6$c[i][j]=0$\uff08\u9012\u5f52\u5173\u7cfb\u7684base case\uff09\uff0c\u5176\u4ed6\u60c5\u51b5\u4e0b\uff0c\u6709\u6700\u4f18\u5b50\u7ed3\u6784\u7684\u6027\u8d28\u53ef\u5efa\u7acb\u9012\u5f52\u5173\u7cfb\u5982\u4e0b\uff1a $$ c[i][j]= \\begin{cases} 0 & i=0, j=0 \\ c[i-1][j-1] + 1 & i,j \\gt 0 ; x_i = y_j \\ \\max{ c[i][j-1], c[i-1][j] } i,j \\gt 0 ;x_i \\ne y_j \\end{cases} $$ step3\u8ba1\u7b97\u6700\u4f18\u503c # step4\u6784\u9020\u6700\u4f18\u89e3 # Q&A # Q:\u8ba1\u7b97 \u5b50\u95ee\u9898\u7684\u89e3 \uff0c\u5373\u4e0d\u540c\u957f\u5ea6\u7684\u5b50\u5e8f\u5217 \u5bf9 \u7684\u6700\u957f\u516c\u5171\u5b50\u5e8f\u5217\uff0c\u7a0b\u5e8f\u4e2d\u9700\u8981\u5c06\u6240\u6709\u7684\u4e0d\u540c\u957f\u5ea6\u7684\u5b50\u5e8f\u5217\u7684 \u7ec4\u5408 \u60c5\u51b5\u90fd \u679a\u4e3e \u51fa\u6765\uff0c\u90a3\u4e00\u5171\u6709\u591a\u5c11\u79cd\u7ec4\u5408\u60c5\u51b5\u5462\uff1f\u8fd9\u8fd8\u771f\u662f\u4e00\u4e2a\u6bd4\u8f83\u590d\u6742\u7684\u95ee\u9898\u3002 A:\u4e0a\u9762\u7684\u60f3\u6cd5\u662f\u9519\u8bef\u7684\uff0c\u5728\u8fd9\u4e2a\u95ee\u9898\u4e2d\uff0c\u5b83\u5e76\u4e0d\u9700\u8981\u5c06\u5e8f\u5217A\u7684\u6240\u6709\u5b50\u5e8f\u5217\u548c\u5e8f\u5217B\u7684\u6240\u6709\u5b50\u5e8f\u5217\u8fdb\u884c\u7ec4\u5408\uff0c\u770b\u4e86\u5b83\u7684 \u9012\u5f52\u5173\u7cfb \u4e0e\u7a0b\u5e8f\u7684\u5b9e\u73b0\uff0c\u5b83\u5e76\u6ca1\u6709\u679a\u4e3e\u51fa\u4e24\u4e2a\u5e8f\u5217\u6240\u6709\u7684\u5b50\u5e8f\u5217\u7684\u7ec4\u5408\uff1b \u4ece \u9012\u5f52\u5173\u7cfb \u6765\u770b\uff0c\u5b83\u662f\u4ece\u4e24\u4e2a\u5e8f\u5217\u672b\u7aef\u5373\u6700\u540e\u4e00\u4e2a\u5143\u7d20\u5f00\u59cb\uff0c\u6bcf\u6b21\u5265\u79bb\u4e00\u4e2a\u5b57\u7b26\uff1b \u4ece\u52a8\u6001\u89c4\u5212\u7684\u7a0b\u5e8f\u5b9e\u73b0\u6765\u770b\uff0c\u7531\u4e8e\u5b83\u662f\u4ece\u53f3\u81f3\u5de6\uff08\u81ea\u5e95\u5411\u4e0a\uff09\u5730\u5e94\u7528\u9012\u5f52\u5173\u7cfb\uff0c\u6240\u4ee5\u5b83\u662f\u5e8f\u5217\u7684\u7b2c\u4e00\u4e2a\u5b57\u7b26\u5f00\u59cb\uff0c\u6bcf\u6b21\u6dfb\u52a0\u4e00\u4e2a\u5b57\u7b26\uff0c\u76f4\u81f3\u6784\u5efa\u4e86\u4e00\u4e2a\u5b8c\u6574\u7684\u5e8f\u5217\uff1b \u53e6\u5916\u4e00\u4e2a\u975e\u5e38\u91cd\u8981\u7684\u95ee\u9898\u662f\uff0c\u5982\u4f55\u5c06\u4fdd\u5b58\u5b50\u95ee\u9898\u7684\u89e3\uff0c\u4f7f\u7528\u4ec0\u4e48\u6837\u7684\u6570\u636e\u7ed3\u6784\uff1f\u7531\u4e8e\u5b50\u95ee\u9898\u7684\u89e3\u662f\u4e0d\u540c\u957f\u5ea6\u7684\u5b50\u5e8f\u5217\u7684\u7ec4\u5408\uff0c\u4e00\u4e2a\u4e8c\u7ef4\u8868\u662f\u53ef\u4ee5\u6ee1\u8db3\u5b83\u7684\u9700\u6c42\u7684\uff0c\u5373\u4f7f\u7528\u4e00\u4e2a \u4e8c\u7ef4\u8868 \u6765\u5c06 \u5b50\u95ee\u9898\u7684\u89e3 \u4fdd\u5b58\u8d77\u6765\uff0c\u4e8c\u7ef4\u8868\u7684\u7684\u7b2c\u4e00\u7ef4\u662f\u7b2c\u4e00\u4e2a\u5b50\u5e8f\u5217\u7684\u957f\u5ea6\uff0c\u7b2c\u4e8c\u7ef4\u662f\u7b2c\u4e8c\u4e2a\u5b50\u5e8f\u5217\u7684\u957f\u5ea6\uff0c\u4e8c\u7ef4\u8868\u8bb0\u5f55\u7684\u662f\u8fd9\u79cd\u7ec4\u5408\u4e0b\u5b50\u5e8f\u5217\u7684\u957f\u5ea6\uff1b \u4e0a\u8ff0\u4e24\u4e2a\u95ee\u9898\u90fd\u662f\u5178\u578b\u7684\u5e94\u7528\u52a8\u6001\u89c4\u5212\u662f\u5426\u89e3\u51b3\u5b9e\u9645\u95ee\u9898\u7684\uff0c\u6211\u89c9\u5f97\u6211\u4e0d\u80fd\u591f\u4ec5\u4ec5\u5c40\u9650\u4e8e\u5b83\u4eec\uff0c\u800c\u5e94\u8be5\u5c06\u601d\u7ef4\u6253\u5f00\uff1a - \u9012\u5f52\u5173\u7cfb\u7684\u5efa\u7acb\uff0c\u8fd9\u662f\u89e3\u51b3\u95ee\u9898\u7684\u6839\u672c\u6240\u5728 - \u89e3\u7684\u8868\u793a\u4e0e\u8bb0\u5f55\uff1b\u4e0a\u8ff0\u4e24\u4e2a\u95ee\u9898\u90fd\u662f\u4f7f\u7528\u4e8c\u7ef4\u8868\uff0c\u5176\u5b9e\u5b83\u662f\u7531\u95ee\u9898\u800c\u51b3\u5b9a\u7684\uff0c\u5e76\u4e0d\u4e00\u5b9a\u662f\u6bcf\u79cd\u52a8\u6001\u89c4\u5212\u7b97\u6cd5\u90fd\u9700\u8981\u4f7f\u7528\uff0c\u6211\u60f3\u5bf9\u4e8e\u4e00\u4e9b\u66f4\u52a0\u590d\u6742\u7684\u95ee\u9898\uff0c\u53ef\u80fd\u9700\u8981\u4f7f\u7528\u4e09\u7ef4\u8868\u7b49\uff0c\u6bd4\u5982\u6c42\u4e09\u4e2a\u5b57\u7b26\u4e32\u7684\u6700\u957f\u516c\u5171\u5b50\u5e8f\u5217\uff1b \u5176\u5b9e\u7ecf\u8fc7\u4e0a\u8ff0\u5206\u6790\u53ef\u4ee5\u770b\u51fa\uff0c\u52a8\u6001\u89c4\u5212\u7b97\u6cd5\u5230\u4e86\u6700\u540e\u8fdb\u884c\u5b9e\u73b0\u7684\u65f6\u5019\uff0c\u5b9e\u9645\u4e0a\u662f\u586b\u8868\uff1b \u95ee\u9898\uff1a\u6700\u5927\u5b57\u6bb5\u548c # \u53c2\u8003\uff1a \u6700\u5927\u5b50\u6bb5\u548c\u95ee\u9898\uff1a\u86ee\u529b\u3001\u9012\u5f52\u53ca\u52a8\u6001\u89c4\u5212 \u95ee\u9898\u63cf\u8ff0 # \u7ed9\u7684$n$\u4e2a\u6574\u6570\u7ec4\u6210\u7684\u5e8f\u5217$a_1, a_2, \\dots , a_n$\uff0c\u6c42\u8be5\u5e8f\u5217\u5f62\u5982$\\sum_{k=i}^j {a_k}$\u7684\u5b50\u6bb5\u548c\u7684\u6700\u5927\u503c\u3002\u5f53\u6240\u6709\u6574\u6570\u5747\u4e3a\u8d1f\u6574\u6570\u65f6\uff0c\u5b9a\u4e49\u5176\u6700\u5927\u5b50\u6bb5\u548c\u4e3a0\u3002\u4f9d\u6b21\u5b9a\u4e49\uff0c\u6240\u6c42\u7684\u6700\u4f18\u503c\u4e3a\uff1a $$ \\max{ 0, \\max \\limits_{1 \\le i \\le j \\le n} \\sum_{k=i}^j a_k } $$ SUMMARY : \u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u5b50\u5e8f\u5217\u662f\u8fde\u7eed\u7684\uff0c\u8fd9\u610f\u5473\u4e2d\uff0c\u5728\u9047\u5230\u4e00\u4e2a\u65b0\u7684\u5143\u7d20\u7684\u65f6\u5019\uff0c\u5fc5\u987b\u8981\u5c06\u5b83\u52a0\u5165\u5230\u5b50\u5e8f\u5217\u4e2d\uff0c\u4f46\u662f\u53ef\u9009\u7684\u662f\u5c06\u4e4b\u524d\u7684\u5b50\u5e8f\u5217\u7ed9\u629b\u5f03\u6389\uff1b $O(n^3)$\u89e3\u51b3\u7b97\u6cd5 # \u601d\u60f3 \uff1a\u4ece\u5e8f\u5217\u9996\u5143\u7d20\u5f00\u59cb\u7a77\u4e3e\u6240\u6709\u53ef\u80fd\u7684\u5b50\u5e8f\u5217\u3002 #include<iostream> using namespace std; int MaxSubsequenceSum(const int array[], int n) { int tempSum, maxSum; maxSum = 0; for (int i = 0;i < n;i++) // \u5b50\u5e8f\u5217\u8d77\u59cb\u4f4d\u7f6e { for (int j = i;j < n;j++) // \u5b50\u5e8f\u5217\u7ec8\u6b62\u4f4d\u7f6e { tempSum = 0; for (int k = i;k < j;k++) // \u5b50\u5e8f\u5217\u904d\u5386\u6c42\u548c tempSum += array[k]; if (tempSum > maxSum) // \u66f4\u65b0\u6700\u5927\u548c\u503c maxSum = tempSum; } } return maxSum; } int main() { const int a[] = { 4, -3, 5, -2, -1, 2, 6, -2 }; int maxSubSum = MaxSubsequenceSum(a, 8); cout << \"The max subsequence sum of a is: \" << maxSubSum << endl; system(\"pause\"); return 0; } \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014 \u7248\u6743\u58f0\u660e\uff1a\u672c\u6587\u4e3aCSDN\u535a\u4e3b\u300cSanFanCSgo\u300d\u7684\u539f\u521b\u6587\u7ae0\uff0c\u9075\u5faa CC 4.0 BY-SA \u7248\u6743\u534f\u8bae\uff0c\u8f6c\u8f7d\u8bf7\u9644\u4e0a\u539f\u6587\u51fa\u5904\u94fe\u63a5\u53ca\u672c\u58f0\u660e\u3002 \u539f\u6587\u94fe\u63a5\uff1ahttps://blog.csdn.net/weixin_40170902/article/details/80585218 $O(n^2)$\u89e3\u51b3\u7b97\u6cd5 # \u601d\u60f3\uff1a\u76f4\u63a5\u5728\u5212\u5b9a\u5b50\u5e8f\u5217\u65f6\u7d2f\u52a0\u5143\u7d20\u503c\uff0c\u51cf\u5c11\u4e00\u5c42\u5faa\u73af\u3002 #include<iostream> using namespace std; int MaxSubsequenceSum(const int array[],int n) { int tempSum, maxSum; maxSum = 0; for (int i = 0;i < n;i++) { tempSum = 0; for (int j = i;j < n;j++) { tempSum += array[j]; if (tempSum > maxSum) maxSum = tempSum; } } return maxSum; } int main() { const int a[] = { 4, -3, 5, -2, -1, 2, 6, -2 }; int maxSubSum = MaxSubsequenceSum(a, 8); cout << \"The max subsequence sum of a is: \" << maxSubSum << endl; system(\"pause\"); return 0; } \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014 \u7248\u6743\u58f0\u660e\uff1a\u672c\u6587\u4e3aCSDN\u535a\u4e3b\u300cSanFanCSgo\u300d\u7684\u539f\u521b\u6587\u7ae0\uff0c\u9075\u5faa CC 4.0 BY-SA \u7248\u6743\u534f\u8bae\uff0c\u8f6c\u8f7d\u8bf7\u9644\u4e0a\u539f\u6587\u51fa\u5904\u94fe\u63a5\u53ca\u672c\u58f0\u660e\u3002 \u539f\u6587\u94fe\u63a5\uff1ahttps://blog.csdn.net/weixin_40170902/article/details/80585218 $O(n \\log n)$\u89e3\u51b3\u7b97\u6cd5-\u4e8c\u5206\u6cd5 # $O(n)$\u7b97\u6cd5-\u52a8\u6001\u89c4\u5212\u6cd5 # \u82e5\u8bb0$b[j]=\\max \\limits_{1 \\le i \\le j} { \\sum_{k=i}^{j} a[k] }, 1 \\le j \\le n$\uff0c\u5219\u6240\u6c42\u7684\u6700\u5927\u5b50\u6bb5\u548c\u4e3a $$ \\max \\limits_{1 \\le i \\le j \\le n} \\sum_{k=i}^j a[k] = \\max \\limits_{1 \\le j \\le n} \\max \\limits_{1 \\le i \\le j } \\sum_{k=i}^j a[k] = \\max \\limits_{1 \\le j \\le n} b[j] $$ \u7531$b[j]$\u7684\u5b9a\u4e49\u53ef\u77e5\uff0c\u5f53$b[j-1] \\gt 0 $\u65f6\uff0c$b[j] = b[j-1] + a[j]$\uff0c\u5426\u5219 $b[j] = a[j]$\u3002\u7531\u6b64\u53ef\u77e5\u8ba1\u7b97$b[j]$\u7684\u52a8\u6001\u89c4\u5212\u9012\u5f52\u5f0f\uff1a $$ b[j] = \\max{b[j-1] + a[j], a[j] }, 1 \\le j \\le n $$ SUMMARY : \u9012\u5f52\u5173\u7cfb$b[j] = b[j-1] + a[j]$\u4e0e\u6700\u957f\u516c\u5171\u5b50\u5e8f\u5217\u7684\u975e\u5e38\u7c7b\u4f3c\uff1b$b[j]$\u7684\u8ba1\u7b97\u4ec5\u4ec5\u4f9d\u8d56\u4e8e$b[j-1]$\uff0c\u5c31\u5982 Fibonacci sequence \u7684\u8ba1\u7b97\u4ec5\u4ec5\u4f9d\u8d56\u4e8e\u524d\u4e24\u9879\u4e00\u6837\uff1b SUMMARY : \u6b63\u5982\u5728 \u753b\u89e3\u7b97\u6cd5\uff1a53. \u6700\u5927\u5b50\u5e8f\u548c \u4e2d\u6240\u8bf4\u7684\uff1a $b[j-1] \\gt 0 $ \u8bf4\u660e $b[j-1]$\u5bf9\u7ed3\u679c\u6709\u589e\u76ca\u6548\u679c\uff0c\u5219 $b[j-1]$\u4fdd\u7559\u5e76\u52a0\u4e0a\u5f53\u524d\u904d\u5386\u6570\u5b57 \u5982\u679c$b[j-1] \\le 0$\u5219\u8bf4\u660e\u5b83\u5bf9\u7ed3\u679c\u5e76\u6ca1\u6709\u589e\u76ca\uff0c\u9700\u8981\u820d\u5f03\uff0c \u5219 $b[j]$ \u76f4\u63a5\u66f4\u65b0\u4e3a\u5f53\u524d\u904d\u5386\u6570\u5b57 \u636e\u6b64\uff0c\u53ef\u8bbe\u8ba1\u51fa\u6c42\u6700\u5927\u5b50\u6bb5\u548c\u7684\u52a8\u6001\u89c4\u5212\u7b97\u6cd5\u5982\u4e0b\uff1a int MaxSum(int n, int *a){ int sum = 0, b= 0; int start = 0, end = 0;//\u8bb0\u5f55\u4e0b\u5b50\u5e8f\u5217\u7684\u8d77\u59cb\u548c\u7ec8\u6b62\u4f4d\u7f6e for(int i=1; i <= n; ++i){ if(b>0) b+=a[i]; else { b=a[i]; start = i; } if(b>sum) { sum=b; end = i; } } } \u5728 \u6700\u5927\u5b50\u6bb5\u548c\u95ee\u9898\uff1a\u86ee\u529b\u3001\u9012\u5f52\u53ca\u52a8\u6001\u89c4\u5212 \u4e2d\u7ed9\u51fa\u7684\u7a0b\u5e8f\u662f\u8fd9\u6837\u7684\uff1a #include<iostream> using namespace std; int MaxSubsequenceSum(const int A[], int n) { int tempSum = 0; int maxSum = 0; for (int j = 0;j < n;j++) // \u5b50\u95ee\u9898\u540e\u8fb9\u754c { tempSum = (tempSum + A[j]) > A[j] ? (tempSum + A[j]) : A[j]; if (tempSum > maxSum) // \u66f4\u65b0\u6700\u5927\u548c maxSum = tempSum; } return maxSum; } int main() { const int a[] = { 4, -3, 5, -2, -1, 2, 6, -2 }; int maxSubSum = MaxSubsequenceSum(a, 8); cout << \"The max subsequence sum of a is: \" << maxSubSum << endl; system(\"pause\"); return 0; } \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014 \u7248\u6743\u58f0\u660e\uff1a\u672c\u6587\u4e3aCSDN\u535a\u4e3b\u300cSanFanCSgo\u300d\u7684\u539f\u521b\u6587\u7ae0\uff0c\u9075\u5faa CC 4.0 BY-SA \u7248\u6743\u534f\u8bae\uff0c\u8f6c\u8f7d\u8bf7\u9644\u4e0a\u539f\u6587\u51fa\u5904\u94fe\u63a5\u53ca\u672c\u58f0\u660e\u3002 \u539f\u6587\u94fe\u63a5\uff1ahttps://blog.csdn.net/weixin_40170902/article/details/80585218 \u8fd9\u79cd\u5b9e\u73b0\u548c\u4e0a\u9762\u7684\u90a3\u79cd\u5b9e\u73b0\u662f\u5b8c\u5168\u4e0d\u540c\u7684\uff1b summary # \u4e0a\u8ff0\u7684\u6240\u6709\u7a0b\u5e8f\u90fd\u662f\u5bf9\u89e3\u51b3\u95ee\u9898\u7684\u6570\u5b66\u516c\u5f0f\u7684\u63cf\u8ff0\uff0c\u6240\u4ee5\u5728\u7b97\u6cd5\u9886\u57df\u4e00\u4e2a\u975e\u5e38\u91cd\u8981\u7684\u95ee\u9898\u5c31\u662f\uff1a \u4ece\u6570\u5b66\u516c\u5f0f\u5230\u7a0b\u5e8f","title":"VS-\u6700\u957f\u516c\u5171\u5b50\u5e8f\u5217-VS-\u77e9\u9635\u8fde\u4e58\u95ee\u9898-VS-\u6700\u5927\u5b50\u6bb5\u548c"},{"location":"Algorithm-design-paradigm/02-Dynamic-Programming&Greedy-Algorithm/Dynamic-Programming/VS-\u6700\u957f\u516c\u5171\u5b50\u5e8f\u5217-VS-\u77e9\u9635\u8fde\u4e58\u95ee\u9898-VS-\u6700\u5927\u5b50\u6bb5\u548c/#_1","text":"\u5728\u300a\u8ba1\u7b97\u673a\u7b97\u6cd5\u8bbe\u8ba1\u4e0e\u5206\u6790\u300b\u7b2c\u56db\u7248\u738b\u6653\u4e1c\u8457\u7684chapter 3 dynamic programming\u4e2d\u5c06\u8fd9\u4e09\u4e2a\u95ee\u9898\u4f5c\u4e3a\u4f8b\u5b50\u6765\u8bb2\u8ff0\u52a8\u6001\u89c4\u5212\u7b97\u6cd5\uff0c\u4eca\u5929\u5c06\u8fd9\u4e09\u4e2a\u95ee\u9898\u770b\u4e86\u4e00\u4e0b\uff0c\u53d1\u73b0\u8fd9\u4e09\u4e2a\u95ee\u9898\u5176\u5b9e\u662f\u5b58\u5728\u7740\u8fd9\u6837\u7684\u5171\u6027\uff1a\u5bf9 \u5e8f\u5217 \u8fdb\u884c\u64cd\u4f5c\uff1a \u77e9\u9635\u8fde\u4e58\u95ee\u9898\u662f\u5bf9 \u77e9\u9635\u94fe \u8fdb\u884c\u64cd\u4f5c\uff08\u5c06\u77e9\u9635\u94fe\u5206\u5272\u4e3a\u5b50\u94fe\uff09 \u6700\u957f\u516c\u5171\u5b50\u5e8f\u5217\u662f\u5bf9\u4e24\u4e2a\u5e8f\u5217\u8fdb\u884c\u64cd\u4f5c\uff08\u524d\u7f00\uff09 \u6700\u5927\u5b50\u6bb5\u548c\u95ee\u9898\u662f\u5bf9\u4e00\u4e2a\u6570\u4e32\u8fdb\u884c\u64cd\u4f5c \u8fd9\u4e09\u4e2a\u95ee\u9898\u7684\u5206\u6790\u5982\u4e0b\u3002","title":"\u524d\u8a00"},{"location":"Algorithm-design-paradigm/02-Dynamic-Programming&Greedy-Algorithm/Dynamic-Programming/VS-\u6700\u957f\u516c\u5171\u5b50\u5e8f\u5217-VS-\u77e9\u9635\u8fde\u4e58\u95ee\u9898-VS-\u6700\u5927\u5b50\u6bb5\u548c/#_2","text":"","title":"\u95ee\u9898\uff1a\u77e9\u9635\u8fde\u4e58\u7684\u6700\u4f18\u8ba1\u7b97\u6b21\u5e8f\u95ee\u9898"},{"location":"Algorithm-design-paradigm/02-Dynamic-Programming&Greedy-Algorithm/Dynamic-Programming/VS-\u6700\u957f\u516c\u5171\u5b50\u5e8f\u5217-VS-\u77e9\u9635\u8fde\u4e58\u95ee\u9898-VS-\u6700\u5927\u5b50\u6bb5\u548c/#_3","text":"\u77e9\u9635\u8fde\u4e58\u7684\u6700\u4f18\u6b21\u5e8f\u95ee\u9898 \u89e3\u51b3\u7684\u662f\u591a\u4e2a\u77e9\u9635\u8fde\u4e58\u6b21\u6570\u7684\u95ee\u9898\uff0c\u53ef\u4ee5\u5c06\u8fd9\u4e9b\u77e9\u9635\u770b\u505a\u662f\u4e00\u4e2a \u77e9\u9635\u94fe \uff1b\u5bf9\u5b83\u8fdb\u884c\u9012\u5f52\u5c31\u53ef\u4ee5\u5c06\u539f\u957f\u7684\u77e9\u9635\u94fe\u5207\u5272\u4e3a\u4e24\u4e2a \u5b50\u94fe \uff0c\u7136\u540e\u5c06\u4e24\u4e2a \u5b50\u94fe \u76f8\u4e58\uff1b\u4e0d\u540c\u7684\u5207\u5272\u65b9\u6848\u5bfc\u81f4 \u5b50\u94fe \u7684\u957f\u5ea6\u95ee\u9898\uff0c\u4e0d\u540c\u957f\u5ea6\u7684\u5b50\u94fe\u5b83\u4eec\u7684\u4e58\u6cd5\u6b21\u6570\u4e5f\u662f\u4e0d\u540c\u7684\uff1b\u53ef\u4ee5\u6839\u636e\u5b83\u5f97\u5230\u66f4\u957f\u7684 \u77e9\u9635\u94fe \u7684\u4e58\u6cd5\u6b21\u6570\uff1b","title":"\u95ee\u9898\u63cf\u8ff0"},{"location":"Algorithm-design-paradigm/02-Dynamic-Programming&Greedy-Algorithm/Dynamic-Programming/VS-\u6700\u957f\u516c\u5171\u5b50\u5e8f\u5217-VS-\u77e9\u9635\u8fde\u4e58\u95ee\u9898-VS-\u6700\u5927\u5b50\u6bb5\u548c/#step1","text":"\u5c06 \u77e9\u9635\u94fe \u8bb0\u4e3a$A_iA_{i+1} \\dots A_j$\uff0c\u7b80\u8bb0\u4e3a$A[i:j]$\u3002 \u8003\u5bdf\u8ba1\u7b97$A[1:n]$\u7684\u6700\u4f18\u8ba1\u7b97\u6b21\u5e8f\u95ee\u9898\uff1a\u8bbe\u8fd9\u4e2a\u8ba1\u7b97\u6b21\u5e8f\u5728$A_k$\u548c$A_{k+1}$\u4e4b\u95f4\u5c06 \u77e9\u9635\u94fe \u65ad\u5f00\uff0c$1 \\le k \\lt n$\uff0c\u5219\u5176\u76f8\u5e94\u7684\u5b8c\u5168\u52a0\u62ec\u53f7\u65b9\u5f0f\u4e3a$ (A_1 \\dots A_{k}) (A_{k+1} \\ldots A_n)$\u3002\u4f9d\u7167\u6b64\u6b21\u5e8f\uff0c\u5148\u8ba1\u7b97$A[1:k]$\u548c$A[k+1, n]$\uff0c\u7136\u540e\u5c06\u8ba1\u7b97\u7ed3\u679c\u76f8\u4e58\u5f97\u5230$A[1:n]$\u3002 \u8fd9\u4e2a\u95ee\u9898\u7684\u4e00\u4e2a\u5173\u952e\u7279\u5f81\u662f\uff1a\u8ba1\u7b97$A[1:n]$\u6700\u4f18\u6b21\u5e8f\u6240\u5305\u542b\u7684\u8ba1\u7b97\u77e9\u9635\u5b50\u94fe$A[1:k]$\u548c$A[k+1, n]$\u7684\u6b21\u5e8f\u4e5f\u662f\u6700\u4f18\u7684\u3002\u56e0\u6b64\uff0c\u77e9\u9635\u8fde\u4e58\u8ba1\u7b97\u6b21\u5e8f\u95ee\u9898\u7684\u6700\u4f18\u89e3\u8574\u542b\u7740\u5176\u5b50\u95ee\u9898\u7684\u6700\u4f18\u89e3\uff0c\u8fd9\u79cd\u6027\u8d28\u79f0\u4e3a \u6700\u4f18\u5b50\u7ed3\u6784\u6027\u8d28 \u3002","title":"step1\u5206\u6790\u6700\u4f18\u89e3\u7684\u7ed3\u6784"},{"location":"Algorithm-design-paradigm/02-Dynamic-Programming&Greedy-Algorithm/Dynamic-Programming/VS-\u6700\u957f\u516c\u5171\u5b50\u5e8f\u5217-VS-\u77e9\u9635\u8fde\u4e58\u95ee\u9898-VS-\u6700\u5927\u5b50\u6bb5\u548c/#step2","text":"\u8bbe\u8ba1 \u52a8\u6001\u89c4\u5212\u7b97\u6cd5 \u7684\u7b2c2\u6b65\u662f\u9012\u5f52\u5730\u5b9a\u4e49 \u6700\u4f18\u503c \u3002\u8bbe$A[i:j], 1 \\le i\\le j \\le n$\uff0c\u6240\u9700\u7684\u6700\u5c11\u4e58\u6cd5\u6b21\u6570\u662f$m[i][j]$\uff0c\u5219\u539f\u95ee\u9898\u7684 \u6700\u4f18\u503c \u4e3a$m[1][n]$\u3002 $$ m[i][j] = \\begin{cases} 0, & i=j \\ \\min\\limits_{i \\le k \\lt j} { m[i][k] + m[k+i][j] + p_{i-1}p_k p_j } & i \\lt j \\end{cases} $$ $m[i][j]$\u7ed9\u51fa\u4e86\u6700\u4f18\u89e3\uff0c\u540c\u65f6\u8fd8\u786e\u5b9a\u4e86$A[i:j]$\u7684\u6700\u4f18\u6b21\u5e8f\u4e2d\u7684\u65ad\u5f00\u4f4d\u7f6e$k$\uff0c\u82e5\u5c06\u5bf9\u5e94\u4e8e$m[i][j]$\u7684\u65ad\u5f00\u4f4d\u7f6e$k$\u8bb0\u4e3a$s[i][j]$\uff0c\u5728\u8ba1\u7b97\u51fa\u6700\u4f18\u503c$m[i][j]$\u540e\uff0c\u53ef\u9012\u5f52\u5730\u7531$s[i][j]$\u6784\u9020\u51fa\u76f8\u5e94\u7684\u6700\u4f18\u89e3\u3002 SUMMARY : $i=j$\u662f\u9012\u5f52\u5173\u7cfb\u4e2d\u7684base case\uff0c\u662f\u9012\u5f52\u7684\u7ec8\u6b62\u6761\u4ef6\uff1b \u5728\u77e9\u9635\u8fde\u4e58\u95ee\u9898\u4e2d\uff0c\u5982\u4f55\u6765\u5b9a\u4e49\u548c\u4fdd\u5b58\u5b50\u95ee\u9898\u7684\u89e3\u5462\uff1f\u4f7f\u7528\u4e8c\u7ef4\u8868\u6765\u4fdd\u5b58\u7684\u662f\u4e0d\u540c\u957f\u5ea6\u7684\u77e9\u9635\u94fe\u7684\u4e58\u6cd5\u6b21\u6570\uff0c\u4e8c\u7ef4\u8868\u7684\u7b2c\u4e00\u7ef4\u662f\u77e9\u9635\u94fe\u7684\u8d77\u59cb\u4f4d\u7f6e\uff0c\u7b2c\u4e8c\u7ef4\u662f\u77e9\u9635\u94fe\u7684\u7ec8\u6b62\u4f4d\u7f6e\uff1b","title":"step2\u5efa\u7acb\u9012\u5f52\u5173\u7cfb"},{"location":"Algorithm-design-paradigm/02-Dynamic-Programming&Greedy-Algorithm/Dynamic-Programming/VS-\u6700\u957f\u516c\u5171\u5b50\u5e8f\u5217-VS-\u77e9\u9635\u8fde\u4e58\u95ee\u9898-VS-\u6700\u5927\u5b50\u6bb5\u548c/#step3","text":"\u8f93\u5165\u53c2\u6570$p_0, p_1, \\dots , p_n$\u5b58\u50a8\u4e8e\u6570\u7ec4$p$\u4e2d\uff0c\u7b97\u6cd5\u9664\u4e86\u8f93\u51fa\u6700\u4f18\u503c\u6570\u7ec4 m \u5916\uff0c\u8fd8\u8f93\u51fa\u8bb0\u5f55\u6700\u4f18\u65ad\u5f00\u4f4d\u7f6e\u7684\u6570\u7ec4 s \u3002 void MatrixChain(int *p, int n, int **m, int **s){ for(int i=0;i<n;++i)m[i][i] = 0; int chain_length = 2;//\u77e9\u9635\u94fe\u7684\u957f\u5ea6 for(;chain_length<=n;++chain_length){ for(int i=0;i<n-chain_length;++i){ j = i+chain_length-1;//\u5b50\u94fe\u7684\u7ec8\u6b62\u4f4d\u7f6e // \u4e0b\u9762\u4f7f\u7528\u6253\u64c2\u53f0\u7684\u65b9\u5f0f\u9009\u51fa\u6700\u4f18\u89e3 m[i][j] = m[i+1][j] + p[i-1]*p[i]*p[j]; s[i][j] = i; for(int k=i+1;k<j;++k){ int t = m[i][k] + m[k+1][j] + p[k-1]*p[k]*p[j]; if(t<m[i][j]){ m[i][j] = t; s[i][j] = k; } } } } } \u65f6\u95f4\u590d\u6742\u5ea6\uff1a$O(n^3)$ \u7a7a\u95f4\u590d\u6742\u5ea6\uff1a$O(n^2)$","title":"step3\u8ba1\u7b97\u6700\u4f18\u503c"},{"location":"Algorithm-design-paradigm/02-Dynamic-Programming&Greedy-Algorithm/Dynamic-Programming/VS-\u6700\u957f\u516c\u5171\u5b50\u5e8f\u5217-VS-\u77e9\u9635\u8fde\u4e58\u95ee\u9898-VS-\u6700\u5927\u5b50\u6bb5\u548c/#step4","text":"$s[i][j]$\u8868\u793a\u7684\u662f\u8ba1\u7b97$A[i:j]$\u7684\u6700\u4f73\u65ad\u5f00\u4f4d\u7f6e\uff1b\u6240\u4ee5\u77e9\u9635\u94fe$A[1:n]$\u7684\u6700\u4f73\u65ad\u5f00\u4f4d\u7f6e\u4e3a$s[1][n]$\uff0c\u4ece\u5b83\u53ef\u4ee5\u5f97\u5230\u4e24\u4e2a\u5b50\u94fe$A[1:s[1][n]]$\u3001$A[s[1][n] + 1:n]$\uff0c\u800c\u8fd9\u4e24\u4e2a\u5b50\u94fe\u7684\u65ad\u5f00\u4f4d\u7f6e\u53c8\u53ef\u4ee5\u901a\u8fc7\u67e5\u8868\u5f97\u5230\uff0c\u663e\u7136\u8fd9\u662f\u4e00\u4e2a\u9012\u5f52\u7684\u8fc7\u7a0b\uff0c\u4f7f\u7528\u5982\u4e0b\u51fd\u6570\u53ef\u4ee5\u5b9e\u73b0\u5c06\u89e3\u8f93\u51fa\uff1a void TraceBack(int i, int j, int **s){ if( i == j)return; TraceBack(i, s[i][j], s); TraceBack(s[i][j] + 1, j, s); cout<<\"Multiply A\"<<i<<\",\"<<s[i][j]; cout<<\"and A\"<<(s[i][j]+1)<<\",\"<<j<<endl; } SUMMARY : \u4e8c\u53c9\u6811\u7684\u6df1\u5ea6\u4f18\u5148\u641c\u7d22","title":"step4\u6784\u9020\u6700\u4f18\u89e3"},{"location":"Algorithm-design-paradigm/02-Dynamic-Programming&Greedy-Algorithm/Dynamic-Programming/VS-\u6700\u957f\u516c\u5171\u5b50\u5e8f\u5217-VS-\u77e9\u9635\u8fde\u4e58\u95ee\u9898-VS-\u6700\u5927\u5b50\u6bb5\u548c/#_4","text":"","title":"\u95ee\u9898\uff1a\u6700\u957f\u516c\u5171\u5b50\u5e8f\u5217"},{"location":"Algorithm-design-paradigm/02-Dynamic-Programming&Greedy-Algorithm/Dynamic-Programming/VS-\u6700\u957f\u516c\u5171\u5b50\u5e8f\u5217-VS-\u77e9\u9635\u8fde\u4e58\u95ee\u9898-VS-\u6700\u5927\u5b50\u6bb5\u548c/#_5","text":"\u6700\u957f\u516c\u5171\u5b50\u5e8f\u5217 \u89e3\u51b3\u7684\u662f \u4e24\u4e2a \u5e8f\u5217\u7684\u95ee\u9898\uff0c\u5bf9\u5b83\u8fdb\u884c \u9012\u5f52 \u5c31\u53ef\u4ee5\u5c06\u539f\u95ee\u9898reduce\u5230\u4e24\u4e2a\u5b50\u5e8f\u5217\uff1b\u7531\u4e8e \u52a8\u6001\u89c4\u5212\u7b97\u6cd5 \u662f\u4ece\u53f3\u81f3\u5de6\uff0c\u81ea\u5e95\u5411\u4e0a\u5730\u8fd0\u7528 \u9012\u5f52\u5173\u7cfb \uff0c\u6240\u4ee5\u5b83\u9700\u8981\u9996\u5148\u8ba1\u7b97\u5b50\u95ee\u9898\uff0c\u7136\u540e\u7531\u5b50\u95ee\u9898\u7684\u89e3 \u63a8\u5bfc \u51fa\u66f4\u5927\u7684\u95ee\u9898\u7684\u89e3\uff1b","title":"\u95ee\u9898\u63cf\u8ff0"},{"location":"Algorithm-design-paradigm/02-Dynamic-Programming&Greedy-Algorithm/Dynamic-Programming/VS-\u6700\u957f\u516c\u5171\u5b50\u5e8f\u5217-VS-\u77e9\u9635\u8fde\u4e58\u95ee\u9898-VS-\u6700\u5927\u5b50\u6bb5\u548c/#step1_1","text":"\u8bbe\u5e8f\u5217$X={ x_1, x_2, x_3, \\dots ,x_m }$\u548c$Y={ x_1, y_2, y_3, \\dots ,y_n }$\u7684\u6700\u957f\u516c\u5171\u5b50\u5e8f\u5217\u4e3a$Z={z_1, z_2, z_3, \\dots ,z_k}$\uff0c\u5219 \u82e5$x_m = y_n$\uff0c\u5219$z_k = x_m = y_n$\uff0c\u4e14 $Z_{k-1}$\u662f$X_{m-1}$\u548c$Y_{n-1}$\u7684\u6700\u957f\u516c\u5171\u5b50\u5e8f\u5217\u3002 \u82e5$x_m \\ne y_n$\uff0c\u4e14$z_k \\ne x_m$\uff0c\u5219 $Z$\u662f$X_{m-1}$\u548c$Y$\u7684\u6700\u957f\u516c\u5171\u5b50\u5e8f\u5217\u3002 \u82e5$x_m \\ne y_n$\uff0c\u4e14$z_k \\ne y_n$\uff0c\u5219 $Z$\u662f$X$\u548c$Y_{n-1}$\u7684\u6700\u957f\u516c\u5171\u5b50\u5e8f\u5217\u3002 \u7531\u6b64\u53ef\u89c1\uff1a\u4e24\u4e2a\u5e8f\u5217\u7684\u6700\u957f\u516c\u5171\u5b50\u5e8f\u5217\u8574\u542b\u7740\u8fd9\u4e24\u4e2a\u5e8f\u5217\u7684 \u524d\u7f00 \u7684\u6700\u957f\u516c\u5171\u5b50\u5e8f\u5217\uff0c\u7531\u6b64 \u6700\u957f\u516c\u5171\u5b50\u5e8f\u5217\u95ee\u9898 \u5177\u6709 \u6700\u4f18\u5b50\u7ed3\u6784 \u6027\u8d28\u3002","title":"step1\u6700\u957f\u516c\u5171\u5b50\u5e8f\u5217\u7684\u7ed3\u6784\uff08\u5373\u89e3\u7684\u7ed3\u6784\uff09"},{"location":"Algorithm-design-paradigm/02-Dynamic-Programming&Greedy-Algorithm/Dynamic-Programming/VS-\u6700\u957f\u516c\u5171\u5b50\u5e8f\u5217-VS-\u77e9\u9635\u8fde\u4e58\u95ee\u9898-VS-\u6700\u5927\u5b50\u6bb5\u548c/#step2_1","text":"\u5efa\u7acb\u5b50\u95ee\u9898\u6700\u4f18\u503c\u7684\u9012\u5f52\u5173\u7cfb\uff1a\u7528$c[i][j]$\u8bb0\u5f55\u5e8f\u5217$X_i$\u548c$Y_j$\u7684\u6700\u957f\u516c\u5171\u5b50\u5e8f\u5217\u7684\u957f\u5ea6\uff0c\u5176\u4e2d$X_i= { x_1, x_2, \\dots , x_i }$\uff1b$Y_j = { y_1, y_2, \\dots , y_j }$\u3002\u5f53$i=0$\u6216$j=0$\u65f6\uff0c\u7a7a\u5e8f\u5217\u662f$X_i$\u548c$Y_j$\u7684 \u6700\u957f\u516c\u5171\u5b50\u5e8f\u5217 \uff0c\u6545\u6b64\u65f6$c[i][j]=0$\uff08\u9012\u5f52\u5173\u7cfb\u7684base case\uff09\uff0c\u5176\u4ed6\u60c5\u51b5\u4e0b\uff0c\u6709\u6700\u4f18\u5b50\u7ed3\u6784\u7684\u6027\u8d28\u53ef\u5efa\u7acb\u9012\u5f52\u5173\u7cfb\u5982\u4e0b\uff1a $$ c[i][j]= \\begin{cases} 0 & i=0, j=0 \\ c[i-1][j-1] + 1 & i,j \\gt 0 ; x_i = y_j \\ \\max{ c[i][j-1], c[i-1][j] } i,j \\gt 0 ;x_i \\ne y_j \\end{cases} $$","title":"step2\u5b50\u95ee\u9898\u7684\u9012\u5f52\u7ed3\u6784\uff08\u5373\u5efa\u7acb\u9012\u5f52\u5173\u7cfb\uff09"},{"location":"Algorithm-design-paradigm/02-Dynamic-Programming&Greedy-Algorithm/Dynamic-Programming/VS-\u6700\u957f\u516c\u5171\u5b50\u5e8f\u5217-VS-\u77e9\u9635\u8fde\u4e58\u95ee\u9898-VS-\u6700\u5927\u5b50\u6bb5\u548c/#step3_1","text":"","title":"step3\u8ba1\u7b97\u6700\u4f18\u503c"},{"location":"Algorithm-design-paradigm/02-Dynamic-Programming&Greedy-Algorithm/Dynamic-Programming/VS-\u6700\u957f\u516c\u5171\u5b50\u5e8f\u5217-VS-\u77e9\u9635\u8fde\u4e58\u95ee\u9898-VS-\u6700\u5927\u5b50\u6bb5\u548c/#step4_1","text":"","title":"step4\u6784\u9020\u6700\u4f18\u89e3"},{"location":"Algorithm-design-paradigm/02-Dynamic-Programming&Greedy-Algorithm/Dynamic-Programming/VS-\u6700\u957f\u516c\u5171\u5b50\u5e8f\u5217-VS-\u77e9\u9635\u8fde\u4e58\u95ee\u9898-VS-\u6700\u5927\u5b50\u6bb5\u548c/#qa","text":"Q:\u8ba1\u7b97 \u5b50\u95ee\u9898\u7684\u89e3 \uff0c\u5373\u4e0d\u540c\u957f\u5ea6\u7684\u5b50\u5e8f\u5217 \u5bf9 \u7684\u6700\u957f\u516c\u5171\u5b50\u5e8f\u5217\uff0c\u7a0b\u5e8f\u4e2d\u9700\u8981\u5c06\u6240\u6709\u7684\u4e0d\u540c\u957f\u5ea6\u7684\u5b50\u5e8f\u5217\u7684 \u7ec4\u5408 \u60c5\u51b5\u90fd \u679a\u4e3e \u51fa\u6765\uff0c\u90a3\u4e00\u5171\u6709\u591a\u5c11\u79cd\u7ec4\u5408\u60c5\u51b5\u5462\uff1f\u8fd9\u8fd8\u771f\u662f\u4e00\u4e2a\u6bd4\u8f83\u590d\u6742\u7684\u95ee\u9898\u3002 A:\u4e0a\u9762\u7684\u60f3\u6cd5\u662f\u9519\u8bef\u7684\uff0c\u5728\u8fd9\u4e2a\u95ee\u9898\u4e2d\uff0c\u5b83\u5e76\u4e0d\u9700\u8981\u5c06\u5e8f\u5217A\u7684\u6240\u6709\u5b50\u5e8f\u5217\u548c\u5e8f\u5217B\u7684\u6240\u6709\u5b50\u5e8f\u5217\u8fdb\u884c\u7ec4\u5408\uff0c\u770b\u4e86\u5b83\u7684 \u9012\u5f52\u5173\u7cfb \u4e0e\u7a0b\u5e8f\u7684\u5b9e\u73b0\uff0c\u5b83\u5e76\u6ca1\u6709\u679a\u4e3e\u51fa\u4e24\u4e2a\u5e8f\u5217\u6240\u6709\u7684\u5b50\u5e8f\u5217\u7684\u7ec4\u5408\uff1b \u4ece \u9012\u5f52\u5173\u7cfb \u6765\u770b\uff0c\u5b83\u662f\u4ece\u4e24\u4e2a\u5e8f\u5217\u672b\u7aef\u5373\u6700\u540e\u4e00\u4e2a\u5143\u7d20\u5f00\u59cb\uff0c\u6bcf\u6b21\u5265\u79bb\u4e00\u4e2a\u5b57\u7b26\uff1b \u4ece\u52a8\u6001\u89c4\u5212\u7684\u7a0b\u5e8f\u5b9e\u73b0\u6765\u770b\uff0c\u7531\u4e8e\u5b83\u662f\u4ece\u53f3\u81f3\u5de6\uff08\u81ea\u5e95\u5411\u4e0a\uff09\u5730\u5e94\u7528\u9012\u5f52\u5173\u7cfb\uff0c\u6240\u4ee5\u5b83\u662f\u5e8f\u5217\u7684\u7b2c\u4e00\u4e2a\u5b57\u7b26\u5f00\u59cb\uff0c\u6bcf\u6b21\u6dfb\u52a0\u4e00\u4e2a\u5b57\u7b26\uff0c\u76f4\u81f3\u6784\u5efa\u4e86\u4e00\u4e2a\u5b8c\u6574\u7684\u5e8f\u5217\uff1b \u53e6\u5916\u4e00\u4e2a\u975e\u5e38\u91cd\u8981\u7684\u95ee\u9898\u662f\uff0c\u5982\u4f55\u5c06\u4fdd\u5b58\u5b50\u95ee\u9898\u7684\u89e3\uff0c\u4f7f\u7528\u4ec0\u4e48\u6837\u7684\u6570\u636e\u7ed3\u6784\uff1f\u7531\u4e8e\u5b50\u95ee\u9898\u7684\u89e3\u662f\u4e0d\u540c\u957f\u5ea6\u7684\u5b50\u5e8f\u5217\u7684\u7ec4\u5408\uff0c\u4e00\u4e2a\u4e8c\u7ef4\u8868\u662f\u53ef\u4ee5\u6ee1\u8db3\u5b83\u7684\u9700\u6c42\u7684\uff0c\u5373\u4f7f\u7528\u4e00\u4e2a \u4e8c\u7ef4\u8868 \u6765\u5c06 \u5b50\u95ee\u9898\u7684\u89e3 \u4fdd\u5b58\u8d77\u6765\uff0c\u4e8c\u7ef4\u8868\u7684\u7684\u7b2c\u4e00\u7ef4\u662f\u7b2c\u4e00\u4e2a\u5b50\u5e8f\u5217\u7684\u957f\u5ea6\uff0c\u7b2c\u4e8c\u7ef4\u662f\u7b2c\u4e8c\u4e2a\u5b50\u5e8f\u5217\u7684\u957f\u5ea6\uff0c\u4e8c\u7ef4\u8868\u8bb0\u5f55\u7684\u662f\u8fd9\u79cd\u7ec4\u5408\u4e0b\u5b50\u5e8f\u5217\u7684\u957f\u5ea6\uff1b \u4e0a\u8ff0\u4e24\u4e2a\u95ee\u9898\u90fd\u662f\u5178\u578b\u7684\u5e94\u7528\u52a8\u6001\u89c4\u5212\u662f\u5426\u89e3\u51b3\u5b9e\u9645\u95ee\u9898\u7684\uff0c\u6211\u89c9\u5f97\u6211\u4e0d\u80fd\u591f\u4ec5\u4ec5\u5c40\u9650\u4e8e\u5b83\u4eec\uff0c\u800c\u5e94\u8be5\u5c06\u601d\u7ef4\u6253\u5f00\uff1a - \u9012\u5f52\u5173\u7cfb\u7684\u5efa\u7acb\uff0c\u8fd9\u662f\u89e3\u51b3\u95ee\u9898\u7684\u6839\u672c\u6240\u5728 - \u89e3\u7684\u8868\u793a\u4e0e\u8bb0\u5f55\uff1b\u4e0a\u8ff0\u4e24\u4e2a\u95ee\u9898\u90fd\u662f\u4f7f\u7528\u4e8c\u7ef4\u8868\uff0c\u5176\u5b9e\u5b83\u662f\u7531\u95ee\u9898\u800c\u51b3\u5b9a\u7684\uff0c\u5e76\u4e0d\u4e00\u5b9a\u662f\u6bcf\u79cd\u52a8\u6001\u89c4\u5212\u7b97\u6cd5\u90fd\u9700\u8981\u4f7f\u7528\uff0c\u6211\u60f3\u5bf9\u4e8e\u4e00\u4e9b\u66f4\u52a0\u590d\u6742\u7684\u95ee\u9898\uff0c\u53ef\u80fd\u9700\u8981\u4f7f\u7528\u4e09\u7ef4\u8868\u7b49\uff0c\u6bd4\u5982\u6c42\u4e09\u4e2a\u5b57\u7b26\u4e32\u7684\u6700\u957f\u516c\u5171\u5b50\u5e8f\u5217\uff1b \u5176\u5b9e\u7ecf\u8fc7\u4e0a\u8ff0\u5206\u6790\u53ef\u4ee5\u770b\u51fa\uff0c\u52a8\u6001\u89c4\u5212\u7b97\u6cd5\u5230\u4e86\u6700\u540e\u8fdb\u884c\u5b9e\u73b0\u7684\u65f6\u5019\uff0c\u5b9e\u9645\u4e0a\u662f\u586b\u8868\uff1b","title":"Q&amp;A"},{"location":"Algorithm-design-paradigm/02-Dynamic-Programming&Greedy-Algorithm/Dynamic-Programming/VS-\u6700\u957f\u516c\u5171\u5b50\u5e8f\u5217-VS-\u77e9\u9635\u8fde\u4e58\u95ee\u9898-VS-\u6700\u5927\u5b50\u6bb5\u548c/#_6","text":"\u53c2\u8003\uff1a \u6700\u5927\u5b50\u6bb5\u548c\u95ee\u9898\uff1a\u86ee\u529b\u3001\u9012\u5f52\u53ca\u52a8\u6001\u89c4\u5212","title":"\u95ee\u9898\uff1a\u6700\u5927\u5b57\u6bb5\u548c"},{"location":"Algorithm-design-paradigm/02-Dynamic-Programming&Greedy-Algorithm/Dynamic-Programming/VS-\u6700\u957f\u516c\u5171\u5b50\u5e8f\u5217-VS-\u77e9\u9635\u8fde\u4e58\u95ee\u9898-VS-\u6700\u5927\u5b50\u6bb5\u548c/#_7","text":"\u7ed9\u7684$n$\u4e2a\u6574\u6570\u7ec4\u6210\u7684\u5e8f\u5217$a_1, a_2, \\dots , a_n$\uff0c\u6c42\u8be5\u5e8f\u5217\u5f62\u5982$\\sum_{k=i}^j {a_k}$\u7684\u5b50\u6bb5\u548c\u7684\u6700\u5927\u503c\u3002\u5f53\u6240\u6709\u6574\u6570\u5747\u4e3a\u8d1f\u6574\u6570\u65f6\uff0c\u5b9a\u4e49\u5176\u6700\u5927\u5b50\u6bb5\u548c\u4e3a0\u3002\u4f9d\u6b21\u5b9a\u4e49\uff0c\u6240\u6c42\u7684\u6700\u4f18\u503c\u4e3a\uff1a $$ \\max{ 0, \\max \\limits_{1 \\le i \\le j \\le n} \\sum_{k=i}^j a_k } $$ SUMMARY : \u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u5b50\u5e8f\u5217\u662f\u8fde\u7eed\u7684\uff0c\u8fd9\u610f\u5473\u4e2d\uff0c\u5728\u9047\u5230\u4e00\u4e2a\u65b0\u7684\u5143\u7d20\u7684\u65f6\u5019\uff0c\u5fc5\u987b\u8981\u5c06\u5b83\u52a0\u5165\u5230\u5b50\u5e8f\u5217\u4e2d\uff0c\u4f46\u662f\u53ef\u9009\u7684\u662f\u5c06\u4e4b\u524d\u7684\u5b50\u5e8f\u5217\u7ed9\u629b\u5f03\u6389\uff1b","title":"\u95ee\u9898\u63cf\u8ff0"},{"location":"Algorithm-design-paradigm/02-Dynamic-Programming&Greedy-Algorithm/Dynamic-Programming/VS-\u6700\u957f\u516c\u5171\u5b50\u5e8f\u5217-VS-\u77e9\u9635\u8fde\u4e58\u95ee\u9898-VS-\u6700\u5927\u5b50\u6bb5\u548c/#on3","text":"\u601d\u60f3 \uff1a\u4ece\u5e8f\u5217\u9996\u5143\u7d20\u5f00\u59cb\u7a77\u4e3e\u6240\u6709\u53ef\u80fd\u7684\u5b50\u5e8f\u5217\u3002 #include<iostream> using namespace std; int MaxSubsequenceSum(const int array[], int n) { int tempSum, maxSum; maxSum = 0; for (int i = 0;i < n;i++) // \u5b50\u5e8f\u5217\u8d77\u59cb\u4f4d\u7f6e { for (int j = i;j < n;j++) // \u5b50\u5e8f\u5217\u7ec8\u6b62\u4f4d\u7f6e { tempSum = 0; for (int k = i;k < j;k++) // \u5b50\u5e8f\u5217\u904d\u5386\u6c42\u548c tempSum += array[k]; if (tempSum > maxSum) // \u66f4\u65b0\u6700\u5927\u548c\u503c maxSum = tempSum; } } return maxSum; } int main() { const int a[] = { 4, -3, 5, -2, -1, 2, 6, -2 }; int maxSubSum = MaxSubsequenceSum(a, 8); cout << \"The max subsequence sum of a is: \" << maxSubSum << endl; system(\"pause\"); return 0; } \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014 \u7248\u6743\u58f0\u660e\uff1a\u672c\u6587\u4e3aCSDN\u535a\u4e3b\u300cSanFanCSgo\u300d\u7684\u539f\u521b\u6587\u7ae0\uff0c\u9075\u5faa CC 4.0 BY-SA \u7248\u6743\u534f\u8bae\uff0c\u8f6c\u8f7d\u8bf7\u9644\u4e0a\u539f\u6587\u51fa\u5904\u94fe\u63a5\u53ca\u672c\u58f0\u660e\u3002 \u539f\u6587\u94fe\u63a5\uff1ahttps://blog.csdn.net/weixin_40170902/article/details/80585218","title":"$O(n^3)$\u89e3\u51b3\u7b97\u6cd5"},{"location":"Algorithm-design-paradigm/02-Dynamic-Programming&Greedy-Algorithm/Dynamic-Programming/VS-\u6700\u957f\u516c\u5171\u5b50\u5e8f\u5217-VS-\u77e9\u9635\u8fde\u4e58\u95ee\u9898-VS-\u6700\u5927\u5b50\u6bb5\u548c/#on2","text":"\u601d\u60f3\uff1a\u76f4\u63a5\u5728\u5212\u5b9a\u5b50\u5e8f\u5217\u65f6\u7d2f\u52a0\u5143\u7d20\u503c\uff0c\u51cf\u5c11\u4e00\u5c42\u5faa\u73af\u3002 #include<iostream> using namespace std; int MaxSubsequenceSum(const int array[],int n) { int tempSum, maxSum; maxSum = 0; for (int i = 0;i < n;i++) { tempSum = 0; for (int j = i;j < n;j++) { tempSum += array[j]; if (tempSum > maxSum) maxSum = tempSum; } } return maxSum; } int main() { const int a[] = { 4, -3, 5, -2, -1, 2, 6, -2 }; int maxSubSum = MaxSubsequenceSum(a, 8); cout << \"The max subsequence sum of a is: \" << maxSubSum << endl; system(\"pause\"); return 0; } \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014 \u7248\u6743\u58f0\u660e\uff1a\u672c\u6587\u4e3aCSDN\u535a\u4e3b\u300cSanFanCSgo\u300d\u7684\u539f\u521b\u6587\u7ae0\uff0c\u9075\u5faa CC 4.0 BY-SA \u7248\u6743\u534f\u8bae\uff0c\u8f6c\u8f7d\u8bf7\u9644\u4e0a\u539f\u6587\u51fa\u5904\u94fe\u63a5\u53ca\u672c\u58f0\u660e\u3002 \u539f\u6587\u94fe\u63a5\uff1ahttps://blog.csdn.net/weixin_40170902/article/details/80585218","title":"$O(n^2)$\u89e3\u51b3\u7b97\u6cd5"},{"location":"Algorithm-design-paradigm/02-Dynamic-Programming&Greedy-Algorithm/Dynamic-Programming/VS-\u6700\u957f\u516c\u5171\u5b50\u5e8f\u5217-VS-\u77e9\u9635\u8fde\u4e58\u95ee\u9898-VS-\u6700\u5927\u5b50\u6bb5\u548c/#on_log_n-","text":"","title":"$O(n \\log n)$\u89e3\u51b3\u7b97\u6cd5-\u4e8c\u5206\u6cd5"},{"location":"Algorithm-design-paradigm/02-Dynamic-Programming&Greedy-Algorithm/Dynamic-Programming/VS-\u6700\u957f\u516c\u5171\u5b50\u5e8f\u5217-VS-\u77e9\u9635\u8fde\u4e58\u95ee\u9898-VS-\u6700\u5927\u5b50\u6bb5\u548c/#on-","text":"\u82e5\u8bb0$b[j]=\\max \\limits_{1 \\le i \\le j} { \\sum_{k=i}^{j} a[k] }, 1 \\le j \\le n$\uff0c\u5219\u6240\u6c42\u7684\u6700\u5927\u5b50\u6bb5\u548c\u4e3a $$ \\max \\limits_{1 \\le i \\le j \\le n} \\sum_{k=i}^j a[k] = \\max \\limits_{1 \\le j \\le n} \\max \\limits_{1 \\le i \\le j } \\sum_{k=i}^j a[k] = \\max \\limits_{1 \\le j \\le n} b[j] $$ \u7531$b[j]$\u7684\u5b9a\u4e49\u53ef\u77e5\uff0c\u5f53$b[j-1] \\gt 0 $\u65f6\uff0c$b[j] = b[j-1] + a[j]$\uff0c\u5426\u5219 $b[j] = a[j]$\u3002\u7531\u6b64\u53ef\u77e5\u8ba1\u7b97$b[j]$\u7684\u52a8\u6001\u89c4\u5212\u9012\u5f52\u5f0f\uff1a $$ b[j] = \\max{b[j-1] + a[j], a[j] }, 1 \\le j \\le n $$ SUMMARY : \u9012\u5f52\u5173\u7cfb$b[j] = b[j-1] + a[j]$\u4e0e\u6700\u957f\u516c\u5171\u5b50\u5e8f\u5217\u7684\u975e\u5e38\u7c7b\u4f3c\uff1b$b[j]$\u7684\u8ba1\u7b97\u4ec5\u4ec5\u4f9d\u8d56\u4e8e$b[j-1]$\uff0c\u5c31\u5982 Fibonacci sequence \u7684\u8ba1\u7b97\u4ec5\u4ec5\u4f9d\u8d56\u4e8e\u524d\u4e24\u9879\u4e00\u6837\uff1b SUMMARY : \u6b63\u5982\u5728 \u753b\u89e3\u7b97\u6cd5\uff1a53. \u6700\u5927\u5b50\u5e8f\u548c \u4e2d\u6240\u8bf4\u7684\uff1a $b[j-1] \\gt 0 $ \u8bf4\u660e $b[j-1]$\u5bf9\u7ed3\u679c\u6709\u589e\u76ca\u6548\u679c\uff0c\u5219 $b[j-1]$\u4fdd\u7559\u5e76\u52a0\u4e0a\u5f53\u524d\u904d\u5386\u6570\u5b57 \u5982\u679c$b[j-1] \\le 0$\u5219\u8bf4\u660e\u5b83\u5bf9\u7ed3\u679c\u5e76\u6ca1\u6709\u589e\u76ca\uff0c\u9700\u8981\u820d\u5f03\uff0c \u5219 $b[j]$ \u76f4\u63a5\u66f4\u65b0\u4e3a\u5f53\u524d\u904d\u5386\u6570\u5b57 \u636e\u6b64\uff0c\u53ef\u8bbe\u8ba1\u51fa\u6c42\u6700\u5927\u5b50\u6bb5\u548c\u7684\u52a8\u6001\u89c4\u5212\u7b97\u6cd5\u5982\u4e0b\uff1a int MaxSum(int n, int *a){ int sum = 0, b= 0; int start = 0, end = 0;//\u8bb0\u5f55\u4e0b\u5b50\u5e8f\u5217\u7684\u8d77\u59cb\u548c\u7ec8\u6b62\u4f4d\u7f6e for(int i=1; i <= n; ++i){ if(b>0) b+=a[i]; else { b=a[i]; start = i; } if(b>sum) { sum=b; end = i; } } } \u5728 \u6700\u5927\u5b50\u6bb5\u548c\u95ee\u9898\uff1a\u86ee\u529b\u3001\u9012\u5f52\u53ca\u52a8\u6001\u89c4\u5212 \u4e2d\u7ed9\u51fa\u7684\u7a0b\u5e8f\u662f\u8fd9\u6837\u7684\uff1a #include<iostream> using namespace std; int MaxSubsequenceSum(const int A[], int n) { int tempSum = 0; int maxSum = 0; for (int j = 0;j < n;j++) // \u5b50\u95ee\u9898\u540e\u8fb9\u754c { tempSum = (tempSum + A[j]) > A[j] ? (tempSum + A[j]) : A[j]; if (tempSum > maxSum) // \u66f4\u65b0\u6700\u5927\u548c maxSum = tempSum; } return maxSum; } int main() { const int a[] = { 4, -3, 5, -2, -1, 2, 6, -2 }; int maxSubSum = MaxSubsequenceSum(a, 8); cout << \"The max subsequence sum of a is: \" << maxSubSum << endl; system(\"pause\"); return 0; } \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014 \u7248\u6743\u58f0\u660e\uff1a\u672c\u6587\u4e3aCSDN\u535a\u4e3b\u300cSanFanCSgo\u300d\u7684\u539f\u521b\u6587\u7ae0\uff0c\u9075\u5faa CC 4.0 BY-SA \u7248\u6743\u534f\u8bae\uff0c\u8f6c\u8f7d\u8bf7\u9644\u4e0a\u539f\u6587\u51fa\u5904\u94fe\u63a5\u53ca\u672c\u58f0\u660e\u3002 \u539f\u6587\u94fe\u63a5\uff1ahttps://blog.csdn.net/weixin_40170902/article/details/80585218 \u8fd9\u79cd\u5b9e\u73b0\u548c\u4e0a\u9762\u7684\u90a3\u79cd\u5b9e\u73b0\u662f\u5b8c\u5168\u4e0d\u540c\u7684\uff1b","title":"$O(n)$\u7b97\u6cd5-\u52a8\u6001\u89c4\u5212\u6cd5"},{"location":"Algorithm-design-paradigm/02-Dynamic-Programming&Greedy-Algorithm/Dynamic-Programming/VS-\u6700\u957f\u516c\u5171\u5b50\u5e8f\u5217-VS-\u77e9\u9635\u8fde\u4e58\u95ee\u9898-VS-\u6700\u5927\u5b50\u6bb5\u548c/#summary","text":"\u4e0a\u8ff0\u7684\u6240\u6709\u7a0b\u5e8f\u90fd\u662f\u5bf9\u89e3\u51b3\u95ee\u9898\u7684\u6570\u5b66\u516c\u5f0f\u7684\u63cf\u8ff0\uff0c\u6240\u4ee5\u5728\u7b97\u6cd5\u9886\u57df\u4e00\u4e2a\u975e\u5e38\u91cd\u8981\u7684\u95ee\u9898\u5c31\u662f\uff1a \u4ece\u6570\u5b66\u516c\u5f0f\u5230\u7a0b\u5e8f","title":"summary"},{"location":"Algorithm-design-paradigm/02-Dynamic-Programming&Greedy-Algorithm/Greedy-algorithm/Greedy-algorithm/","text":"Greedy algorithm #","title":"Greedy-algorithm"},{"location":"Algorithm-design-paradigm/02-Dynamic-Programming&Greedy-Algorithm/Greedy-algorithm/Greedy-algorithm/#greedy_algorithm","text":"","title":"Greedy algorithm"},{"location":"Algorithm-design-paradigm/02-Dynamic-Programming&Greedy-Algorithm/Greedy-algorithm/Huffman-Coding/","text":"Huffman Coding | Greedy Algo-3 # Prefix Codes , means the codes (bit sequences) are assigned in such a way that the code assigned to one character is not the prefix of code assigned to any other character. This is how Huffman Coding makes sure that there is no ambiguity when decoding the generated bitstream. Let us understand prefix codes with a counter example. Let there be four characters a , b , c and d , and their corresponding variable length codes be 00 , 01 , 0 and 1 . This coding leads to ambiguity because code assigned to c is the prefix of codes assigned to a and b . If the compressed bit stream is 0001 , the de-compressed output may be \u201ccccd\u201d or \u201cccb\u201d or \u201cacd\u201d or \u201cab\u201d.See this for applications of Huffman Coding. There are mainly two major parts in Huffman Coding 1) Build a Huffman Tree from input characters. 2) Traverse the Huffman Tree and assign codes to characters. Steps to build Huffman Tree # Input is an array of unique characters along with their frequency of occurrences and output is Huffman Tree. 1. Create a leaf node for each unique character and build a min heap of all leaf nodes (Min Heap is used as a priority queue . The value of frequency field is used to compare two nodes in min heap . Initially, the least frequent character is at root) 2. Extract two nodes with the minimum frequency from the min heap. 3. Create a new internal node with a frequency equal to the sum of the two nodes frequencies. Make the first extracted node as its left child and the other extracted node as its right child. Add this node to the min heap. 4. Repeat steps#2 and #3 until the heap contains only one node. The remaining node is the root node and the tree is complete. Let us understand the algorithm with an example: // C program for Huffman Coding #include <stdio.h> #include <stdlib.h> // This constant can be avoided by explicitly // calculating height of Huffman Tree #define MAX_TREE_HT 100 // A Huffman tree node struct MinHeapNode { // One of the input characters char data; // Frequency of the character unsigned freq; // Left and right child of this node struct MinHeapNode *left, *right; }; // A Min Heap: Collection of // min-heap (or Huffman tree) nodes struct MinHeap { // Current size of min heap unsigned size; // capacity of min heap unsigned capacity; // Array of minheap node pointers struct MinHeapNode** array; }; // A utility function allocate a new // min heap node with given character // and frequency of the character struct MinHeapNode* newNode(char data, unsigned freq) { struct MinHeapNode* temp = (struct MinHeapNode*)malloc (sizeof(struct MinHeapNode)); temp->left = temp->right = NULL; temp->data = data; temp->freq = freq; return temp; } // A utility function to create // a min heap of given capacity struct MinHeap* createMinHeap(unsigned capacity) { struct MinHeap* minHeap = (struct MinHeap*)malloc(sizeof(struct MinHeap)); // current size is 0 minHeap->size = 0; minHeap->capacity = capacity; minHeap->array = (struct MinHeapNode**)malloc(minHeap-> capacity * sizeof(struct MinHeapNode*)); return minHeap; } // A utility function to // swap two min heap nodes void swapMinHeapNode(struct MinHeapNode** a, struct MinHeapNode** b) { struct MinHeapNode* t = *a; *a = *b; *b = t; } // The standard minHeapify function. void minHeapify(struct MinHeap* minHeap, int idx) { int smallest = idx; int left = 2 * idx + 1; int right = 2 * idx + 2; if (left < minHeap->size && minHeap->array[left]-> freq < minHeap->array[smallest]->freq) smallest = left; if (right < minHeap->size && minHeap->array[right]-> freq < minHeap->array[smallest]->freq) smallest = right; if (smallest != idx) { swapMinHeapNode(&minHeap->array[smallest], &minHeap->array[idx]); minHeapify(minHeap, smallest); } } // A utility function to check // if size of heap is 1 or not int isSizeOne(struct MinHeap* minHeap) { return (minHeap->size == 1); } // A standard function to extract // minimum value node from heap struct MinHeapNode* extractMin(struct MinHeap* minHeap) { struct MinHeapNode* temp = minHeap->array[0]; minHeap->array[0] = minHeap->array[minHeap->size - 1]; --minHeap->size; minHeapify(minHeap, 0); return temp; } // A utility function to insert // a new node to Min Heap void insertMinHeap(struct MinHeap* minHeap, struct MinHeapNode* minHeapNode) { ++minHeap->size; int i = minHeap->size - 1; while (i && minHeapNode->freq < minHeap->array[(i - 1) / 2]->freq) { minHeap->array[i] = minHeap->array[(i - 1) / 2]; i = (i - 1) / 2; } minHeap->array[i] = minHeapNode; } // A standard function to build min heap void buildMinHeap(struct MinHeap* minHeap) { int n = minHeap->size - 1; int i; for (i = (n - 1) / 2; i >= 0; --i) minHeapify(minHeap, i); } // A utility function to print an array of size n void printArr(int arr[], int n) { int i; for (i = 0; i < n; ++i) printf(\"%d\", arr[i]); printf(\"\\n\"); } // Utility function to check if this node is leaf int isLeaf(struct MinHeapNode* root) { return !(root->left) && !(root->right); } // Creates a min heap of capacity // equal to size and inserts all character of // data[] in min heap. Initially size of // min heap is equal to capacity struct MinHeap* createAndBuildMinHeap(char data[], int freq[], int size) { struct MinHeap* minHeap = createMinHeap(size); for (int i = 0; i < size; ++i) minHeap->array[i] = newNode(data[i], freq[i]); minHeap->size = size; buildMinHeap(minHeap); return minHeap; } // The main function that builds Huffman tree struct MinHeapNode* buildHuffmanTree(char data[], int freq[], int size) { struct MinHeapNode *left, *right, *top; // Step 1: Create a min heap of capacity // equal to size. Initially, there are // modes equal to size. struct MinHeap* minHeap = createAndBuildMinHeap(data, freq, size); // Iterate while size of heap doesn't become 1 while (!isSizeOne(minHeap)) { // Step 2: Extract the two minimum // freq items from min heap left = extractMin(minHeap); right = extractMin(minHeap); // Step 3: Create a new internal // node with frequency equal to the // sum of the two nodes frequencies. // Make the two extracted node as // left and right children of this new node. // Add this node to the min heap // '$' is a special value for internal nodes, not used top = newNode('$', left->freq + right->freq); top->left = left; top->right = right; insertMinHeap(minHeap, top); } // Step 4: The remaining node is the // root node and the tree is complete. return extractMin(minHeap); } // Prints huffman codes from the root of Huffman Tree. // It uses arr[] to store codes void printCodes(struct MinHeapNode* root, int arr[], int top) { // Assign 0 to left edge and recur if (root->left) { arr[top] = 0; printCodes(root->left, arr, top + 1); } // Assign 1 to right edge and recur if (root->right) { arr[top] = 1; printCodes(root->right, arr, top + 1); } // If this is a leaf node, then // it contains one of the input // characters, print the character // and its code from arr[] if (isLeaf(root)) { printf(\"%c: \", root->data); printArr(arr, top); } } // The main function that builds a // Huffman Tree and print codes by traversing // the built Huffman Tree void HuffmanCodes(char data[], int freq[], int size) { // Construct Huffman Tree struct MinHeapNode* root = buildHuffmanTree(data, freq, size); // Print Huffman codes using // the Huffman tree built above int arr[MAX_TREE_HT], top = 0; printCodes(root, arr, top); } // Driver program to test above functions int main() { char arr[] = { 'a', 'b', 'c', 'd', 'e', 'f' }; int freq[] = { 5, 9, 12, 13, 16, 45 }; int size = sizeof(arr) / sizeof(arr[0]); HuffmanCodes(arr, freq, size); return 0; } // C++ program for Huffman Coding #include <iostream> #include <cstdlib> using namespace std; // This constant can be avoided by explicitly // calculating height of Huffman Tree #define MAX_TREE_HT 100 // A Huffman tree node struct MinHeapNode { // One of the input characters char data; // Frequency of the character unsigned freq; // Left and right child of this node struct MinHeapNode *left, *right; }; // A Min Heap: Collection of // min-heap (or Huffman tree) nodes struct MinHeap { // Current size of min heap unsigned size; // capacity of min heap unsigned capacity; // Attay of minheap node pointers struct MinHeapNode** array; }; // A utility function allocate a new // min heap node with given character // and frequency of the character struct MinHeapNode* newNode(char data, unsigned freq) { struct MinHeapNode* temp = (struct MinHeapNode*)malloc (sizeof(struct MinHeapNode)); temp->left = temp->right = NULL; temp->data = data; temp->freq = freq; return temp; } // A utility function to create // a min heap of given capacity struct MinHeap* createMinHeap(unsigned capacity) { struct MinHeap* minHeap = (struct MinHeap*)malloc(sizeof(struct MinHeap)); // current size is 0 minHeap->size = 0; minHeap->capacity = capacity; minHeap->array = (struct MinHeapNode**)malloc(minHeap-> capacity * sizeof(struct MinHeapNode*)); return minHeap; } // A utility function to // swap two min heap nodes void swapMinHeapNode(struct MinHeapNode** a, struct MinHeapNode** b) { struct MinHeapNode* t = *a; *a = *b; *b = t; } // The standard minHeapify function. void minHeapify(struct MinHeap* minHeap, int idx) { int smallest = idx; int left = 2 * idx + 1; int right = 2 * idx + 2; if (left < minHeap->size && minHeap->array[left]-> freq < minHeap->array[smallest]->freq) smallest = left; if (right < minHeap->size && minHeap->array[right]-> freq < minHeap->array[smallest]->freq) smallest = right; if (smallest != idx) { swapMinHeapNode(&minHeap->array[smallest], &minHeap->array[idx]); minHeapify(minHeap, smallest); } } // A utility function to check // if size of heap is 1 or not int isSizeOne(struct MinHeap* minHeap) { return (minHeap->size == 1); } // A standard function to extract // minimum value node from heap struct MinHeapNode* extractMin(struct MinHeap* minHeap) { struct MinHeapNode* temp = minHeap->array[0]; minHeap->array[0] = minHeap->array[minHeap->size - 1]; --minHeap->size; minHeapify(minHeap, 0); return temp; } // A utility function to insert // a new node to Min Heap void insertMinHeap(struct MinHeap* minHeap, struct MinHeapNode* minHeapNode) { ++minHeap->size; int i = minHeap->size - 1; while (i && minHeapNode->freq < minHeap->array[(i - 1) / 2]->freq) { minHeap->array[i] = minHeap->array[(i - 1) / 2]; i = (i - 1) / 2; } minHeap->array[i] = minHeapNode; } // A standard function to build min heap void buildMinHeap(struct MinHeap* minHeap) { int n = minHeap->size - 1; int i; for (i = (n - 1) / 2; i >= 0; --i) minHeapify(minHeap, i); } // A utility function to print an array of size n void printArr(int arr[], int n) { int i; for (i = 0; i < n; ++i) cout<< arr[i]; cout<<\"\\n\"; } // Utility function to check if this node is leaf int isLeaf(struct MinHeapNode* root) { return !(root->left) && !(root->right); } // Creates a min heap of capacity // equal to size and inserts all character of // data[] in min heap. Initially size of // min heap is equal to capacity struct MinHeap* createAndBuildMinHeap(char data[], int freq[], int size) { struct MinHeap* minHeap = createMinHeap(size); for (int i = 0; i < size; ++i) minHeap->array[i] = newNode(data[i], freq[i]); minHeap->size = size; buildMinHeap(minHeap); return minHeap; } // The main function that builds Huffman tree struct MinHeapNode* buildHuffmanTree(char data[], int freq[], int size) { struct MinHeapNode *left, *right, *top; // Step 1: Create a min heap of capacity // equal to size. Initially, there are // modes equal to size. struct MinHeap* minHeap = createAndBuildMinHeap(data, freq, size); // Iterate while size of heap doesn't become 1 while (!isSizeOne(minHeap)) { // Step 2: Extract the two minimum // freq items from min heap left = extractMin(minHeap); right = extractMin(minHeap); // Step 3: Create a new internal // node with frequency equal to the // sum of the two nodes frequencies. // Make the two extracted node as // left and right children of this new node. // Add this node to the min heap // '$' is a special value for internal nodes, not used top = newNode('$', left->freq + right->freq); top->left = left; top->right = right; insertMinHeap(minHeap, top); } // Step 4: The remaining node is the // root node and the tree is complete. return extractMin(minHeap); } // Prints huffman codes from the root of Huffman Tree. // It uses arr[] to store codes void printCodes(struct MinHeapNode* root, int arr[], int top) { // Assign 0 to left edge and recur if (root->left) { arr[top] = 0; printCodes(root->left, arr, top + 1); } // Assign 1 to right edge and recur if (root->right) { arr[top] = 1; printCodes(root->right, arr, top + 1); } // If this is a leaf node, then // it contains one of the input // characters, print the character // and its code from arr[] if (isLeaf(root)) { cout<< root->data <<\": \"; printArr(arr, top); } } // The main function that builds a // Huffman Tree and print codes by traversing // the built Huffman Tree void HuffmanCodes(char data[], int freq[], int size) { // Construct Huffman Tree struct MinHeapNode* root = buildHuffmanTree(data, freq, size); // Print Huffman codes using // the Huffman tree built above int arr[MAX_TREE_HT], top = 0; printCodes(root, arr, top); } // Driver program to test above functions int main() { char arr[] = { 'a', 'b', 'c', 'd', 'e', 'f' }; int freq[] = { 5, 9, 12, 13, 16, 45 }; int size = sizeof(arr) / sizeof(arr[0]); HuffmanCodes(arr, freq, size); return 0; } Huffman-tree-vs-trie #","title":"Huffman-Coding"},{"location":"Algorithm-design-paradigm/02-Dynamic-Programming&Greedy-Algorithm/Greedy-algorithm/Huffman-Coding/#huffman_coding_greedy_algo-3","text":"Prefix Codes , means the codes (bit sequences) are assigned in such a way that the code assigned to one character is not the prefix of code assigned to any other character. This is how Huffman Coding makes sure that there is no ambiguity when decoding the generated bitstream. Let us understand prefix codes with a counter example. Let there be four characters a , b , c and d , and their corresponding variable length codes be 00 , 01 , 0 and 1 . This coding leads to ambiguity because code assigned to c is the prefix of codes assigned to a and b . If the compressed bit stream is 0001 , the de-compressed output may be \u201ccccd\u201d or \u201cccb\u201d or \u201cacd\u201d or \u201cab\u201d.See this for applications of Huffman Coding. There are mainly two major parts in Huffman Coding 1) Build a Huffman Tree from input characters. 2) Traverse the Huffman Tree and assign codes to characters.","title":"Huffman Coding | Greedy Algo-3"},{"location":"Algorithm-design-paradigm/02-Dynamic-Programming&Greedy-Algorithm/Greedy-algorithm/Huffman-Coding/#steps_to_build_huffman_tree","text":"Input is an array of unique characters along with their frequency of occurrences and output is Huffman Tree. 1. Create a leaf node for each unique character and build a min heap of all leaf nodes (Min Heap is used as a priority queue . The value of frequency field is used to compare two nodes in min heap . Initially, the least frequent character is at root) 2. Extract two nodes with the minimum frequency from the min heap. 3. Create a new internal node with a frequency equal to the sum of the two nodes frequencies. Make the first extracted node as its left child and the other extracted node as its right child. Add this node to the min heap. 4. Repeat steps#2 and #3 until the heap contains only one node. The remaining node is the root node and the tree is complete. Let us understand the algorithm with an example: // C program for Huffman Coding #include <stdio.h> #include <stdlib.h> // This constant can be avoided by explicitly // calculating height of Huffman Tree #define MAX_TREE_HT 100 // A Huffman tree node struct MinHeapNode { // One of the input characters char data; // Frequency of the character unsigned freq; // Left and right child of this node struct MinHeapNode *left, *right; }; // A Min Heap: Collection of // min-heap (or Huffman tree) nodes struct MinHeap { // Current size of min heap unsigned size; // capacity of min heap unsigned capacity; // Array of minheap node pointers struct MinHeapNode** array; }; // A utility function allocate a new // min heap node with given character // and frequency of the character struct MinHeapNode* newNode(char data, unsigned freq) { struct MinHeapNode* temp = (struct MinHeapNode*)malloc (sizeof(struct MinHeapNode)); temp->left = temp->right = NULL; temp->data = data; temp->freq = freq; return temp; } // A utility function to create // a min heap of given capacity struct MinHeap* createMinHeap(unsigned capacity) { struct MinHeap* minHeap = (struct MinHeap*)malloc(sizeof(struct MinHeap)); // current size is 0 minHeap->size = 0; minHeap->capacity = capacity; minHeap->array = (struct MinHeapNode**)malloc(minHeap-> capacity * sizeof(struct MinHeapNode*)); return minHeap; } // A utility function to // swap two min heap nodes void swapMinHeapNode(struct MinHeapNode** a, struct MinHeapNode** b) { struct MinHeapNode* t = *a; *a = *b; *b = t; } // The standard minHeapify function. void minHeapify(struct MinHeap* minHeap, int idx) { int smallest = idx; int left = 2 * idx + 1; int right = 2 * idx + 2; if (left < minHeap->size && minHeap->array[left]-> freq < minHeap->array[smallest]->freq) smallest = left; if (right < minHeap->size && minHeap->array[right]-> freq < minHeap->array[smallest]->freq) smallest = right; if (smallest != idx) { swapMinHeapNode(&minHeap->array[smallest], &minHeap->array[idx]); minHeapify(minHeap, smallest); } } // A utility function to check // if size of heap is 1 or not int isSizeOne(struct MinHeap* minHeap) { return (minHeap->size == 1); } // A standard function to extract // minimum value node from heap struct MinHeapNode* extractMin(struct MinHeap* minHeap) { struct MinHeapNode* temp = minHeap->array[0]; minHeap->array[0] = minHeap->array[minHeap->size - 1]; --minHeap->size; minHeapify(minHeap, 0); return temp; } // A utility function to insert // a new node to Min Heap void insertMinHeap(struct MinHeap* minHeap, struct MinHeapNode* minHeapNode) { ++minHeap->size; int i = minHeap->size - 1; while (i && minHeapNode->freq < minHeap->array[(i - 1) / 2]->freq) { minHeap->array[i] = minHeap->array[(i - 1) / 2]; i = (i - 1) / 2; } minHeap->array[i] = minHeapNode; } // A standard function to build min heap void buildMinHeap(struct MinHeap* minHeap) { int n = minHeap->size - 1; int i; for (i = (n - 1) / 2; i >= 0; --i) minHeapify(minHeap, i); } // A utility function to print an array of size n void printArr(int arr[], int n) { int i; for (i = 0; i < n; ++i) printf(\"%d\", arr[i]); printf(\"\\n\"); } // Utility function to check if this node is leaf int isLeaf(struct MinHeapNode* root) { return !(root->left) && !(root->right); } // Creates a min heap of capacity // equal to size and inserts all character of // data[] in min heap. Initially size of // min heap is equal to capacity struct MinHeap* createAndBuildMinHeap(char data[], int freq[], int size) { struct MinHeap* minHeap = createMinHeap(size); for (int i = 0; i < size; ++i) minHeap->array[i] = newNode(data[i], freq[i]); minHeap->size = size; buildMinHeap(minHeap); return minHeap; } // The main function that builds Huffman tree struct MinHeapNode* buildHuffmanTree(char data[], int freq[], int size) { struct MinHeapNode *left, *right, *top; // Step 1: Create a min heap of capacity // equal to size. Initially, there are // modes equal to size. struct MinHeap* minHeap = createAndBuildMinHeap(data, freq, size); // Iterate while size of heap doesn't become 1 while (!isSizeOne(minHeap)) { // Step 2: Extract the two minimum // freq items from min heap left = extractMin(minHeap); right = extractMin(minHeap); // Step 3: Create a new internal // node with frequency equal to the // sum of the two nodes frequencies. // Make the two extracted node as // left and right children of this new node. // Add this node to the min heap // '$' is a special value for internal nodes, not used top = newNode('$', left->freq + right->freq); top->left = left; top->right = right; insertMinHeap(minHeap, top); } // Step 4: The remaining node is the // root node and the tree is complete. return extractMin(minHeap); } // Prints huffman codes from the root of Huffman Tree. // It uses arr[] to store codes void printCodes(struct MinHeapNode* root, int arr[], int top) { // Assign 0 to left edge and recur if (root->left) { arr[top] = 0; printCodes(root->left, arr, top + 1); } // Assign 1 to right edge and recur if (root->right) { arr[top] = 1; printCodes(root->right, arr, top + 1); } // If this is a leaf node, then // it contains one of the input // characters, print the character // and its code from arr[] if (isLeaf(root)) { printf(\"%c: \", root->data); printArr(arr, top); } } // The main function that builds a // Huffman Tree and print codes by traversing // the built Huffman Tree void HuffmanCodes(char data[], int freq[], int size) { // Construct Huffman Tree struct MinHeapNode* root = buildHuffmanTree(data, freq, size); // Print Huffman codes using // the Huffman tree built above int arr[MAX_TREE_HT], top = 0; printCodes(root, arr, top); } // Driver program to test above functions int main() { char arr[] = { 'a', 'b', 'c', 'd', 'e', 'f' }; int freq[] = { 5, 9, 12, 13, 16, 45 }; int size = sizeof(arr) / sizeof(arr[0]); HuffmanCodes(arr, freq, size); return 0; } // C++ program for Huffman Coding #include <iostream> #include <cstdlib> using namespace std; // This constant can be avoided by explicitly // calculating height of Huffman Tree #define MAX_TREE_HT 100 // A Huffman tree node struct MinHeapNode { // One of the input characters char data; // Frequency of the character unsigned freq; // Left and right child of this node struct MinHeapNode *left, *right; }; // A Min Heap: Collection of // min-heap (or Huffman tree) nodes struct MinHeap { // Current size of min heap unsigned size; // capacity of min heap unsigned capacity; // Attay of minheap node pointers struct MinHeapNode** array; }; // A utility function allocate a new // min heap node with given character // and frequency of the character struct MinHeapNode* newNode(char data, unsigned freq) { struct MinHeapNode* temp = (struct MinHeapNode*)malloc (sizeof(struct MinHeapNode)); temp->left = temp->right = NULL; temp->data = data; temp->freq = freq; return temp; } // A utility function to create // a min heap of given capacity struct MinHeap* createMinHeap(unsigned capacity) { struct MinHeap* minHeap = (struct MinHeap*)malloc(sizeof(struct MinHeap)); // current size is 0 minHeap->size = 0; minHeap->capacity = capacity; minHeap->array = (struct MinHeapNode**)malloc(minHeap-> capacity * sizeof(struct MinHeapNode*)); return minHeap; } // A utility function to // swap two min heap nodes void swapMinHeapNode(struct MinHeapNode** a, struct MinHeapNode** b) { struct MinHeapNode* t = *a; *a = *b; *b = t; } // The standard minHeapify function. void minHeapify(struct MinHeap* minHeap, int idx) { int smallest = idx; int left = 2 * idx + 1; int right = 2 * idx + 2; if (left < minHeap->size && minHeap->array[left]-> freq < minHeap->array[smallest]->freq) smallest = left; if (right < minHeap->size && minHeap->array[right]-> freq < minHeap->array[smallest]->freq) smallest = right; if (smallest != idx) { swapMinHeapNode(&minHeap->array[smallest], &minHeap->array[idx]); minHeapify(minHeap, smallest); } } // A utility function to check // if size of heap is 1 or not int isSizeOne(struct MinHeap* minHeap) { return (minHeap->size == 1); } // A standard function to extract // minimum value node from heap struct MinHeapNode* extractMin(struct MinHeap* minHeap) { struct MinHeapNode* temp = minHeap->array[0]; minHeap->array[0] = minHeap->array[minHeap->size - 1]; --minHeap->size; minHeapify(minHeap, 0); return temp; } // A utility function to insert // a new node to Min Heap void insertMinHeap(struct MinHeap* minHeap, struct MinHeapNode* minHeapNode) { ++minHeap->size; int i = minHeap->size - 1; while (i && minHeapNode->freq < minHeap->array[(i - 1) / 2]->freq) { minHeap->array[i] = minHeap->array[(i - 1) / 2]; i = (i - 1) / 2; } minHeap->array[i] = minHeapNode; } // A standard function to build min heap void buildMinHeap(struct MinHeap* minHeap) { int n = minHeap->size - 1; int i; for (i = (n - 1) / 2; i >= 0; --i) minHeapify(minHeap, i); } // A utility function to print an array of size n void printArr(int arr[], int n) { int i; for (i = 0; i < n; ++i) cout<< arr[i]; cout<<\"\\n\"; } // Utility function to check if this node is leaf int isLeaf(struct MinHeapNode* root) { return !(root->left) && !(root->right); } // Creates a min heap of capacity // equal to size and inserts all character of // data[] in min heap. Initially size of // min heap is equal to capacity struct MinHeap* createAndBuildMinHeap(char data[], int freq[], int size) { struct MinHeap* minHeap = createMinHeap(size); for (int i = 0; i < size; ++i) minHeap->array[i] = newNode(data[i], freq[i]); minHeap->size = size; buildMinHeap(minHeap); return minHeap; } // The main function that builds Huffman tree struct MinHeapNode* buildHuffmanTree(char data[], int freq[], int size) { struct MinHeapNode *left, *right, *top; // Step 1: Create a min heap of capacity // equal to size. Initially, there are // modes equal to size. struct MinHeap* minHeap = createAndBuildMinHeap(data, freq, size); // Iterate while size of heap doesn't become 1 while (!isSizeOne(minHeap)) { // Step 2: Extract the two minimum // freq items from min heap left = extractMin(minHeap); right = extractMin(minHeap); // Step 3: Create a new internal // node with frequency equal to the // sum of the two nodes frequencies. // Make the two extracted node as // left and right children of this new node. // Add this node to the min heap // '$' is a special value for internal nodes, not used top = newNode('$', left->freq + right->freq); top->left = left; top->right = right; insertMinHeap(minHeap, top); } // Step 4: The remaining node is the // root node and the tree is complete. return extractMin(minHeap); } // Prints huffman codes from the root of Huffman Tree. // It uses arr[] to store codes void printCodes(struct MinHeapNode* root, int arr[], int top) { // Assign 0 to left edge and recur if (root->left) { arr[top] = 0; printCodes(root->left, arr, top + 1); } // Assign 1 to right edge and recur if (root->right) { arr[top] = 1; printCodes(root->right, arr, top + 1); } // If this is a leaf node, then // it contains one of the input // characters, print the character // and its code from arr[] if (isLeaf(root)) { cout<< root->data <<\": \"; printArr(arr, top); } } // The main function that builds a // Huffman Tree and print codes by traversing // the built Huffman Tree void HuffmanCodes(char data[], int freq[], int size) { // Construct Huffman Tree struct MinHeapNode* root = buildHuffmanTree(data, freq, size); // Print Huffman codes using // the Huffman tree built above int arr[MAX_TREE_HT], top = 0; printCodes(root, arr, top); } // Driver program to test above functions int main() { char arr[] = { 'a', 'b', 'c', 'd', 'e', 'f' }; int freq[] = { 5, 9, 12, 13, 16, 45 }; int size = sizeof(arr) / sizeof(arr[0]); HuffmanCodes(arr, freq, size); return 0; }","title":"Steps to build Huffman Tree"},{"location":"Algorithm-design-paradigm/02-Dynamic-Programming&Greedy-Algorithm/Greedy-algorithm/Huffman-Coding/#huffman-tree-vs-trie","text":"","title":"Huffman-tree-vs-trie"},{"location":"Algorithm-design-paradigm/02-Dynamic-Programming&Greedy-Algorithm/Greedy-algorithm/Reading-list/","text":"","title":"Reading-list"},{"location":"Algorithm-design-paradigm/05-Backtracking/Backtracking/","text":"Backtracking Description of the method Pseudocode Usage considerations Examples Constraint satisfaction Backtracking # Backtracking is a general algorithm for finding all (or some) solutions to some computational problems , notably constraint satisfaction problems , that incrementally builds candidates to the solutions, and abandons a candidate (\"backtracks\") as soon as it determines that the candidate cannot possibly be completed to a valid solution.[ 1] [ 2] NOTE: \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u7684\u6700\u540e\u4e00\u53e5\uff0c\u5c31\u70b9\u660e\u4e86backtrack\u7684\u542b\u4e49\u6240\u5728\uff0c\u975e\u5e38\u7cbe\u51c6\u3002 SUMMARY : \u5f53\u6211\u4eec\u9700\u8981\u8fdb\u884c\u7a77\u4e3e\u7684\u65f6\u5019\uff0c\u4f7f\u7528 Backtracking \uff0c\u5176\u5b9e\u4ece\u672c\u8d28\u4e0a\u6765\u8bf4\uff0c Backtracking \u5bf9\u5e94\u7684\u662fpermutation\uff0c\u5173\u4e8e Backtracking \u548cpermutation\uff0c\u53ef\u4ee5\u53c2\u770b\u5982\u4e0b\u6587\u7ae0\uff1a Write a program to print all permutations of a given string The classic textbook example of the use of backtracking is the eight queens puzzle , that asks for all arrangements of eight chess queens on a standard chessboard so that no queen attacks any other. In the common backtracking approach, the partial candidates are arrangements of k queens in the first k rows of the board, all in different rows and columns. Any partial solution that contains two mutually attacking queens can be abandoned. NOTE : \u4e0d\u65ad\u5730\u518d\u8fdb\u884c\u5c1d\u8bd5\uff0c\u6240\u4ee5\u5bf9\u5e94\u7684solution\u624d\u662fpartial\u7684\uff0c\u800c\u4e0d\u662f\u5168\u90e8\u7684\uff1b\u5e76\u4e14\u6709\u7684\u65f6\u5019\u662f\u53ef\u4ee5\u63d0\u524d\u7ec8\u6b62\u8fd9\u4e2a\u5c1d\u8bd5\u7684\uff1b Backtracking can be applied only for problems which admit the concept of a \"partial candidate solution\" and a relatively quick test of whether it can possibly be completed to a valid solution. It is useless, for example, for locating a given value in an unordered table. When it is applicable, however, backtracking is often much faster than brute force enumeration of all complete candidates, since it can eliminate\uff08\u6d88\u9664\uff09 many candidates with a single test. SUMMARY : Backtracking VS brute force enumeration Backtracking is an important tool for solving constraint satisfaction problems ,[ 3] such as crosswords , verbal arithmetic , Sudoku , and many other puzzles. It is often the most convenient (if not the most efficient[ citation needed ]) technique for parsing ,[ 4] for the knapsack problem and other combinatorial optimization problems. It is also the basis of the so-called logic programming languages such as Icon , Planner and Prolog . Backtracking depends on user-given \" black box procedures \" that define the problem to be solved, the nature of the partial candidates, and how they are extended into complete candidates. It is therefore a metaheuristic \uff08\u5143\u542f\u53d1\uff09rather than a specific algorithm \u2013 although, unlike many other meta-heuristics, it is guaranteed to find all solutions to a finite problem in a bounded amount of time. NOTE: \u9700\u8981\u6ce8\u610f\u7684\u662f\uff0cbacktracking\u662f\u4e00\u79cd\u7b97\u6cd5\u6846\u67b6\uff0c\u6216\u8005\u8bf4\u662f\u4e00\u79cd\u7b97\u6cd5\u6280\u672f\uff0c\u800c\u4e0d\u662f\u4e00\u79cd\u4e13\u7528\u7684\u7b97\u6cd5\u3002 The term \"backtrack\" was coined by American mathematician D. H. Lehmer in the 1950s.[ 5] The pioneer string-processing language SNOBOL (1962) may have been the first to provide a built-in general backtracking facility. Description of the method # The backtracking algorithm enumerates a set of partial candidates that, in principle, could be completed in various ways to give all the possible solutions to the given problem. The completion is done incrementally, by a sequence of candidate extension steps. Conceptually, the partial candidates are represented as the nodes of a tree structure , the potential search tree. Each partial candidate is the parent of the candidates that differ from it by a single extension step ; the leaves of the tree are the partial candidates that cannot be extended any further. The backtracking algorithm traverses this search tree recursively , from the root down, in depth-first order . At each node c , the algorithm checks whether c can be completed to a valid solution. If it cannot, the whole sub-tree rooted at c is skipped ( pruned \uff08\u526a\u679d\uff09). Otherwise, the algorithm (1) checks whether c itself is a valid solution, and if so reports it to the user; and (2) recursively enumerates all sub-trees of c . The two tests and the children of each node are defined by user-given procedures. Therefore, the actual search tree that is traversed by the algorithm is only a part of the potential tree. The total cost of the algorithm is the number of nodes of the actual tree times the cost of obtaining and processing each node. This fact should be considered when choosing the potential search tree and implementing the pruning test. Pseudocode # In order to apply backtracking to a specific class of problems, one must provide the data P for the particular instance of the problem that is to be solved, and six procedural parameters , root , reject , accept , first , next , and output . These procedures should take the instance data P as a parameter and should do the following: root ( P ): return the partial candidate at the root of the search tree. reject ( P , c ): return true only if the partial candidate c is not worth completing. accept ( P , c ): return true if c is a solution of P , and false otherwise. first ( P , c ): generate the first extension of candidate c . next ( P , s ): generate the next alternative extension of a candidate, after the extension s . output ( P , c ): use the solution c of P , as appropriate to the application. The backtracking algorithm reduces the problem to the call bt ( root ( P )), where bt is the following recursive procedure: procedure bt(c) if reject(P,c) then return if accept(P,c) then output(P,c) s \u2190 first(P,c) while s \u2260 NULL do bt(s) s \u2190 next(P,s) Usage considerations # The reject procedure should be a boolean-valued function that returns true only if it is certain that no possible extension of c is a valid solution for P . If the procedure cannot reach a definite conclusion, it should return false . An incorrect true result may cause the bt procedure to miss some valid solutions. The procedure may assume that reject ( P , t ) returned false for every ancestor t of c in the search tree. On the other hand, the efficiency of the backtracking algorithm depends on reject returning true for candidates that are as close to the root as possible. If reject always returns false , the algorithm will still find all solutions, but it will be equivalent to a brute-force search. The accept procedure should return true if c is a complete and valid solution for the problem instance P , and false otherwise. It may assume that the partial candidate c and all its ancestors in the tree have passed the reject test. The general pseudo-code above does not assume that the valid solutions are always leaves of the potential search tree. In other words, it admits the possibility that a valid solution for P can be further extended to yield other valid solutions. The first and next procedures are used by the backtracking algorithm to enumerate the children of a node c of the tree, that is, the candidates that differ from c by a single extension step. The call first ( P , c ) should yield the first child of c , in some order; and the call next ( P , s ) should return the next sibling of node s , in that order. Both functions should return a distinctive \"NULL\" candidate, if the requested child does not exist. Together, the root , first , and next functions define the set of partial candidates and the potential search tree. They should be chosen so that every solution of P occurs somewhere in the tree, and no partial candidate occurs more than once. Moreover, they should admit an efficient and effective reject predicate. Examples # Examples where backtracking can be used to solve puzzles or problems include: Puzzles such as eight queens puzzle , crosswords , verbal arithmetic , Sudoku [ nb 1] , and Peg Solitaire . Combinatorial optimization problems such as parsing and the knapsack problem . Logic programming languages such as Icon , Planner and Prolog , which use backtracking internally to generate answers. The following is an example where backtracking is used for the constraint satisfaction problem : Constraint satisfaction # The general constraint satisfaction problem consists in finding a list of integers x = ( x [1], x [2], \u2026, x [ n ]), each in some range {1, 2, \u2026, m }, that satisfies some arbitrary constraint (boolean function) F . For this class of problems, the instance data P would be the integers m and n , and the predicate F . In a typical backtracking solution to this problem, one could define a partial candidate as a list of integers c = ( c [1], c [2], \u2026, c [k]), for any k between 0 and n , that are to be assigned to the first k variables x [1], x [2], \u2026, x [ k ]. The root candidate would then be the empty list (). The first and next procedures would then be function first(P, c) k \u2190 length(c) if k = n then return NULL else return (c[1], c[2], \u2026, c[k], 1) function next(P, s) k \u2190 length(s) if s[k] = m then return NULL else return (s[1], s[2], \u2026, s[k - 1], 1 + s[k]) Here length ( c ) is the number of elements in the list c . The call reject ( P , c ) should return true if the constraint F cannot be satisfied by any list of n integers that begins with the k elements of c . For backtracking to be effective, there must be a way to detect this situation, at least for some candidates c , without enumerating all those m n \u2212 k n -tuples. For example, if F is the conjunction of several boolean predicates, F = F [1] \u2227 F [2] \u2227 \u2026 \u2227 F [ p ], and each F [ i ] depends only on a small subset of the variables x [1], \u2026, x [ n ], then the reject procedure could simply check the terms F [ i ] that depend only on variables x [1], \u2026, x [ k ], and return true if any of those terms returns false . In fact, reject needs only check those terms that do depend on x [ k ], since the terms that depend only on x [1], \u2026, x [ k \u2212 1] will have been tested further up in the search tree. Assuming that reject is implemented as above, then accept ( P , c ) needs only check whether c is complete, that is, whether it has n elements. It is generally better to order the list of variables so that it begins with the most critical ones (i.e. the ones with fewest value options, or which have a greater impact on subsequent choices). One could also allow the next function to choose which variable should be assigned when extending a partial candidate, based on the values of the variables already assigned by it. Further improvements can be obtained by the technique of constraint propagation . In addition to retaining minimal recovery values used in backing up, backtracking implementations commonly keep a variable trail, to record value change history. An efficient implementation will avoid creating a variable trail entry between two successive changes when there is no choice point, as the backtracking will erase all of the changes as a single operation. An alternative to the variable trail is to keep a timestamp of when the last change was made to the variable. The timestamp is compared to the timestamp of a choice point. If the choice point has an associated time later than that of the variable, it is unnecessary to revert the variable when the choice point is backtracked, as it was changed before the choice point occurred.","title":"Backtracking"},{"location":"Algorithm-design-paradigm/05-Backtracking/Backtracking/#backtracking","text":"Backtracking is a general algorithm for finding all (or some) solutions to some computational problems , notably constraint satisfaction problems , that incrementally builds candidates to the solutions, and abandons a candidate (\"backtracks\") as soon as it determines that the candidate cannot possibly be completed to a valid solution.[ 1] [ 2] NOTE: \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u7684\u6700\u540e\u4e00\u53e5\uff0c\u5c31\u70b9\u660e\u4e86backtrack\u7684\u542b\u4e49\u6240\u5728\uff0c\u975e\u5e38\u7cbe\u51c6\u3002 SUMMARY : \u5f53\u6211\u4eec\u9700\u8981\u8fdb\u884c\u7a77\u4e3e\u7684\u65f6\u5019\uff0c\u4f7f\u7528 Backtracking \uff0c\u5176\u5b9e\u4ece\u672c\u8d28\u4e0a\u6765\u8bf4\uff0c Backtracking \u5bf9\u5e94\u7684\u662fpermutation\uff0c\u5173\u4e8e Backtracking \u548cpermutation\uff0c\u53ef\u4ee5\u53c2\u770b\u5982\u4e0b\u6587\u7ae0\uff1a Write a program to print all permutations of a given string The classic textbook example of the use of backtracking is the eight queens puzzle , that asks for all arrangements of eight chess queens on a standard chessboard so that no queen attacks any other. In the common backtracking approach, the partial candidates are arrangements of k queens in the first k rows of the board, all in different rows and columns. Any partial solution that contains two mutually attacking queens can be abandoned. NOTE : \u4e0d\u65ad\u5730\u518d\u8fdb\u884c\u5c1d\u8bd5\uff0c\u6240\u4ee5\u5bf9\u5e94\u7684solution\u624d\u662fpartial\u7684\uff0c\u800c\u4e0d\u662f\u5168\u90e8\u7684\uff1b\u5e76\u4e14\u6709\u7684\u65f6\u5019\u662f\u53ef\u4ee5\u63d0\u524d\u7ec8\u6b62\u8fd9\u4e2a\u5c1d\u8bd5\u7684\uff1b Backtracking can be applied only for problems which admit the concept of a \"partial candidate solution\" and a relatively quick test of whether it can possibly be completed to a valid solution. It is useless, for example, for locating a given value in an unordered table. When it is applicable, however, backtracking is often much faster than brute force enumeration of all complete candidates, since it can eliminate\uff08\u6d88\u9664\uff09 many candidates with a single test. SUMMARY : Backtracking VS brute force enumeration Backtracking is an important tool for solving constraint satisfaction problems ,[ 3] such as crosswords , verbal arithmetic , Sudoku , and many other puzzles. It is often the most convenient (if not the most efficient[ citation needed ]) technique for parsing ,[ 4] for the knapsack problem and other combinatorial optimization problems. It is also the basis of the so-called logic programming languages such as Icon , Planner and Prolog . Backtracking depends on user-given \" black box procedures \" that define the problem to be solved, the nature of the partial candidates, and how they are extended into complete candidates. It is therefore a metaheuristic \uff08\u5143\u542f\u53d1\uff09rather than a specific algorithm \u2013 although, unlike many other meta-heuristics, it is guaranteed to find all solutions to a finite problem in a bounded amount of time. NOTE: \u9700\u8981\u6ce8\u610f\u7684\u662f\uff0cbacktracking\u662f\u4e00\u79cd\u7b97\u6cd5\u6846\u67b6\uff0c\u6216\u8005\u8bf4\u662f\u4e00\u79cd\u7b97\u6cd5\u6280\u672f\uff0c\u800c\u4e0d\u662f\u4e00\u79cd\u4e13\u7528\u7684\u7b97\u6cd5\u3002 The term \"backtrack\" was coined by American mathematician D. H. Lehmer in the 1950s.[ 5] The pioneer string-processing language SNOBOL (1962) may have been the first to provide a built-in general backtracking facility.","title":"Backtracking"},{"location":"Algorithm-design-paradigm/05-Backtracking/Backtracking/#description_of_the_method","text":"The backtracking algorithm enumerates a set of partial candidates that, in principle, could be completed in various ways to give all the possible solutions to the given problem. The completion is done incrementally, by a sequence of candidate extension steps. Conceptually, the partial candidates are represented as the nodes of a tree structure , the potential search tree. Each partial candidate is the parent of the candidates that differ from it by a single extension step ; the leaves of the tree are the partial candidates that cannot be extended any further. The backtracking algorithm traverses this search tree recursively , from the root down, in depth-first order . At each node c , the algorithm checks whether c can be completed to a valid solution. If it cannot, the whole sub-tree rooted at c is skipped ( pruned \uff08\u526a\u679d\uff09). Otherwise, the algorithm (1) checks whether c itself is a valid solution, and if so reports it to the user; and (2) recursively enumerates all sub-trees of c . The two tests and the children of each node are defined by user-given procedures. Therefore, the actual search tree that is traversed by the algorithm is only a part of the potential tree. The total cost of the algorithm is the number of nodes of the actual tree times the cost of obtaining and processing each node. This fact should be considered when choosing the potential search tree and implementing the pruning test.","title":"Description of the method"},{"location":"Algorithm-design-paradigm/05-Backtracking/Backtracking/#pseudocode","text":"In order to apply backtracking to a specific class of problems, one must provide the data P for the particular instance of the problem that is to be solved, and six procedural parameters , root , reject , accept , first , next , and output . These procedures should take the instance data P as a parameter and should do the following: root ( P ): return the partial candidate at the root of the search tree. reject ( P , c ): return true only if the partial candidate c is not worth completing. accept ( P , c ): return true if c is a solution of P , and false otherwise. first ( P , c ): generate the first extension of candidate c . next ( P , s ): generate the next alternative extension of a candidate, after the extension s . output ( P , c ): use the solution c of P , as appropriate to the application. The backtracking algorithm reduces the problem to the call bt ( root ( P )), where bt is the following recursive procedure: procedure bt(c) if reject(P,c) then return if accept(P,c) then output(P,c) s \u2190 first(P,c) while s \u2260 NULL do bt(s) s \u2190 next(P,s)","title":"Pseudocode"},{"location":"Algorithm-design-paradigm/05-Backtracking/Backtracking/#usage_considerations","text":"The reject procedure should be a boolean-valued function that returns true only if it is certain that no possible extension of c is a valid solution for P . If the procedure cannot reach a definite conclusion, it should return false . An incorrect true result may cause the bt procedure to miss some valid solutions. The procedure may assume that reject ( P , t ) returned false for every ancestor t of c in the search tree. On the other hand, the efficiency of the backtracking algorithm depends on reject returning true for candidates that are as close to the root as possible. If reject always returns false , the algorithm will still find all solutions, but it will be equivalent to a brute-force search. The accept procedure should return true if c is a complete and valid solution for the problem instance P , and false otherwise. It may assume that the partial candidate c and all its ancestors in the tree have passed the reject test. The general pseudo-code above does not assume that the valid solutions are always leaves of the potential search tree. In other words, it admits the possibility that a valid solution for P can be further extended to yield other valid solutions. The first and next procedures are used by the backtracking algorithm to enumerate the children of a node c of the tree, that is, the candidates that differ from c by a single extension step. The call first ( P , c ) should yield the first child of c , in some order; and the call next ( P , s ) should return the next sibling of node s , in that order. Both functions should return a distinctive \"NULL\" candidate, if the requested child does not exist. Together, the root , first , and next functions define the set of partial candidates and the potential search tree. They should be chosen so that every solution of P occurs somewhere in the tree, and no partial candidate occurs more than once. Moreover, they should admit an efficient and effective reject predicate.","title":"Usage considerations"},{"location":"Algorithm-design-paradigm/05-Backtracking/Backtracking/#examples","text":"Examples where backtracking can be used to solve puzzles or problems include: Puzzles such as eight queens puzzle , crosswords , verbal arithmetic , Sudoku [ nb 1] , and Peg Solitaire . Combinatorial optimization problems such as parsing and the knapsack problem . Logic programming languages such as Icon , Planner and Prolog , which use backtracking internally to generate answers. The following is an example where backtracking is used for the constraint satisfaction problem :","title":"Examples"},{"location":"Algorithm-design-paradigm/05-Backtracking/Backtracking/#constraint_satisfaction","text":"The general constraint satisfaction problem consists in finding a list of integers x = ( x [1], x [2], \u2026, x [ n ]), each in some range {1, 2, \u2026, m }, that satisfies some arbitrary constraint (boolean function) F . For this class of problems, the instance data P would be the integers m and n , and the predicate F . In a typical backtracking solution to this problem, one could define a partial candidate as a list of integers c = ( c [1], c [2], \u2026, c [k]), for any k between 0 and n , that are to be assigned to the first k variables x [1], x [2], \u2026, x [ k ]. The root candidate would then be the empty list (). The first and next procedures would then be function first(P, c) k \u2190 length(c) if k = n then return NULL else return (c[1], c[2], \u2026, c[k], 1) function next(P, s) k \u2190 length(s) if s[k] = m then return NULL else return (s[1], s[2], \u2026, s[k - 1], 1 + s[k]) Here length ( c ) is the number of elements in the list c . The call reject ( P , c ) should return true if the constraint F cannot be satisfied by any list of n integers that begins with the k elements of c . For backtracking to be effective, there must be a way to detect this situation, at least for some candidates c , without enumerating all those m n \u2212 k n -tuples. For example, if F is the conjunction of several boolean predicates, F = F [1] \u2227 F [2] \u2227 \u2026 \u2227 F [ p ], and each F [ i ] depends only on a small subset of the variables x [1], \u2026, x [ n ], then the reject procedure could simply check the terms F [ i ] that depend only on variables x [1], \u2026, x [ k ], and return true if any of those terms returns false . In fact, reject needs only check those terms that do depend on x [ k ], since the terms that depend only on x [1], \u2026, x [ k \u2212 1] will have been tested further up in the search tree. Assuming that reject is implemented as above, then accept ( P , c ) needs only check whether c is complete, that is, whether it has n elements. It is generally better to order the list of variables so that it begins with the most critical ones (i.e. the ones with fewest value options, or which have a greater impact on subsequent choices). One could also allow the next function to choose which variable should be assigned when extending a partial candidate, based on the values of the variables already assigned by it. Further improvements can be obtained by the technique of constraint propagation . In addition to retaining minimal recovery values used in backing up, backtracking implementations commonly keep a variable trail, to record value change history. An efficient implementation will avoid creating a variable trail entry between two successive changes when there is no choice point, as the backtracking will erase all of the changes as a single operation. An alternative to the variable trail is to keep a timestamp of when the last change was made to the variable. The timestamp is compared to the timestamp of a choice point. If the choice point has an associated time later than that of the variable, it is unnecessary to revert the variable when the choice point is backtracked, as it was changed before the choice point occurred.","title":"Constraint satisfaction"},{"location":"Algorithm-design-paradigm/05-Backtracking/Reading-list/","text":"Reading list # https://www.geeksforgeeks.org/top-20-backtracking-algorithm-interview-questions/ Pattern Searching # \u5728\u5b57\u7b26\u4e32\uff08\u4e5f\u53eb\u4e3b\u4e32\uff09\u4e2d\u7684\u5b9a\u4f4d\u6a21\u5f0f\uff08pattern\uff09\u95ee\u9898\u53ef\u4ee5\u4f7f\u7528\u56de\u6eaf\u6cd5\u8fdb\u884c\u89e3\u51b3\uff0c\u4f46\u662f\u8fd9\u79cd\u89e3\u6cd5\u662fnaive\u7684\u3002\u4f18\u5316\u65b9\u6cd5\u662fKMP\u7b97\u6cd5\uff0c\u5728\u4e0b\u9762\u4e24\u7bc7\u6587\u7ae0\u4e2d\u5bf9\u4e24\u79cd\u8fdb\u884c\u4e86\u6bd4\u8f83\u5206\u6790\uff1a - \u8be6\u89e3KMP\u7b97\u6cd5 # eight queens puzzle # crosswords # verbal arithmetic # Sudoku # parsing # \u53c2\u89c1 Compilers: Principles, Techniques, and Tools \u76844.4 Top-Down Parsing\uff0c\u5176\u4e2d\u4ecb\u7ecd\u4e86\u4f7f\u7528backtrack\u6765\u5b9e\u73b0parsing\u3002 \u5728GitHub\u4e2d\uff0c\u53ef\u4ee5\u68c0\u7d22\u975e\u5e38\u591a\u7684\u8fd9\u79cd\u9879\u76ee\uff1a https://github.com/search?utf8=%E2%9C%93&q=backtrack+parse&type=","title":"Reading-list"},{"location":"Algorithm-design-paradigm/05-Backtracking/Reading-list/#reading_list","text":"https://www.geeksforgeeks.org/top-20-backtracking-algorithm-interview-questions/","title":"Reading list"},{"location":"Algorithm-design-paradigm/05-Backtracking/Reading-list/#pattern_searching","text":"\u5728\u5b57\u7b26\u4e32\uff08\u4e5f\u53eb\u4e3b\u4e32\uff09\u4e2d\u7684\u5b9a\u4f4d\u6a21\u5f0f\uff08pattern\uff09\u95ee\u9898\u53ef\u4ee5\u4f7f\u7528\u56de\u6eaf\u6cd5\u8fdb\u884c\u89e3\u51b3\uff0c\u4f46\u662f\u8fd9\u79cd\u89e3\u6cd5\u662fnaive\u7684\u3002\u4f18\u5316\u65b9\u6cd5\u662fKMP\u7b97\u6cd5\uff0c\u5728\u4e0b\u9762\u4e24\u7bc7\u6587\u7ae0\u4e2d\u5bf9\u4e24\u79cd\u8fdb\u884c\u4e86\u6bd4\u8f83\u5206\u6790\uff1a","title":"Pattern Searching"},{"location":"Algorithm-design-paradigm/05-Backtracking/Reading-list/#-_kmp","text":"","title":"- \u8be6\u89e3KMP\u7b97\u6cd5"},{"location":"Algorithm-design-paradigm/05-Backtracking/Reading-list/#eight_queens_puzzle","text":"","title":"eight queens puzzle"},{"location":"Algorithm-design-paradigm/05-Backtracking/Reading-list/#crosswords","text":"","title":"crosswords"},{"location":"Algorithm-design-paradigm/05-Backtracking/Reading-list/#verbal_arithmetic","text":"","title":"verbal arithmetic"},{"location":"Algorithm-design-paradigm/05-Backtracking/Reading-list/#sudoku","text":"","title":"Sudoku"},{"location":"Algorithm-design-paradigm/05-Backtracking/Reading-list/#parsing","text":"\u53c2\u89c1 Compilers: Principles, Techniques, and Tools \u76844.4 Top-Down Parsing\uff0c\u5176\u4e2d\u4ecb\u7ecd\u4e86\u4f7f\u7528backtrack\u6765\u5b9e\u73b0parsing\u3002 \u5728GitHub\u4e2d\uff0c\u53ef\u4ee5\u68c0\u7d22\u975e\u5e38\u591a\u7684\u8fd9\u79cd\u9879\u76ee\uff1a https://github.com/search?utf8=%E2%9C%93&q=backtrack+parse&type=","title":"parsing"},{"location":"Algorithm-design-paradigm/05-Backtracking/Sudoku-solving-algorithms/","text":"Sudoku solving algorithms Sudoku solving algorithms #","title":"Sudoku-solving-algorithms"},{"location":"Algorithm-design-paradigm/05-Backtracking/Sudoku-solving-algorithms/#sudoku_solving_algorithms","text":"","title":"Sudoku solving algorithms"},{"location":"Algorithm-design-paradigm/05-Backtracking/Permutation-and-combination/Permutation/","text":"Permutation # Permutations in computing # Algorithms to generate permutations # In computing it may be required to generate permutations of a given sequence of values. The methods best adapted to do this depend on whether one wants some randomly chosen permutations, or all permutations, and in the latter case if a specific ordering is required. Another question is whether possible equality among entries in the given sequence is to be taken into account; if so, one should only generate distinct multiset permutations of the sequence. An obvious way to generate permutations of n is to generate values for the Lehmer code (possibly using the factorial number system representation of integers up to n !), and convert those into the corresponding permutations. However, the latter step, while straightforward, is hard to implement efficiently, because it requires n operations each of selection from a sequence and deletion from it, at an arbitrary position; of the obvious representations of the sequence as an array or a linked list , both require (for different reasons) about n 2/4 operations to perform the conversion. With n likely to be rather small (especially if generation of all permutations is needed) that is not too much of a problem, but it turns out that both for random and for systematic generation there are simple alternatives that do considerably better. For this reason it does not seem useful, although certainly possible, to employ a special data structure that would allow performing the conversion from Lehmer code to permutation in O ( n log n ) time. \u751f\u6210n\u7684\u6392\u5217\u7684\u4e00\u4e2a\u660e\u663e\u7684\u65b9\u6cd5\u662f\u751f\u6210Lehmer\u4ee3\u7801\u7684\u503c(\u53ef\u80fd\u4f7f\u7528\u5230n\u7684\u6574\u6570\u7684\u9636\u4e58\u6570\u5b57\u7cfb\u7edf\u8868\u793a)\uff0c\u5e76\u5c06\u5b83\u4eec\u8f6c\u6362\u6210\u76f8\u5e94\u7684\u6392\u5217\u3002 \u4f46\u662f\uff0c\u540e\u9762\u7684\u6b65\u9aa4\u867d\u7136\u7b80\u5355\uff0c\u4f46\u662f\u5f88\u96be\u6709\u6548\u5730\u5b9e\u73b0\uff0c\u56e0\u4e3a\u5b83\u9700\u8981\u5728\u4efb\u610f\u4f4d\u7f6e\u5bf9\u5e8f\u5217\u8fdb\u884cn\u6b21\u9009\u62e9\u548c\u5220\u9664\u64cd\u4f5c; \u5e8f\u5217\u7684\u660e\u663e\u8868\u793a\u5f62\u5f0f\u4e3a\u6570\u7ec4\u6216\u94fe\u8868\uff0c\u4e24\u8005\u90fd\u9700\u8981(\u51fa\u4e8e\u4e0d\u540c\u7684\u539f\u56e0)\u5927\u7ea6n2/4\u64cd\u4f5c\u6765\u6267\u884c\u8f6c\u6362\u3002 n\u53ef\u80fd\u5f88\u5c0f(\u7279\u522b\u662f\u5982\u679c\u9700\u8981\u751f\u6210\u6240\u6709\u6392\u5217)\uff0c\u8fd9\u4e0d\u662f\u4ec0\u4e48\u5927\u95ee\u9898\uff0c\u4f46\u4e8b\u5b9e\u8bc1\u660e\uff0c\u65e0\u8bba\u662f\u968f\u673a\u751f\u6210\u8fd8\u662f\u7cfb\u7edf\u751f\u6210\uff0c\u90fd\u6709\u7b80\u5355\u7684\u66ff\u4ee3\u65b9\u6cd5\u53ef\u4ee5\u505a\u5f97\u66f4\u597d\u3002 \u56e0\u6b64\uff0c\u4f7f\u7528\u4e00\u79cd\u7279\u6b8a\u7684\u6570\u636e\u7ed3\u6784\uff0c\u5141\u8bb8\u5728O(n log n)\u65f6\u95f4\u5185\u6267\u884c\u4eceLehmer\u4ee3\u7801\u5230\u7f6e\u6362\u7684\u8f6c\u6362\uff0c\u867d\u7136\u8fd9\u79cd\u6570\u636e\u7ed3\u6784\u80af\u5b9a\u662f\u53ef\u884c\u7684\uff0c\u4f46\u4f3c\u4e4e\u5e76\u4e0d\u6709\u7528\u3002","title":"Permutation"},{"location":"Algorithm-design-paradigm/05-Backtracking/Permutation-and-combination/Permutation/#permutation","text":"","title":"Permutation"},{"location":"Algorithm-design-paradigm/05-Backtracking/Permutation-and-combination/Permutation/#permutations_in_computing","text":"","title":"Permutations in computing"},{"location":"Algorithm-design-paradigm/05-Backtracking/Permutation-and-combination/Permutation/#algorithms_to_generate_permutations","text":"In computing it may be required to generate permutations of a given sequence of values. The methods best adapted to do this depend on whether one wants some randomly chosen permutations, or all permutations, and in the latter case if a specific ordering is required. Another question is whether possible equality among entries in the given sequence is to be taken into account; if so, one should only generate distinct multiset permutations of the sequence. An obvious way to generate permutations of n is to generate values for the Lehmer code (possibly using the factorial number system representation of integers up to n !), and convert those into the corresponding permutations. However, the latter step, while straightforward, is hard to implement efficiently, because it requires n operations each of selection from a sequence and deletion from it, at an arbitrary position; of the obvious representations of the sequence as an array or a linked list , both require (for different reasons) about n 2/4 operations to perform the conversion. With n likely to be rather small (especially if generation of all permutations is needed) that is not too much of a problem, but it turns out that both for random and for systematic generation there are simple alternatives that do considerably better. For this reason it does not seem useful, although certainly possible, to employ a special data structure that would allow performing the conversion from Lehmer code to permutation in O ( n log n ) time. \u751f\u6210n\u7684\u6392\u5217\u7684\u4e00\u4e2a\u660e\u663e\u7684\u65b9\u6cd5\u662f\u751f\u6210Lehmer\u4ee3\u7801\u7684\u503c(\u53ef\u80fd\u4f7f\u7528\u5230n\u7684\u6574\u6570\u7684\u9636\u4e58\u6570\u5b57\u7cfb\u7edf\u8868\u793a)\uff0c\u5e76\u5c06\u5b83\u4eec\u8f6c\u6362\u6210\u76f8\u5e94\u7684\u6392\u5217\u3002 \u4f46\u662f\uff0c\u540e\u9762\u7684\u6b65\u9aa4\u867d\u7136\u7b80\u5355\uff0c\u4f46\u662f\u5f88\u96be\u6709\u6548\u5730\u5b9e\u73b0\uff0c\u56e0\u4e3a\u5b83\u9700\u8981\u5728\u4efb\u610f\u4f4d\u7f6e\u5bf9\u5e8f\u5217\u8fdb\u884cn\u6b21\u9009\u62e9\u548c\u5220\u9664\u64cd\u4f5c; \u5e8f\u5217\u7684\u660e\u663e\u8868\u793a\u5f62\u5f0f\u4e3a\u6570\u7ec4\u6216\u94fe\u8868\uff0c\u4e24\u8005\u90fd\u9700\u8981(\u51fa\u4e8e\u4e0d\u540c\u7684\u539f\u56e0)\u5927\u7ea6n2/4\u64cd\u4f5c\u6765\u6267\u884c\u8f6c\u6362\u3002 n\u53ef\u80fd\u5f88\u5c0f(\u7279\u522b\u662f\u5982\u679c\u9700\u8981\u751f\u6210\u6240\u6709\u6392\u5217)\uff0c\u8fd9\u4e0d\u662f\u4ec0\u4e48\u5927\u95ee\u9898\uff0c\u4f46\u4e8b\u5b9e\u8bc1\u660e\uff0c\u65e0\u8bba\u662f\u968f\u673a\u751f\u6210\u8fd8\u662f\u7cfb\u7edf\u751f\u6210\uff0c\u90fd\u6709\u7b80\u5355\u7684\u66ff\u4ee3\u65b9\u6cd5\u53ef\u4ee5\u505a\u5f97\u66f4\u597d\u3002 \u56e0\u6b64\uff0c\u4f7f\u7528\u4e00\u79cd\u7279\u6b8a\u7684\u6570\u636e\u7ed3\u6784\uff0c\u5141\u8bb8\u5728O(n log n)\u65f6\u95f4\u5185\u6267\u884c\u4eceLehmer\u4ee3\u7801\u5230\u7f6e\u6362\u7684\u8f6c\u6362\uff0c\u867d\u7136\u8fd9\u79cd\u6570\u636e\u7ed3\u6784\u80af\u5b9a\u662f\u53ef\u884c\u7684\uff0c\u4f46\u4f3c\u4e4e\u5e76\u4e0d\u6709\u7528\u3002","title":"Algorithms to generate permutations"},{"location":"Algorithm-design-paradigm/05-Backtracking/Permutation-and-combination/Reading-list/","text":"Algorithm to return all combinations of k elements from n The coolest way to generate combinations Algorithms: Generating Combinations #100DaysOfCode GENERATING COMBINATIONS Print all possible combinations of r elements in a given array of size n Print all possible strings of length k that can be formed from a set of n characters Algorithm to return all combinations of k elements from n # The coolest way to generate combinations # Algorithms: Generating Combinations #100DaysOfCode # GENERATING COMBINATIONS # Print all possible combinations of r elements in a given array of size n # Print all possible strings of length k that can be formed from a set of n characters #","title":"Reading-list"},{"location":"Algorithm-design-paradigm/05-Backtracking/Permutation-and-combination/Reading-list/#algorithm_to_return_all_combinations_of_k_elements_from_n","text":"","title":"Algorithm to return all combinations of k elements from n"},{"location":"Algorithm-design-paradigm/05-Backtracking/Permutation-and-combination/Reading-list/#the_coolest_way_to_generate_combinations","text":"","title":"The coolest way to generate combinations"},{"location":"Algorithm-design-paradigm/05-Backtracking/Permutation-and-combination/Reading-list/#algorithms_generating_combinations_100daysofcode","text":"","title":"Algorithms: Generating Combinations #100DaysOfCode"},{"location":"Algorithm-design-paradigm/05-Backtracking/Permutation-and-combination/Reading-list/#generating_combinations","text":"","title":"GENERATING COMBINATIONS"},{"location":"Algorithm-design-paradigm/05-Backtracking/Permutation-and-combination/Reading-list/#print_all_possible_combinations_of_r_elements_in_a_given_array_of_size_n","text":"","title":"Print all possible combinations of r elements in a given array of size n"},{"location":"Algorithm-design-paradigm/05-Backtracking/Permutation-and-combination/Reading-list/#print_all_possible_strings_of_length_k_that_can_be_formed_from_a_set_of_n_characters","text":"","title":"Print all possible strings of length k that can be formed from a set of n characters"},{"location":"Algorithm-design-paradigm/05-Backtracking/Permutation-and-combination/Enum-permutation/Backtrack-enum-all-permutations-of-string/","text":"Write a program to print all permutations of a given string # \u95ee\u9898\u63cf\u8ff0 # A permutation, also called an \u201carrangement number\u201d or \u201corder,\u201d is a rearrangement of the elements of an ordered list S into a one-to-one correspondence with S itself. A string of length n has n! permutation. Source: Mathword(http://mathworld.wolfram.com/Permutation.html) Below are the permutations of string ABC. ABC ACB BAC BCA CBA CAB \u7b97\u6cd5 # Here is a solution that is used as a basis in backtracking. SUMMARY : \u4e0a\u8ff0\u56fe\u5df2\u7ecf\u975e\u5e38\u597d\u5730\u5c55\u793a\u4e86 backtracking \u7b97\u6cd5\u7684\u8c03\u7528\u8fc7\u7a0b\uff0c\u7ed3\u5408\u4e0b\u9762\u7684\u4ee3\u7801\u6765\u770b\u7684\u8bdd\uff0c\u5bf9\u4e0a\u9762\u7684recursion tree\u8fdb\u884c\u5148\u5e8f\u904d\u5386\u5c31\u662f\u4e0b\u9762\u7684\u51fd\u6570\u7684\u8c03\u7528\u8fc7\u7a0b\uff1b\u8be5\u51fd\u6570\u7684\u76ee\u7684\u662f\u751f\u6210\u6240\u6709\u7684permutation\u7684\uff0c\u5373\u5b83\u9700\u8981\u7a77\u4e3e\uff1b \u751f\u6210permutation\u7684\u8fc7\u7a0b\u53ef\u4ee5\u7b80\u8ff0\u4e3a\uff1a \u4ecen\u4e2a\u5143\u7d20\u4e2d\u6311\u9009\u51fa\u4e00\u4e2a\u6765\u4f5c\u4e3a\u7b2c0\u4e2a\u5143\u7d20 \u4ecen-1\u4e2a\u5143\u7d20\u4e2d\u6311\u9009\u51fa\u4e00\u4e2a\u6765\u4f5c\u4e3a\u7b2c1\u4e2a\u5143\u7d20 \u4ecen-2\u4e2a\u5143\u7d20\u4e2d\u6311\u9009\u51fa\u4e00\u4e2a\u6765\u4f5c\u4e3a\u7b2c2\u4e2a\u5143\u7d20 ... \u4ece2\u4e2a\u5143\u7d20\u4e2d\u6311\u9009\u51fa\u4e00\u4e2a\u6765\u4f5c\u4e3a\u7b2cn-2\u4e2a\u5143\u7d20 \u4ece1\u4e2a\u5143\u7d20\u4e2d\u6311\u9009\u51fa\u4e00\u4e2a\u6765\u4f5c\u4e3a\u7b2cn-1\u4e2a\u5143\u7d20 \u663e\u7136\u4e0a\u8ff0\u8fc7\u7a0b\u662f\u4e00\u4e2a\u9012\u5f52\u7684\u8fc7\u7a0b\uff0c\u4e0a\u8ff0\u8fc7\u7a0b\u5982\u679c\u4f7f\u7528\u56fe\u5f62\u5316\u6765\u5c55\u793a\u7684\u8bdd\uff0c\u5176\u5b9e\u548c\u4e0a\u9762\u7684recursion tree\u662f\u5b8c\u7f8e\u5bf9\u5e94\u7684\uff1a recursion tree\u7684\u7b2c\u4e00\u5c42\u8282\u70b9\uff0c\u5bf9\u5e94\u4e86\u4ecen\uff08n=3\uff09\u4e2a\u5143\u7d20\u4e2d\u6311\u9009\u51fa\u4e00\u4e2a\u6765\u4f5c\u4e3a\u7b2c0\u4e2a\u5143\u7d20 recursion tree\u7684\u7b2c\u4e8c\u5c42\u8282\u70b9\uff0c\u5bf9\u5e94\u4e86\u4ecen-1\uff08n-1=2\uff09\u4e2a\u5143\u7d20\u4e2d\u6311\u9009\u51fa\u4e00\u4e2a\u6765\u4f5c\u4e3a\u7b2c1\u4e2a\u5143\u7d20 recursion tree\u4e2d\u7684\u6bcf\u4e00\u6761\u8def\u5f84\u5c31\u5bf9\u5e94\u4e86\u4e00\u4e2a\u7ec4\u5408\uff1b \u6240\u4ee5\uff0c\u5171\u6709$n \\times (n-1) \\times (n-2) \\times \\ldots \\times 2 \\times 1$\u79cd\u6392\u5217\uff0c$n \\times (n-1) \\times (n-2) \\times \\ldots \\times 2 \\times 1$\u662f\u548c\u4e0a\u8ff0recursion tree\u5b8c\u7f8e\u5bf9\u5e94\u7684: \u7b2c\u4e00\u4e2a\u5143\u7d20\u9009\u5b9a\u4e3a A \uff0c\u5bf9\u5e94\u4e86\u7b2c\u4e00\u68f5\u5b50\u6811\uff0c\u663e\u7136\u5171\u6709n\u68f5\u7c7b\u4f3c\u8fd9\u6837\u7684\u5b50\u6811\uff0c\u6240\u4ee5\u662f$n \\times$ \u7531\u4e8e\u6211\u4eec\u7684backtrack\u7b97\u6cd5\u76ee\u7684\u662f \u7a77\u4e3e \uff0c\u6240\u4ee5\u5728\u7b2c\u4e00\u4e2a\u5143\u7d20\u9009\u5b9a\u4e3a A \u540e\uff0c\u8fd8\u9700\u8981\u53bb\u5c1d\u8bd5\u7b2c\u4e00\u4e2a\u5143\u7d20\u9009\u5b9a\u4e3a B \u7684\u60c5\u51b5\uff0c\u8fd8\u9700\u8981\u53bb\u4ea7\u751f\u7b2c\u4e00\u4e2a\u5143\u7d20\u9009\u5b9a\u4e3a C \u7684\u60c5\u51b5\uff1b\u4ee5\u6b64\u7c7b\u63a8\uff0c\u5728\u7b2c\u4e8c\u4e2a\u5143\u7d20\u9009\u5b9a\u4e3a A \u540e\uff0c\u8fd8\u9700\u8981\u53bb\u5c1d\u8bd5\u7b2c\u4e8c\u4e2a\u5143\u7d20\u9009\u5b9a\u4e3a B \u7684\u60c5\u51b5\uff0c\u8fd8\u9700\u8981\u53bb\u4ea7\u751f\u7b2c\u4e8c\u4e2a\u5143\u7d20\u9009\u5b9a\u4e3a C \u7684\u60c5\u51b5\uff1b\u6240\u4ee5\u6211\u4eec\u9700\u8981\u56de\u6eaf\uff0c\u5373\u56de\u9000\u5230\u4e0a\u4e00\u5c42\u7684\u72b6\u6001\uff0c\u8fd9\u6837\u4e0a\u4e00\u5c42\u5c31\u80fd\u591f\u5c1d\u8bd5\u53e6\u5916\u4e00\u79cd\u60c5\u51b5\u4e86\uff1b\u5176\u5b9e\u8fd9\u5c31\u662f backtracking \u7b97\u6cd5\u7684\u6838\u5fc3\u601d\u60f3\u6240\u5728\u4e86\uff1b \u5bf9\u4e0a\u8ff0\u9012\u5f52\u6570\u8fdb\u884c\u5148\u5e8f\u904d\u5386\u5c31\u5bf9\u5e94\u4e86\u9012\u5f52\u51fd\u6570\u5b9e\u9645\u7684\u6267\u884c\u8fc7\u7a0b\uff0c\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u5f53\u7b2c\u4e00\u6b21\u7531\u4e0a\u5230\u4e0b\u7ecf\u8fc7\u4e00\u6761\u8fb9\u7684\u65f6\u5019\uff0c\u6267\u884cswap\u51fd\u6570\uff0c\u5f53\u7b2c\u4e8c\u6b21\u7531\u4e0b\u5230\u4e0a\u7ecf\u8fc7\u8be5\u8fb9\u7684\u65f6\u5019\uff0c\u6267\u884c\u65b9\u5411\u76f8\u53cd\u7684swap\u51fd\u6570\uff1b\u8fd9\u4e5f\u8bf4\u660e\u9012\u5f52\u8c03\u7528\u662f\u6df1\u5ea6\u4f18\u5148\u5730\u904d\u5386\u3002 \u90a3\u4e0a\u8ff0\u8fc7\u7a0b\u5982\u4f55\u4f7f\u7528\u9012\u5f52\u51fd\u6570\u6765\u8fdb\u884c\u5b9e\u73b0\u5462\uff1f\u8fd9\u5c31\u662f\u4e00\u4e2a\u7a0b\u5e8f\u5458\u9700\u8981\u8003\u8651\u7684\u95ee\u9898\u4e86\u3002\u5bf9\u4e8e\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u5e38\u5e38\u8003\u8651\u7684\u662f\u4f7f\u7528 backtracking \u7b97\u6cd5\uff1a C++ # // C++ program to print all // permutations with duplicates allowed #include <bits/stdc++.h> using namespace std; // Function to print permutations of string // This function takes three parameters: // 1. String // 2. Starting index of the string // 3. Ending index of the string. void permute(string a, int l, int r) { // Base case if (l == r) cout<<a<<endl; else { // Permutations made for (int i = l; i <= r; i++) { // Swapping done swap(a[l], a[i]); // Recursion called permute(a, l+1, r); //backtrack swap(a[l], a[i]); } } } // Driver Code int main() { string str = \"ABC\"; int n = str.size(); permute(str, 0, n-1); return 0; } // This is code is contributed by rathbhupendra C # // C program to print all permutations with duplicates allowed #include <stdio.h> #include <string.h> /* Function to swap values at two pointers */ void swap(char *x, char *y) { char temp; temp = *x; *x = *y; *y = temp; } /* Function to print permutations of string This function takes three parameters: 1. String 2. Starting index of the string 3. Ending index of the string. */ void permute(char *a, int l, int r) { int i; if (l == r) printf(\"%s\\n\", a); else { for (i = l; i <= r; i++) { swap((a+l), (a+i)); permute(a, l+1, r); swap((a+l), (a+i)); //backtrack } } } /* Driver program to test above functions */ int main() { char str[] = \"ABC\"; int n = strlen(str); permute(str, 0, n-1); return 0; }","title":"Backtrack-enum-all-permutations-of-string"},{"location":"Algorithm-design-paradigm/05-Backtracking/Permutation-and-combination/Enum-permutation/Backtrack-enum-all-permutations-of-string/#write_a_program_to_print_all_permutations_of_a_given_string","text":"","title":"Write a program to print all permutations of a given string"},{"location":"Algorithm-design-paradigm/05-Backtracking/Permutation-and-combination/Enum-permutation/Backtrack-enum-all-permutations-of-string/#_1","text":"A permutation, also called an \u201carrangement number\u201d or \u201corder,\u201d is a rearrangement of the elements of an ordered list S into a one-to-one correspondence with S itself. A string of length n has n! permutation. Source: Mathword(http://mathworld.wolfram.com/Permutation.html) Below are the permutations of string ABC. ABC ACB BAC BCA CBA CAB","title":"\u95ee\u9898\u63cf\u8ff0"},{"location":"Algorithm-design-paradigm/05-Backtracking/Permutation-and-combination/Enum-permutation/Backtrack-enum-all-permutations-of-string/#_2","text":"Here is a solution that is used as a basis in backtracking. SUMMARY : \u4e0a\u8ff0\u56fe\u5df2\u7ecf\u975e\u5e38\u597d\u5730\u5c55\u793a\u4e86 backtracking \u7b97\u6cd5\u7684\u8c03\u7528\u8fc7\u7a0b\uff0c\u7ed3\u5408\u4e0b\u9762\u7684\u4ee3\u7801\u6765\u770b\u7684\u8bdd\uff0c\u5bf9\u4e0a\u9762\u7684recursion tree\u8fdb\u884c\u5148\u5e8f\u904d\u5386\u5c31\u662f\u4e0b\u9762\u7684\u51fd\u6570\u7684\u8c03\u7528\u8fc7\u7a0b\uff1b\u8be5\u51fd\u6570\u7684\u76ee\u7684\u662f\u751f\u6210\u6240\u6709\u7684permutation\u7684\uff0c\u5373\u5b83\u9700\u8981\u7a77\u4e3e\uff1b \u751f\u6210permutation\u7684\u8fc7\u7a0b\u53ef\u4ee5\u7b80\u8ff0\u4e3a\uff1a \u4ecen\u4e2a\u5143\u7d20\u4e2d\u6311\u9009\u51fa\u4e00\u4e2a\u6765\u4f5c\u4e3a\u7b2c0\u4e2a\u5143\u7d20 \u4ecen-1\u4e2a\u5143\u7d20\u4e2d\u6311\u9009\u51fa\u4e00\u4e2a\u6765\u4f5c\u4e3a\u7b2c1\u4e2a\u5143\u7d20 \u4ecen-2\u4e2a\u5143\u7d20\u4e2d\u6311\u9009\u51fa\u4e00\u4e2a\u6765\u4f5c\u4e3a\u7b2c2\u4e2a\u5143\u7d20 ... \u4ece2\u4e2a\u5143\u7d20\u4e2d\u6311\u9009\u51fa\u4e00\u4e2a\u6765\u4f5c\u4e3a\u7b2cn-2\u4e2a\u5143\u7d20 \u4ece1\u4e2a\u5143\u7d20\u4e2d\u6311\u9009\u51fa\u4e00\u4e2a\u6765\u4f5c\u4e3a\u7b2cn-1\u4e2a\u5143\u7d20 \u663e\u7136\u4e0a\u8ff0\u8fc7\u7a0b\u662f\u4e00\u4e2a\u9012\u5f52\u7684\u8fc7\u7a0b\uff0c\u4e0a\u8ff0\u8fc7\u7a0b\u5982\u679c\u4f7f\u7528\u56fe\u5f62\u5316\u6765\u5c55\u793a\u7684\u8bdd\uff0c\u5176\u5b9e\u548c\u4e0a\u9762\u7684recursion tree\u662f\u5b8c\u7f8e\u5bf9\u5e94\u7684\uff1a recursion tree\u7684\u7b2c\u4e00\u5c42\u8282\u70b9\uff0c\u5bf9\u5e94\u4e86\u4ecen\uff08n=3\uff09\u4e2a\u5143\u7d20\u4e2d\u6311\u9009\u51fa\u4e00\u4e2a\u6765\u4f5c\u4e3a\u7b2c0\u4e2a\u5143\u7d20 recursion tree\u7684\u7b2c\u4e8c\u5c42\u8282\u70b9\uff0c\u5bf9\u5e94\u4e86\u4ecen-1\uff08n-1=2\uff09\u4e2a\u5143\u7d20\u4e2d\u6311\u9009\u51fa\u4e00\u4e2a\u6765\u4f5c\u4e3a\u7b2c1\u4e2a\u5143\u7d20 recursion tree\u4e2d\u7684\u6bcf\u4e00\u6761\u8def\u5f84\u5c31\u5bf9\u5e94\u4e86\u4e00\u4e2a\u7ec4\u5408\uff1b \u6240\u4ee5\uff0c\u5171\u6709$n \\times (n-1) \\times (n-2) \\times \\ldots \\times 2 \\times 1$\u79cd\u6392\u5217\uff0c$n \\times (n-1) \\times (n-2) \\times \\ldots \\times 2 \\times 1$\u662f\u548c\u4e0a\u8ff0recursion tree\u5b8c\u7f8e\u5bf9\u5e94\u7684: \u7b2c\u4e00\u4e2a\u5143\u7d20\u9009\u5b9a\u4e3a A \uff0c\u5bf9\u5e94\u4e86\u7b2c\u4e00\u68f5\u5b50\u6811\uff0c\u663e\u7136\u5171\u6709n\u68f5\u7c7b\u4f3c\u8fd9\u6837\u7684\u5b50\u6811\uff0c\u6240\u4ee5\u662f$n \\times$ \u7531\u4e8e\u6211\u4eec\u7684backtrack\u7b97\u6cd5\u76ee\u7684\u662f \u7a77\u4e3e \uff0c\u6240\u4ee5\u5728\u7b2c\u4e00\u4e2a\u5143\u7d20\u9009\u5b9a\u4e3a A \u540e\uff0c\u8fd8\u9700\u8981\u53bb\u5c1d\u8bd5\u7b2c\u4e00\u4e2a\u5143\u7d20\u9009\u5b9a\u4e3a B \u7684\u60c5\u51b5\uff0c\u8fd8\u9700\u8981\u53bb\u4ea7\u751f\u7b2c\u4e00\u4e2a\u5143\u7d20\u9009\u5b9a\u4e3a C \u7684\u60c5\u51b5\uff1b\u4ee5\u6b64\u7c7b\u63a8\uff0c\u5728\u7b2c\u4e8c\u4e2a\u5143\u7d20\u9009\u5b9a\u4e3a A \u540e\uff0c\u8fd8\u9700\u8981\u53bb\u5c1d\u8bd5\u7b2c\u4e8c\u4e2a\u5143\u7d20\u9009\u5b9a\u4e3a B \u7684\u60c5\u51b5\uff0c\u8fd8\u9700\u8981\u53bb\u4ea7\u751f\u7b2c\u4e8c\u4e2a\u5143\u7d20\u9009\u5b9a\u4e3a C \u7684\u60c5\u51b5\uff1b\u6240\u4ee5\u6211\u4eec\u9700\u8981\u56de\u6eaf\uff0c\u5373\u56de\u9000\u5230\u4e0a\u4e00\u5c42\u7684\u72b6\u6001\uff0c\u8fd9\u6837\u4e0a\u4e00\u5c42\u5c31\u80fd\u591f\u5c1d\u8bd5\u53e6\u5916\u4e00\u79cd\u60c5\u51b5\u4e86\uff1b\u5176\u5b9e\u8fd9\u5c31\u662f backtracking \u7b97\u6cd5\u7684\u6838\u5fc3\u601d\u60f3\u6240\u5728\u4e86\uff1b \u5bf9\u4e0a\u8ff0\u9012\u5f52\u6570\u8fdb\u884c\u5148\u5e8f\u904d\u5386\u5c31\u5bf9\u5e94\u4e86\u9012\u5f52\u51fd\u6570\u5b9e\u9645\u7684\u6267\u884c\u8fc7\u7a0b\uff0c\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u5f53\u7b2c\u4e00\u6b21\u7531\u4e0a\u5230\u4e0b\u7ecf\u8fc7\u4e00\u6761\u8fb9\u7684\u65f6\u5019\uff0c\u6267\u884cswap\u51fd\u6570\uff0c\u5f53\u7b2c\u4e8c\u6b21\u7531\u4e0b\u5230\u4e0a\u7ecf\u8fc7\u8be5\u8fb9\u7684\u65f6\u5019\uff0c\u6267\u884c\u65b9\u5411\u76f8\u53cd\u7684swap\u51fd\u6570\uff1b\u8fd9\u4e5f\u8bf4\u660e\u9012\u5f52\u8c03\u7528\u662f\u6df1\u5ea6\u4f18\u5148\u5730\u904d\u5386\u3002 \u90a3\u4e0a\u8ff0\u8fc7\u7a0b\u5982\u4f55\u4f7f\u7528\u9012\u5f52\u51fd\u6570\u6765\u8fdb\u884c\u5b9e\u73b0\u5462\uff1f\u8fd9\u5c31\u662f\u4e00\u4e2a\u7a0b\u5e8f\u5458\u9700\u8981\u8003\u8651\u7684\u95ee\u9898\u4e86\u3002\u5bf9\u4e8e\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u5e38\u5e38\u8003\u8651\u7684\u662f\u4f7f\u7528 backtracking \u7b97\u6cd5\uff1a","title":"\u7b97\u6cd5"},{"location":"Algorithm-design-paradigm/05-Backtracking/Permutation-and-combination/Enum-permutation/Backtrack-enum-all-permutations-of-string/#c","text":"// C++ program to print all // permutations with duplicates allowed #include <bits/stdc++.h> using namespace std; // Function to print permutations of string // This function takes three parameters: // 1. String // 2. Starting index of the string // 3. Ending index of the string. void permute(string a, int l, int r) { // Base case if (l == r) cout<<a<<endl; else { // Permutations made for (int i = l; i <= r; i++) { // Swapping done swap(a[l], a[i]); // Recursion called permute(a, l+1, r); //backtrack swap(a[l], a[i]); } } } // Driver Code int main() { string str = \"ABC\"; int n = str.size(); permute(str, 0, n-1); return 0; } // This is code is contributed by rathbhupendra","title":"C++"},{"location":"Algorithm-design-paradigm/05-Backtracking/Permutation-and-combination/Enum-permutation/Backtrack-enum-all-permutations-of-string/#c_1","text":"// C program to print all permutations with duplicates allowed #include <stdio.h> #include <string.h> /* Function to swap values at two pointers */ void swap(char *x, char *y) { char temp; temp = *x; *x = *y; *y = temp; } /* Function to print permutations of string This function takes three parameters: 1. String 2. Starting index of the string 3. Ending index of the string. */ void permute(char *a, int l, int r) { int i; if (l == r) printf(\"%s\\n\", a); else { for (i = l; i <= r; i++) { swap((a+l), (a+i)); permute(a, l+1, r); swap((a+l), (a+i)); //backtrack } } } /* Driver program to test above functions */ int main() { char str[] = \"ABC\"; int n = strlen(str); permute(str, 0, n-1); return 0; }","title":"C"},{"location":"Algorithm-design-paradigm/05-Backtracking/Permutation-and-combination/Enum-permutation/Enum-distinct-permutation-of-with-duplicate/","text":"Print all distinct permutations of a given string with duplicates \u95ee\u9898\u63cf\u8ff0 \u7b97\u6cd5 Print all distinct permutations of a given string with duplicates # \u95ee\u9898\u63cf\u8ff0 # Given a string that may contain duplicates, write a function to print all permutations of given string such that no permutation is repeated in output. Examples: Input: str[] = \"AB\" Output: AB BA Input: str[] = \"AA\" Output: AA Input: str[] = \"ABC\" Output: ABC ACB BAC BCA CBA CAB Input: str[] = \"ABA\" Output: ABA AAB BAA Input: str[] = \"ABCA\" Output: AABC AACB ABAC ABCA ACBA ACAB BAAC BACA BCAA CABA CAAB CBAA We have discussed an algorithm to print all permutations in below post. It is strongly recommended to refer below post as a prerequisite of this post. \u7b97\u6cd5 # Write a C program to print all permutations of a given string The algorithm discussed on above link doesn\u2019t handle duplicates. // Program to print all permutations of a // string in sorted order. #include <stdio.h> #include <stdlib.h> #include <string.h> /* Following function is needed for library function qsort(). */ int compare(const void* a, const void* b) { return (*(char*)a - *(char*)b); } // A utility function two swap two characters // a and b void swap(char* a, char* b) { char t = *a; *a = *b; *b = t; } // This function finds the index of the // smallest character which is greater // than 'first' and is present in str[l..h] int findCeil(char str[], char first, int l, int h) { // initialize index of ceiling element int ceilIndex = l; // Now iterate through rest of the // elements and find the smallest // character greater than 'first' for (int i = l + 1; i <= h; i++) if (str[i] > first && str[i] < str[ceilIndex]) ceilIndex = i; return ceilIndex; } // Print all permutations of str in sorted order void sortedPermutations(char str[]) { // Get size of string int size = strlen(str); // Sort the string in increasing order qsort(str, size, sizeof(str[0]), compare); // Print permutations one by one bool isFinished = false; while (!isFinished) { // print this permutation static int x = 1; printf(\"%d %s \\n\", x++, str); // Find the rightmost character // which is smaller than its next // character. Let us call it 'first // char' int i; for (i = size - 2; i >= 0; --i) if (str[i] < str[i + 1]) break; // If there is no such character, all // are sorted in decreasing order, // means we just printed the last // permutation and we are done. if (i == -1) isFinished = true; else { // Find the ceil of 'first char' // in right of first character. // Ceil of a character is the // smallest character greater // than it int ceilIndex = findCeil(str, str[i], i + 1, size - 1); // Swap first and second characters swap(&str[i], &str[ceilIndex]); // Sort the string on right of 'first char' qsort(str + i + 1, size - i - 1, sizeof(str[0]), compare); } } } // Driver program to test above function int main() { char str[] = \"ACBC\"; sortedPermutations(str); return 0; }","title":"Enum-distinct-permutation-of-with-duplicate"},{"location":"Algorithm-design-paradigm/05-Backtracking/Permutation-and-combination/Enum-permutation/Enum-distinct-permutation-of-with-duplicate/#print_all_distinct_permutations_of_a_given_string_with_duplicates","text":"","title":"Print all distinct permutations of a given string with duplicates"},{"location":"Algorithm-design-paradigm/05-Backtracking/Permutation-and-combination/Enum-permutation/Enum-distinct-permutation-of-with-duplicate/#_1","text":"Given a string that may contain duplicates, write a function to print all permutations of given string such that no permutation is repeated in output. Examples: Input: str[] = \"AB\" Output: AB BA Input: str[] = \"AA\" Output: AA Input: str[] = \"ABC\" Output: ABC ACB BAC BCA CBA CAB Input: str[] = \"ABA\" Output: ABA AAB BAA Input: str[] = \"ABCA\" Output: AABC AACB ABAC ABCA ACBA ACAB BAAC BACA BCAA CABA CAAB CBAA We have discussed an algorithm to print all permutations in below post. It is strongly recommended to refer below post as a prerequisite of this post.","title":"\u95ee\u9898\u63cf\u8ff0"},{"location":"Algorithm-design-paradigm/05-Backtracking/Permutation-and-combination/Enum-permutation/Enum-distinct-permutation-of-with-duplicate/#_2","text":"Write a C program to print all permutations of a given string The algorithm discussed on above link doesn\u2019t handle duplicates. // Program to print all permutations of a // string in sorted order. #include <stdio.h> #include <stdlib.h> #include <string.h> /* Following function is needed for library function qsort(). */ int compare(const void* a, const void* b) { return (*(char*)a - *(char*)b); } // A utility function two swap two characters // a and b void swap(char* a, char* b) { char t = *a; *a = *b; *b = t; } // This function finds the index of the // smallest character which is greater // than 'first' and is present in str[l..h] int findCeil(char str[], char first, int l, int h) { // initialize index of ceiling element int ceilIndex = l; // Now iterate through rest of the // elements and find the smallest // character greater than 'first' for (int i = l + 1; i <= h; i++) if (str[i] > first && str[i] < str[ceilIndex]) ceilIndex = i; return ceilIndex; } // Print all permutations of str in sorted order void sortedPermutations(char str[]) { // Get size of string int size = strlen(str); // Sort the string in increasing order qsort(str, size, sizeof(str[0]), compare); // Print permutations one by one bool isFinished = false; while (!isFinished) { // print this permutation static int x = 1; printf(\"%d %s \\n\", x++, str); // Find the rightmost character // which is smaller than its next // character. Let us call it 'first // char' int i; for (i = size - 2; i >= 0; --i) if (str[i] < str[i + 1]) break; // If there is no such character, all // are sorted in decreasing order, // means we just printed the last // permutation and we are done. if (i == -1) isFinished = true; else { // Find the ceil of 'first char' // in right of first character. // Ceil of a character is the // smallest character greater // than it int ceilIndex = findCeil(str, str[i], i + 1, size - 1); // Swap first and second characters swap(&str[i], &str[ceilIndex]); // Sort the string on right of 'first char' qsort(str + i + 1, size - i - 1, sizeof(str[0]), compare); } } } // Driver program to test above function int main() { char str[] = \"ACBC\"; sortedPermutations(str); return 0; }","title":"\u7b97\u6cd5"},{"location":"Algorithm-design-paradigm/05-Backtracking/Permutation-and-combination/Enum-permutation/Enum-permutation-reading-list/","text":"Generate permutation # Generate list of all possible permutations of a string # Write a program to print all permutations of a given string # Generating all permutations of a given string # Iterative program to generate distinct Permutations of a String # Generate list of all possible permutations of a string # Write a program to print all permutations of a given string # Generating all permutations of a given string #","title":"Enum-permutation-reading-list"},{"location":"Algorithm-design-paradigm/05-Backtracking/Permutation-and-combination/Enum-permutation/Enum-permutation-reading-list/#generate_permutation","text":"","title":"Generate permutation"},{"location":"Algorithm-design-paradigm/05-Backtracking/Permutation-and-combination/Enum-permutation/Enum-permutation-reading-list/#generate_list_of_all_possible_permutations_of_a_string","text":"","title":"Generate list of all possible permutations of a string"},{"location":"Algorithm-design-paradigm/05-Backtracking/Permutation-and-combination/Enum-permutation/Enum-permutation-reading-list/#write_a_program_to_print_all_permutations_of_a_given_string","text":"","title":"Write a program to print all permutations of a given string"},{"location":"Algorithm-design-paradigm/05-Backtracking/Permutation-and-combination/Enum-permutation/Enum-permutation-reading-list/#generating_all_permutations_of_a_given_string","text":"","title":"Generating all permutations of a given string"},{"location":"Algorithm-design-paradigm/05-Backtracking/Permutation-and-combination/Enum-permutation/Enum-permutation-reading-list/#iterative_program_to_generate_distinct_permutations_of_a_string","text":"","title":"Iterative program to generate distinct Permutations of a String"},{"location":"Algorithm-design-paradigm/05-Backtracking/Permutation-and-combination/Enum-permutation/Enum-permutation-reading-list/#generate_list_of_all_possible_permutations_of_a_string_1","text":"","title":"Generate list of all possible permutations of a string"},{"location":"Algorithm-design-paradigm/05-Backtracking/Permutation-and-combination/Enum-permutation/Enum-permutation-reading-list/#write_a_program_to_print_all_permutations_of_a_given_string_1","text":"","title":"Write a program to print all permutations of a given string"},{"location":"Algorithm-design-paradigm/05-Backtracking/Permutation-and-combination/Enum-permutation/Enum-permutation-reading-list/#generating_all_permutations_of_a_given_string_1","text":"","title":"Generating all permutations of a given string"},{"location":"Algorithm-design-paradigm/06-Branch-and-Bound-Algorithm/Branch-and-bound/","text":"Branch and bound Branch and bound #","title":"Branch-and-bound"},{"location":"Algorithm-design-paradigm/06-Branch-and-Bound-Algorithm/Branch-and-bound/#branch_and_bound","text":"","title":"Branch and bound"},{"location":"Algorithm-design-paradigm/06-Branch-and-Bound-Algorithm/Branch-and-cut/","text":"Branch and cut Branch and cut #","title":"Branch-and-cut"},{"location":"Algorithm-design-paradigm/06-Branch-and-Bound-Algorithm/Branch-and-cut/#branch_and_cut","text":"","title":"Branch and cut"},{"location":"Algorithm-design-paradigm/Heuristic/Heuristic(computer-science)/","text":"Heuristic (computer science) Heuristic (computer science) #","title":"Heuristic(computer-science)"},{"location":"Algorithm-design-paradigm/Heuristic/Heuristic(computer-science)/#heuristic_computer_science","text":"","title":"Heuristic (computer science)"},{"location":"Algorithm-design-paradigm/Heuristic/Heuristic-algorithm-reading-list/","text":"Heuristic algorithms #","title":"Heuristic-algorithm-reading-list"},{"location":"Algorithm-design-paradigm/Heuristic/Heuristic-algorithm-reading-list/#heuristic_algorithms","text":"","title":"Heuristic algorithms"},{"location":"Algorithm-design-paradigm/Heuristic/Heuristic-algorithms/","text":"Heuristic algorithms Heuristic algorithms #","title":"Heuristic-algorithms"},{"location":"Algorithm-design-paradigm/Heuristic/Heuristic-algorithms/#heuristic_algorithms","text":"","title":"Heuristic algorithms"},{"location":"Analysis-of-algorithms/","text":"\u5173\u4e8e\u672c\u7ae0 # \u672c\u7ae0\u7b80\u8ff0\u7b97\u6cd5\u5206\u6790\u7684\u5185\u5bb9\u3002","title":"Introduction"},{"location":"Analysis-of-algorithms/#_1","text":"\u672c\u7ae0\u7b80\u8ff0\u7b97\u6cd5\u5206\u6790\u7684\u5185\u5bb9\u3002","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"Analysis-of-algorithms/Combinatorial-explosion/","text":"Combinatorial explosion Combinatorial explosion #","title":"Combinatorial-explosion"},{"location":"Analysis-of-algorithms/Combinatorial-explosion/#combinatorial_explosion","text":"","title":"Combinatorial explosion"},{"location":"Application/Computational-problem/","text":"Computational problem # In theoretical computer science , a computational problem is a mathematical object representing a collection of questions that computers might be able to solve. For example, the problem of factoring \"Given a positive integer n , find a nontrivial prime factor of n .\" is a computational problem. Computational problems are one of the main objects of study in theoretical computer science. The field of algorithms studies methods of solving computational problems efficiently. The complementary field of computational complexity attempts to explain why certain computational problems are intractable for computers. A computational problem can be viewed as an infinite collection of instances together with a solution for every instance. For example, in the factoring problem, the instances are the integers n , and solutions are prime numbers p that describe nontrivial prime factors of n . It is conventional to represent both instances and solutions by binary strings , namely elements of {0, 1}*. For example, numbers can be represented as binary strings using the binary encoding. (For readability, we identify numbers with their binary encodings in the examples below.) Types of computational problems # decision problem search problem counting problem optimization problem combinatorial optimization function problem","title":"Computational-problem"},{"location":"Application/Computational-problem/#computational_problem","text":"In theoretical computer science , a computational problem is a mathematical object representing a collection of questions that computers might be able to solve. For example, the problem of factoring \"Given a positive integer n , find a nontrivial prime factor of n .\" is a computational problem. Computational problems are one of the main objects of study in theoretical computer science. The field of algorithms studies methods of solving computational problems efficiently. The complementary field of computational complexity attempts to explain why certain computational problems are intractable for computers. A computational problem can be viewed as an infinite collection of instances together with a solution for every instance. For example, in the factoring problem, the instances are the integers n , and solutions are prime numbers p that describe nontrivial prime factors of n . It is conventional to represent both instances and solutions by binary strings , namely elements of {0, 1}*. For example, numbers can be represented as binary strings using the binary encoding. (For readability, we identify numbers with their binary encodings in the examples below.)","title":"Computational problem"},{"location":"Application/Computational-problem/#types_of_computational_problems","text":"decision problem search problem counting problem optimization problem combinatorial optimization function problem","title":"Types of computational problems"},{"location":"Application/optimization/Linear-programming/wikipedia-Integer-programming/","text":"Integer programming Integer programming #","title":"wikipedia Integer programming"},{"location":"Application/optimization/Linear-programming/wikipedia-Integer-programming/#integer_programming","text":"","title":"Integer programming"},{"location":"Application/optimization/Linear-programming/wikipedia-Linear-programming/","text":"Linear programming Linear programming # Linear programming ( LP , also called linear optimization ) is a method to achieve the best outcome (such as maximum profit or lowest cost) in a mathematical model whose requirements are represented by linear relationships . Linear programming is a special case of mathematical programming (also known as mathematical optimization ). More formally, linear programming is a technique for the optimization of a linear objective function , subject to linear equality and linear inequality constraints . Its feasible region is a convex polytope , which is a set defined as the intersection of finitely many half spaces , each of which is defined by a linear inequality. Its objective function is a real -valued affine (linear) function defined on this polyhedron. A linear programming algorithm finds a point in the polyhedron where this function has the smallest (or largest) value if such a point exists. Linear programs are problems that can be expressed in canonical form as where x represents the vector of variables (to be determined), c and b are vectors of (known) coefficients, A is a (known) matrix of coefficients, and $ (\\cdot )^{\\mathrm {T} } $ is the matrix transpose . The expression to be maximized or minimized is called the objective function ( c T x in this case). The inequalities A x * \u2264 b and x \u2265 0 * are the constraints which specify a convex polytope over which the objective function is to be optimized. In this context, two vectors are comparable when they have the same dimensions. If every entry in the first is less-than or equal-to the corresponding entry in the second, then it can be said that the first vector is less-than or equal-to the second vector. Linear programming can be applied to various fields of study. It is widely used in mathematics, and to a lesser extent in business, economics , and for some engineering problems. Industries that use linear programming models include transportation, energy, telecommunications, and manufacturing. It has proven useful in modeling diverse types of problems in planning , routing , scheduling , assignment , and design.","title":"wikipedia Linear programming"},{"location":"Application/optimization/Linear-programming/wikipedia-Linear-programming/#linear_programming","text":"Linear programming ( LP , also called linear optimization ) is a method to achieve the best outcome (such as maximum profit or lowest cost) in a mathematical model whose requirements are represented by linear relationships . Linear programming is a special case of mathematical programming (also known as mathematical optimization ). More formally, linear programming is a technique for the optimization of a linear objective function , subject to linear equality and linear inequality constraints . Its feasible region is a convex polytope , which is a set defined as the intersection of finitely many half spaces , each of which is defined by a linear inequality. Its objective function is a real -valued affine (linear) function defined on this polyhedron. A linear programming algorithm finds a point in the polyhedron where this function has the smallest (or largest) value if such a point exists. Linear programs are problems that can be expressed in canonical form as where x represents the vector of variables (to be determined), c and b are vectors of (known) coefficients, A is a (known) matrix of coefficients, and $ (\\cdot )^{\\mathrm {T} } $ is the matrix transpose . The expression to be maximized or minimized is called the objective function ( c T x in this case). The inequalities A x * \u2264 b and x \u2265 0 * are the constraints which specify a convex polytope over which the objective function is to be optimized. In this context, two vectors are comparable when they have the same dimensions. If every entry in the first is less-than or equal-to the corresponding entry in the second, then it can be said that the first vector is less-than or equal-to the second vector. Linear programming can be applied to various fields of study. It is widely used in mathematics, and to a lesser extent in business, economics , and for some engineering problems. Industries that use linear programming models include transportation, energy, telecommunications, and manufacturing. It has proven useful in modeling diverse types of problems in planning , routing , scheduling , assignment , and design.","title":"Linear programming"},{"location":"Application/optimization/Optimization-Algorithms-methods-and-heuristics/Gradient-descent/","text":"Gradient descent Description Examples Gradient descent # Gradient descent is a first-order iterative optimization algorithm for finding the minimum of a function. To find a local minimum of a function using gradient descent, one takes steps proportional to the negative of the gradient (or approximate gradient) of the function at the current point. If, instead, one takes steps proportional to the positive of the gradient, one approaches a local maximum of that function; the procedure is then known as gradient ascent . SUMMARY : \u68af\u5ea6\u4e0a\u5347\u4e0e\u68af\u5ea6\u4e0b\u964d Gradient descent is also known as steepest descent \uff08\u6781\u901f\u4e0b\u964d\uff09. However, gradient descent should not be confused with the method of steepest descent for approximating integrals. Description # Gradient descent is based on the observation that if the multi-variable function $ F(\\mathbf {x} ) $ is defined and differentiable in a neighborhood of a point $ \\mathbf {a} $, then $ F(\\mathbf {x} ) $ decreases fastest if one goes from $ \\mathbf {a} $ in the direction of the negative gradient of $ F $ at $ \\mathbf {a} ,-\\nabla F(\\mathbf {a} ) $. It follows that, if $ \\mathbf {a} {n+1}=\\mathbf {a} {n}-\\gamma \\nabla F(\\mathbf {a} _{n}) $ for $ \\gamma \\in \\mathbb {R} {+} $ small enough, then $ F(\\mathbf {a {n}} )\\geq F(\\mathbf {a_{n+1}} ) $. In other words, the term $ \\gamma \\nabla F(\\mathbf {a} ) $ is subtracted from $ \\mathbf {a} $ because we want to move against the gradient, toward the minimum. With this observation in mind, one starts with a guess $ \\mathbf {x} {0} $ for a local minimum of $ F $, and considers the sequence $ \\mathbf {x} {0},\\mathbf {x} {1},\\mathbf {x} {2},\\ldots $ such that $ \\mathbf {x} {n+1}=\\mathbf {x} {n}-\\gamma {n}\\nabla F(\\mathbf {x} {n}),\\ n\\geq 0. $ We have a monotonic sequence $ F(\\mathbf {x} {0})\\geq F(\\mathbf {x} {1})\\geq F(\\mathbf {x} _{2})\\geq \\cdots , $ so hopefully the sequence $ (\\mathbf {x} _{n}) $ converges to the desired local minimum. Note that the value of the step size $ \\gamma $ is allowed to change at every iteration. With certain assumptions on the function $ F $ (for example, $ F $ convex and $ \\nabla F $ Lipschitz ) and particular choices of $ \\gamma $ (e.g., chosen either via a line search that satisfies the Wolfe conditions or the Barzilai-Borwein[ 1] method shown as following), $ \\gamma {n}={\\frac {\\left|\\left(\\mathbf {x} {n}-\\mathbf {x} {n-1}\\right)^{T}\\left[\\nabla F(\\mathbf {x} {n})-\\nabla F(\\mathbf {x} {n-1})\\right]\\right|}{\\left|\\nabla F(\\mathbf {x} {n})-\\nabla F(\\mathbf {x} _{n-1})\\right|^{2}}} $ convergence to a local minimum can be guaranteed. When the function $ F $ is convex , all local minima are also global minima , so in this case gradient descent can converge to the global solution. This process is illustrated in the adjacent picture. Here $ F $ is assumed to be defined on the plane, and that its graph has a bowl shape. The blue curves are the contour lines , that is, the regions on which the value of $ F $ is constant. A red arrow originating at a point shows the direction of the negative gradient at that point. Note that the (negative) gradient at a point is orthogonal to the contour line going through that point. We see that gradient descent leads us to the bottom of the bowl, that is, to the point where the value of the function $ F $ is minimal. Illustration of gradient descent on a series of level sets. Examples # Gradient descent has problems with pathological\uff08\u75c5\u6001\uff09 functions such as the Rosenbrock function shown here. $ f(x_{1},x_{2})=(1-x_{1})^{2}+100(x_{2}-{x_{1}}^{2})^{2}. $ The Rosenbrock function has a narrow curved valley which contains the minimum. The bottom of the valley is very flat. Because of the curved flat valley the optimization is zigzagging slowly with small step sizes towards the minimum. The zigzagging nature of the method is also evident below, where the gradient descent method is applied to $ F(x,y)=\\sin \\left({\\frac {1}{2}}x^{2}-{\\frac {1}{4}}y^{2}+3\\right)\\cos \\left(2x+1-e^{y}\\right). $","title":"Gradient descent"},{"location":"Application/optimization/Optimization-Algorithms-methods-and-heuristics/Gradient-descent/#gradient_descent","text":"Gradient descent is a first-order iterative optimization algorithm for finding the minimum of a function. To find a local minimum of a function using gradient descent, one takes steps proportional to the negative of the gradient (or approximate gradient) of the function at the current point. If, instead, one takes steps proportional to the positive of the gradient, one approaches a local maximum of that function; the procedure is then known as gradient ascent . SUMMARY : \u68af\u5ea6\u4e0a\u5347\u4e0e\u68af\u5ea6\u4e0b\u964d Gradient descent is also known as steepest descent \uff08\u6781\u901f\u4e0b\u964d\uff09. However, gradient descent should not be confused with the method of steepest descent for approximating integrals.","title":"Gradient descent"},{"location":"Application/optimization/Optimization-Algorithms-methods-and-heuristics/Gradient-descent/#description","text":"Gradient descent is based on the observation that if the multi-variable function $ F(\\mathbf {x} ) $ is defined and differentiable in a neighborhood of a point $ \\mathbf {a} $, then $ F(\\mathbf {x} ) $ decreases fastest if one goes from $ \\mathbf {a} $ in the direction of the negative gradient of $ F $ at $ \\mathbf {a} ,-\\nabla F(\\mathbf {a} ) $. It follows that, if $ \\mathbf {a} {n+1}=\\mathbf {a} {n}-\\gamma \\nabla F(\\mathbf {a} _{n}) $ for $ \\gamma \\in \\mathbb {R} {+} $ small enough, then $ F(\\mathbf {a {n}} )\\geq F(\\mathbf {a_{n+1}} ) $. In other words, the term $ \\gamma \\nabla F(\\mathbf {a} ) $ is subtracted from $ \\mathbf {a} $ because we want to move against the gradient, toward the minimum. With this observation in mind, one starts with a guess $ \\mathbf {x} {0} $ for a local minimum of $ F $, and considers the sequence $ \\mathbf {x} {0},\\mathbf {x} {1},\\mathbf {x} {2},\\ldots $ such that $ \\mathbf {x} {n+1}=\\mathbf {x} {n}-\\gamma {n}\\nabla F(\\mathbf {x} {n}),\\ n\\geq 0. $ We have a monotonic sequence $ F(\\mathbf {x} {0})\\geq F(\\mathbf {x} {1})\\geq F(\\mathbf {x} _{2})\\geq \\cdots , $ so hopefully the sequence $ (\\mathbf {x} _{n}) $ converges to the desired local minimum. Note that the value of the step size $ \\gamma $ is allowed to change at every iteration. With certain assumptions on the function $ F $ (for example, $ F $ convex and $ \\nabla F $ Lipschitz ) and particular choices of $ \\gamma $ (e.g., chosen either via a line search that satisfies the Wolfe conditions or the Barzilai-Borwein[ 1] method shown as following), $ \\gamma {n}={\\frac {\\left|\\left(\\mathbf {x} {n}-\\mathbf {x} {n-1}\\right)^{T}\\left[\\nabla F(\\mathbf {x} {n})-\\nabla F(\\mathbf {x} {n-1})\\right]\\right|}{\\left|\\nabla F(\\mathbf {x} {n})-\\nabla F(\\mathbf {x} _{n-1})\\right|^{2}}} $ convergence to a local minimum can be guaranteed. When the function $ F $ is convex , all local minima are also global minima , so in this case gradient descent can converge to the global solution. This process is illustrated in the adjacent picture. Here $ F $ is assumed to be defined on the plane, and that its graph has a bowl shape. The blue curves are the contour lines , that is, the regions on which the value of $ F $ is constant. A red arrow originating at a point shows the direction of the negative gradient at that point. Note that the (negative) gradient at a point is orthogonal to the contour line going through that point. We see that gradient descent leads us to the bottom of the bowl, that is, to the point where the value of the function $ F $ is minimal. Illustration of gradient descent on a series of level sets.","title":"Description"},{"location":"Application/optimization/Optimization-Algorithms-methods-and-heuristics/Gradient-descent/#examples","text":"Gradient descent has problems with pathological\uff08\u75c5\u6001\uff09 functions such as the Rosenbrock function shown here. $ f(x_{1},x_{2})=(1-x_{1})^{2}+100(x_{2}-{x_{1}}^{2})^{2}. $ The Rosenbrock function has a narrow curved valley which contains the minimum. The bottom of the valley is very flat. Because of the curved flat valley the optimization is zigzagging slowly with small step sizes towards the minimum. The zigzagging nature of the method is also evident below, where the gradient descent method is applied to $ F(x,y)=\\sin \\left({\\frac {1}{2}}x^{2}-{\\frac {1}{4}}y^{2}+3\\right)\\cos \\left(2x+1-e^{y}\\right). $","title":"Examples"},{"location":"Application/optimization/Optimization-Algorithms-methods-and-heuristics/Hill-climbing/","text":"","title":"Hill climbing"},{"location":"Application/optimization/Optimization-Algorithms-methods-and-heuristics/Iterative-method/","text":"Iterative method Iterative method # In computational mathematics , an iterative method is a mathematical procedure that uses an initial guess to generate a sequence of improving approximate solutions for a class of problems, in which the n -th approximation is derived from the previous ones. A specific implementation of an iterative method, including the termination criteria, is an algorithm of the iterative method. An iterative method is called convergent if the corresponding sequence converges for given initial approximations. A mathematically rigorous convergence analysis of an iterative method is usually performed; however, heuristic -based iterative methods are also common. In contrast, direct methods attempt to solve the problem by a finite sequence of operations. In the absence of rounding errors , direct methods would deliver an exact solution (like solving a linear system of equations $ A\\mathbf {x} =\\mathbf {b} $ by Gaussian elimination ). Iterative methods are often the only choice for nonlinear equations . However, iterative methods are often useful even for linear problems involving a large number of variables (sometimes of the order of millions), where direct methods would be prohibitively expensive (and in some cases impossible) even with the best available computing power.[ 1]","title":"Iterative method"},{"location":"Application/optimization/Optimization-Algorithms-methods-and-heuristics/Iterative-method/#iterative_method","text":"In computational mathematics , an iterative method is a mathematical procedure that uses an initial guess to generate a sequence of improving approximate solutions for a class of problems, in which the n -th approximation is derived from the previous ones. A specific implementation of an iterative method, including the termination criteria, is an algorithm of the iterative method. An iterative method is called convergent if the corresponding sequence converges for given initial approximations. A mathematically rigorous convergence analysis of an iterative method is usually performed; however, heuristic -based iterative methods are also common. In contrast, direct methods attempt to solve the problem by a finite sequence of operations. In the absence of rounding errors , direct methods would deliver an exact solution (like solving a linear system of equations $ A\\mathbf {x} =\\mathbf {b} $ by Gaussian elimination ). Iterative methods are often the only choice for nonlinear equations . However, iterative methods are often useful even for linear problems involving a large number of variables (sometimes of the order of millions), where direct methods would be prohibitively expensive (and in some cases impossible) even with the best available computing power.[ 1]","title":"Iterative method"},{"location":"Application/optimization/Optimization-Algorithms-methods-and-heuristics/Loss-function-or-Objective-function/","text":"Loss function Loss function #","title":"Loss function or Objective function"},{"location":"Application/optimization/Optimization-Algorithms-methods-and-heuristics/Loss-function-or-Objective-function/#loss_function","text":"","title":"Loss function"},{"location":"Application/optimization/Optimization-Algorithms-methods-and-heuristics/Mathematical-optimization/","text":"Mathematical optimization Major subfields Mathematical optimization # Mathematical optimization (alternatively spelled optimisation ) or mathematical programming is the selection of a best element (with regard to some criterion) from some set of available alternatives.[ 1] Optimization problems of sorts arise in all quantitative disciplines from computer science and engineering to operations research and economics , and the development of solution methods has been of interest in mathematics for centuries.[ 2] In the simplest case, an optimization problem consists of maximizing or minimizing a real function by systematically choosing input values from within an allowed set and computing the value of the function. The generalization of optimization theory and techniques to other formulations constitutes a large area of applied mathematics . More generally, optimization includes finding \"best available\" values of some objective function given a defined domain (or input), including a variety of different types of objective functions and different types of domains. Major subfields # Convex programming studies the case when the objective function is convex (minimization) or concave (maximization) and the constraint set is convex. This can be viewed as a particular case of nonlinear programming or as generalization of linear or convex quadratic programming. Linear programming (LP), a type of convex programming, studies the case in which the objective function f is linear and the constraints are specified using only linear equalities and inequalities. Such a constraint set is called a polyhedron or a polytope if it is bounded . Second order cone programming (SOCP) is a convex program, and includes certain types of quadratic programs. Semidefinite programming (SDP) is a subfield of convex optimization where the underlying variables are semidefinite matrices . It is a generalization of linear and convex quadratic programming. Conic programming is a general form of convex programming. LP, SOCP and SDP can all be viewed as conic programs with the appropriate type of cone. Geometric programming is a technique whereby objective and inequality constraints expressed as posynomials and equality constraints as monomials can be transformed into a convex program. Integer programming studies linear programs in which some or all variables are constrained to take on integer values. This is not convex, and in general much more difficult than regular linear programming. Quadratic programming allows the objective function to have quadratic terms, while the feasible set must be specified with linear equalities and inequalities. For specific forms of the quadratic term, this is a type of convex programming. Fractional programming studies optimization of ratios of two nonlinear functions. The special class of concave fractional programs can be transformed to a convex optimization problem. Nonlinear programming studies the general case in which the objective function or the constraints or both contain nonlinear parts. This may or may not be a convex program. In general, whether the program is convex affects the difficulty of solving it. Stochastic programming studies the case in which some of the constraints or parameters depend on random variables . Robust programming is, like stochastic programming, an attempt to capture uncertainty in the data underlying the optimization problem. Robust optimization aims to find solutions that are valid under all possible realizations of the uncertainties. Combinatorial optimization is concerned with problems where the set of feasible solutions is discrete or can be reduced to a discrete one. Stochastic optimization is used with random (noisy) function measurements or random inputs in the search process. Infinite-dimensional optimization studies the case when the set of feasible solutions is a subset of an infinite- dimensional space, such as a space of functions. Heuristics and metaheuristics make few or no assumptions about the problem being optimized. Usually, heuristics do not guarantee that any optimal solution need be found. On the other hand, heuristics are used to find approximate solutions for many complicated optimization problems. Constraint satisfaction studies the case in which the objective function f is constant (this is used in artificial intelligence, particularly in automated reasoning). Constraint programming is a programming paradigm wherein relations between variables are stated in the form of constraints. Disjunctive programming is used where at least one constraint must be satisfied but not all. It is of particular use in scheduling. Space mapping is a concept for modeling and optimization of an engineering system to high-fidelity (fine) model accuracy exploiting a suitable physically meaningful coarse or surrogate model . In a number of subfields, the techniques are designed primarily for optimization in dynamic contexts (that is, decision making over time): Calculus of variations seeks to optimize an action integral over some space to an extremum by varying a function of the coordinates. Optimal control theory is a generalization of the calculus of variations which introduces control policies. Dynamic programming is the approach to solve the stochastic optimization problem with stochastic, randomness, and unknown model parameters. It studies the case in which the optimization strategy is based on splitting the problem into smaller subproblems. The equation that describes the relationship between these subproblems is called the Bellman equation . Mathematical programming with equilibrium constraints is where the constraints include variational inequalities or complementarities .","title":"Mathematical optimization"},{"location":"Application/optimization/Optimization-Algorithms-methods-and-heuristics/Mathematical-optimization/#mathematical_optimization","text":"Mathematical optimization (alternatively spelled optimisation ) or mathematical programming is the selection of a best element (with regard to some criterion) from some set of available alternatives.[ 1] Optimization problems of sorts arise in all quantitative disciplines from computer science and engineering to operations research and economics , and the development of solution methods has been of interest in mathematics for centuries.[ 2] In the simplest case, an optimization problem consists of maximizing or minimizing a real function by systematically choosing input values from within an allowed set and computing the value of the function. The generalization of optimization theory and techniques to other formulations constitutes a large area of applied mathematics . More generally, optimization includes finding \"best available\" values of some objective function given a defined domain (or input), including a variety of different types of objective functions and different types of domains.","title":"Mathematical optimization"},{"location":"Application/optimization/Optimization-Algorithms-methods-and-heuristics/Mathematical-optimization/#major_subfields","text":"Convex programming studies the case when the objective function is convex (minimization) or concave (maximization) and the constraint set is convex. This can be viewed as a particular case of nonlinear programming or as generalization of linear or convex quadratic programming. Linear programming (LP), a type of convex programming, studies the case in which the objective function f is linear and the constraints are specified using only linear equalities and inequalities. Such a constraint set is called a polyhedron or a polytope if it is bounded . Second order cone programming (SOCP) is a convex program, and includes certain types of quadratic programs. Semidefinite programming (SDP) is a subfield of convex optimization where the underlying variables are semidefinite matrices . It is a generalization of linear and convex quadratic programming. Conic programming is a general form of convex programming. LP, SOCP and SDP can all be viewed as conic programs with the appropriate type of cone. Geometric programming is a technique whereby objective and inequality constraints expressed as posynomials and equality constraints as monomials can be transformed into a convex program. Integer programming studies linear programs in which some or all variables are constrained to take on integer values. This is not convex, and in general much more difficult than regular linear programming. Quadratic programming allows the objective function to have quadratic terms, while the feasible set must be specified with linear equalities and inequalities. For specific forms of the quadratic term, this is a type of convex programming. Fractional programming studies optimization of ratios of two nonlinear functions. The special class of concave fractional programs can be transformed to a convex optimization problem. Nonlinear programming studies the general case in which the objective function or the constraints or both contain nonlinear parts. This may or may not be a convex program. In general, whether the program is convex affects the difficulty of solving it. Stochastic programming studies the case in which some of the constraints or parameters depend on random variables . Robust programming is, like stochastic programming, an attempt to capture uncertainty in the data underlying the optimization problem. Robust optimization aims to find solutions that are valid under all possible realizations of the uncertainties. Combinatorial optimization is concerned with problems where the set of feasible solutions is discrete or can be reduced to a discrete one. Stochastic optimization is used with random (noisy) function measurements or random inputs in the search process. Infinite-dimensional optimization studies the case when the set of feasible solutions is a subset of an infinite- dimensional space, such as a space of functions. Heuristics and metaheuristics make few or no assumptions about the problem being optimized. Usually, heuristics do not guarantee that any optimal solution need be found. On the other hand, heuristics are used to find approximate solutions for many complicated optimization problems. Constraint satisfaction studies the case in which the objective function f is constant (this is used in artificial intelligence, particularly in automated reasoning). Constraint programming is a programming paradigm wherein relations between variables are stated in the form of constraints. Disjunctive programming is used where at least one constraint must be satisfied but not all. It is of particular use in scheduling. Space mapping is a concept for modeling and optimization of an engineering system to high-fidelity (fine) model accuracy exploiting a suitable physically meaningful coarse or surrogate model . In a number of subfields, the techniques are designed primarily for optimization in dynamic contexts (that is, decision making over time): Calculus of variations seeks to optimize an action integral over some space to an extremum by varying a function of the coordinates. Optimal control theory is a generalization of the calculus of variations which introduces control policies. Dynamic programming is the approach to solve the stochastic optimization problem with stochastic, randomness, and unknown model parameters. It studies the case in which the optimization strategy is based on splitting the problem into smaller subproblems. The equation that describes the relationship between these subproblems is called the Bellman equation . Mathematical programming with equilibrium constraints is where the constraints include variational inequalities or complementarities .","title":"Major subfields"},{"location":"Application/optimization/Optimization-Algorithms-methods-and-heuristics/Optimization-problem/","text":"Optimization problem Continuous optimization problem Combinatorial optimization problem Optimization problem # For broader coverage of this topic, see Mathematical optimization . In mathematics and computer science , an optimization problem is the problem of finding the best solution from all feasible solutions . Optimization problems can be divided into two categories depending on whether the variables are continuous or discrete . An optimization problem with discrete variables is known as a discrete optimization . In a discrete optimization problem, we are looking for an object such as an integer , permutation or graph from a countable set . Problems with continuous variables include constrained problems and multimodal problems. Continuous optimization problem # The standard form of a continuous optimization problem is[ 1] where $ f:\\mathbb {R} ^{n}\\to \\mathbb {R} $ is the objective function to be minimized over the n -variable vector $ x $, $ g_{i}(x)\\leq 0 $ are called inequality constraints $ h_{j}(x)=0 $ are called equality constraints , and $ m\\geq 0\\ and\\ p\\geq 0 $. If $ m $ and $ p $ equal 0, the problem is an unconstrained optimization problem. By convention, the standard form defines a minimization problem . A maximization problem can be treated by negating the objective function. Combinatorial optimization problem # Main article: Combinatorial optimization Formally, a combinatorial optimization problem $ A $ is a quadruple[ citation needed ] $ (I,f,m,g) $, where $ I $ is a set of instances; given an instance $ x\\in I $, $ f(x) $ is the set of feasible solutions; given an instance $ x $ and a feasible solution $ y $ of $ x $, $ m(x,y) $ denotes the measure of $ y $, which is usually a positive real . $ g $ is the goal function, and is either $ \\min $ or $ \\max $. The goal is then to find for some instance $ x $ an optimal solution , that is, a feasible solution $ y $ with For each combinatorial optimization problem, there is a corresponding decision problem that asks whether there is a feasible solution for some particular measure $ m_{0} $. For example, if there is a graph $ G $ which contains vertices $ u $ and $ v $, an optimization problem might be \"find a path from $ u $ to $ v $ that uses the fewest edges\". This problem might have an answer of, say, 4. A corresponding decision problem would be \"is there a path from $ u $ to $ v $ that uses 10 or fewer edges?\" This problem can be answered with a simple 'yes' or 'no'. In the field of approximation algorithms , algorithms are designed to find near-optimal solutions to hard problems. The usual decision version is then an inadequate definition of the problem since it only specifies acceptable solutions. Even though we could introduce suitable decision problems, the problem is more naturally characterized as an optimization problem.[ 2]","title":"Optimization problem"},{"location":"Application/optimization/Optimization-Algorithms-methods-and-heuristics/Optimization-problem/#optimization_problem","text":"For broader coverage of this topic, see Mathematical optimization . In mathematics and computer science , an optimization problem is the problem of finding the best solution from all feasible solutions . Optimization problems can be divided into two categories depending on whether the variables are continuous or discrete . An optimization problem with discrete variables is known as a discrete optimization . In a discrete optimization problem, we are looking for an object such as an integer , permutation or graph from a countable set . Problems with continuous variables include constrained problems and multimodal problems.","title":"Optimization problem"},{"location":"Application/optimization/Optimization-Algorithms-methods-and-heuristics/Optimization-problem/#continuous_optimization_problem","text":"The standard form of a continuous optimization problem is[ 1] where $ f:\\mathbb {R} ^{n}\\to \\mathbb {R} $ is the objective function to be minimized over the n -variable vector $ x $, $ g_{i}(x)\\leq 0 $ are called inequality constraints $ h_{j}(x)=0 $ are called equality constraints , and $ m\\geq 0\\ and\\ p\\geq 0 $. If $ m $ and $ p $ equal 0, the problem is an unconstrained optimization problem. By convention, the standard form defines a minimization problem . A maximization problem can be treated by negating the objective function.","title":"Continuous optimization problem"},{"location":"Application/optimization/Optimization-Algorithms-methods-and-heuristics/Optimization-problem/#combinatorial_optimization_problem","text":"Main article: Combinatorial optimization Formally, a combinatorial optimization problem $ A $ is a quadruple[ citation needed ] $ (I,f,m,g) $, where $ I $ is a set of instances; given an instance $ x\\in I $, $ f(x) $ is the set of feasible solutions; given an instance $ x $ and a feasible solution $ y $ of $ x $, $ m(x,y) $ denotes the measure of $ y $, which is usually a positive real . $ g $ is the goal function, and is either $ \\min $ or $ \\max $. The goal is then to find for some instance $ x $ an optimal solution , that is, a feasible solution $ y $ with For each combinatorial optimization problem, there is a corresponding decision problem that asks whether there is a feasible solution for some particular measure $ m_{0} $. For example, if there is a graph $ G $ which contains vertices $ u $ and $ v $, an optimization problem might be \"find a path from $ u $ to $ v $ that uses the fewest edges\". This problem might have an answer of, say, 4. A corresponding decision problem would be \"is there a path from $ u $ to $ v $ that uses 10 or fewer edges?\" This problem can be answered with a simple 'yes' or 'no'. In the field of approximation algorithms , algorithms are designed to find near-optimal solutions to hard problems. The usual decision version is then an inadequate definition of the problem since it only specifies acceptable solutions. Even though we could introduce suitable decision problems, the problem is more naturally characterized as an optimization problem.[ 2]","title":"Combinatorial optimization problem"},{"location":"Application/optimization/Optimization-Algorithms-methods-and-heuristics/Stochastic-gradient-descent/","text":"Stochastic gradient descent Background Stochastic gradient descent # Stochastic gradient descent (often abbreviated SGD ) is an iterative method for optimizing an objective function with suitable smoothness properties (e.g. differentiable or subdifferentiable ). It is called stochastic because the method uses randomly selected (or shuffled) samples to evaluate the gradients, hence SGD can be regarded as a stochastic approximation of gradient descent optimization. The ideas can be traced back[ 1] at least to the 1951 article titled \"A Stochastic Approximation Method\" by Herbert Robbins and Sutton Monro , who proposed with detailed analysis a root-finding method now called the Robbins\u2013Monro algorithm . Background # Main article: M-estimation See also: Estimating equation Both statistical estimation and machine learning consider the problem of minimizing an objective function that has the form of a sum: $ Q(w)={\\frac {1}{n}}\\sum {i=1}^{n}Q {i}(w), $ where the parameter $ w $ that minimizes $ Q(w) $ is to be estimated . Each summand function $ Q_{i} $ is typically associated with the $ i $-th observation in the data set (used for training). In classical statistics, sum-minimization problems arise in least squares and in maximum-likelihood estimation (for independent observations). The general class of estimators that arise as minimizers of sums are called M-estimators . However, in statistics, it has been long recognized that requiring even local minimization is too restrictive for some problems of maximum-likelihood estimation.[ 2] Therefore, contemporary statistical theorists often consider stationary points of the likelihood function (or zeros of its derivative, the score function , and other estimating equations ). The sum-minimization problem also arises for empirical risk minimization . In this case, $ Q_{i}(w) $ is the value of the loss function at $ i $-th example, and $ Q(w) $ is the empirical risk. When used to minimize the above function, a standard (or \"batch\") gradient descent method would perform the following iterations : where $ \\eta $ is a step size (sometimes called the learning rate in machine learning). In many cases, the summand functions have a simple form that enables inexpensive evaluations of the sum-function and the sum gradient. For example, in statistics, one-parameter exponential families allow economical function-evaluations and gradient-evaluations. However, in other cases, evaluating the sum-gradient may require expensive evaluations of the gradients from all summand functions. When the training set is enormous and no simple formulas exist, evaluating the sums of gradients becomes very expensive, because evaluating the gradient requires evaluating all the summand functions' gradients. To economize on the computational cost at every iteration, stochastic gradient descent samples a subset of summand functions at every step. This is very effective in the case of large-scale machine learning problems.[ 3]","title":"Stochastic gradient descent"},{"location":"Application/optimization/Optimization-Algorithms-methods-and-heuristics/Stochastic-gradient-descent/#stochastic_gradient_descent","text":"Stochastic gradient descent (often abbreviated SGD ) is an iterative method for optimizing an objective function with suitable smoothness properties (e.g. differentiable or subdifferentiable ). It is called stochastic because the method uses randomly selected (or shuffled) samples to evaluate the gradients, hence SGD can be regarded as a stochastic approximation of gradient descent optimization. The ideas can be traced back[ 1] at least to the 1951 article titled \"A Stochastic Approximation Method\" by Herbert Robbins and Sutton Monro , who proposed with detailed analysis a root-finding method now called the Robbins\u2013Monro algorithm .","title":"Stochastic gradient descent"},{"location":"Application/optimization/Optimization-Algorithms-methods-and-heuristics/Stochastic-gradient-descent/#background","text":"Main article: M-estimation See also: Estimating equation Both statistical estimation and machine learning consider the problem of minimizing an objective function that has the form of a sum: $ Q(w)={\\frac {1}{n}}\\sum {i=1}^{n}Q {i}(w), $ where the parameter $ w $ that minimizes $ Q(w) $ is to be estimated . Each summand function $ Q_{i} $ is typically associated with the $ i $-th observation in the data set (used for training). In classical statistics, sum-minimization problems arise in least squares and in maximum-likelihood estimation (for independent observations). The general class of estimators that arise as minimizers of sums are called M-estimators . However, in statistics, it has been long recognized that requiring even local minimization is too restrictive for some problems of maximum-likelihood estimation.[ 2] Therefore, contemporary statistical theorists often consider stationary points of the likelihood function (or zeros of its derivative, the score function , and other estimating equations ). The sum-minimization problem also arises for empirical risk minimization . In this case, $ Q_{i}(w) $ is the value of the loss function at $ i $-th example, and $ Q(w) $ is the empirical risk. When used to minimize the above function, a standard (or \"batch\") gradient descent method would perform the following iterations : where $ \\eta $ is a step size (sometimes called the learning rate in machine learning). In many cases, the summand functions have a simple form that enables inexpensive evaluations of the sum-function and the sum gradient. For example, in statistics, one-parameter exponential families allow economical function-evaluations and gradient-evaluations. However, in other cases, evaluating the sum-gradient may require expensive evaluations of the gradients from all summand functions. When the training set is enormous and no simple formulas exist, evaluating the sums of gradients becomes very expensive, because evaluating the gradient requires evaluating all the summand functions' gradients. To economize on the computational cost at every iteration, stochastic gradient descent samples a subset of summand functions at every step. This is very effective in the case of large-scale machine learning problems.[ 3]","title":"Background"},{"location":"Application/optimization/Optimization-Algorithms-methods-and-heuristics/iterative-optimization-algorithm/","text":"\u524d\u8a00 \u524d\u8a00 # \u5728\u9605\u8bfb Neural Networks Tutorial \u2013 A Pathway to Deep Learning \u7684 4.1 A simple example in code \u5c0f\u8282\u7684\u65f6\u5019\uff0c\u5176\u4e2d\u7684\u4f8b\u5b50\uff1a x_old = 0 # The value does not matter as long as abs(x_new - x_old) > precision x_new = 6 # The algorithm starts at x=6 gamma = 0.01 # step size precision = 0.00001 def df(x): y = 4 * x**3 - 9 * x**2 return y while abs(x_new - x_old) > precision: x_old = x_new x_new += -gamma * df(x_old) print(\"The local minimum occurs at %f\" % x_new) \u8fd9\u662f\u5178\u578b\u7684\u901a\u8fc7\u4e0d\u65ad\u8fdb\u884c\u8fed\u4ee3\uff0c\u4ece\u800c\u8ba1\u7b97\u5f97\u5230\u6700\u503c\u7684\u65b9\u6cd5\uff1b\u5728algorithm\u9886\u57df\u8fd9\u53eb\u4ec0\u4e48\u5462\uff1f\u4ee5\u6211\u7684\u76f4\u89c9\u6765\u770b\uff0c\u6211\u6682\u65f6\u53eb\u505a\u5b83\uff1aiterative optimization algorithm\uff1b\u5b83\u6709\u4e9b\u8d2a\u5fc3\u7b97\u6cd5\u7684\u610f\u5473\uff1b","title":"Iterative optimization algorithm"},{"location":"Application/optimization/Optimization-Algorithms-methods-and-heuristics/iterative-optimization-algorithm/#_1","text":"\u5728\u9605\u8bfb Neural Networks Tutorial \u2013 A Pathway to Deep Learning \u7684 4.1 A simple example in code \u5c0f\u8282\u7684\u65f6\u5019\uff0c\u5176\u4e2d\u7684\u4f8b\u5b50\uff1a x_old = 0 # The value does not matter as long as abs(x_new - x_old) > precision x_new = 6 # The algorithm starts at x=6 gamma = 0.01 # step size precision = 0.00001 def df(x): y = 4 * x**3 - 9 * x**2 return y while abs(x_new - x_old) > precision: x_old = x_new x_new += -gamma * df(x_old) print(\"The local minimum occurs at %f\" % x_new) \u8fd9\u662f\u5178\u578b\u7684\u901a\u8fc7\u4e0d\u65ad\u8fdb\u884c\u8fed\u4ee3\uff0c\u4ece\u800c\u8ba1\u7b97\u5f97\u5230\u6700\u503c\u7684\u65b9\u6cd5\uff1b\u5728algorithm\u9886\u57df\u8fd9\u53eb\u4ec0\u4e48\u5462\uff1f\u4ee5\u6211\u7684\u76f4\u89c9\u6765\u770b\uff0c\u6211\u6682\u65f6\u53eb\u505a\u5b83\uff1aiterative optimization algorithm\uff1b\u5b83\u6709\u4e9b\u8d2a\u5fc3\u7b97\u6cd5\u7684\u610f\u5473\uff1b","title":"\u524d\u8a00"},{"location":"Application/search/Search-algorithm/","text":"Search algorithm Classes For virtual search spaces For sub-structures of a given structure Search for the maximum of a function For quantum computers See also Bibliography Books Search algorithm # NOTE: \u8fd9\u7bc7\u6587\u7ae0\u5bf9search algorithm\u7684\u63cf\u8ff0\u662f\u975e\u5e38\u597d\u7684\uff0c\u5c24\u5176\u662f\u5bf9\u95ee\u9898\u7684\u5206\u7c7b In computer science , a search algorithm is any algorithm which solves the search problem , namely, to retrieve information stored within some data structure, or calculated in the search space \uff08\u53ef\u884c\u57df\u3001\u89e3\u7a7a\u95f4\uff09 of a problem domain , either with discrete or continuous values . Specific applications of search algorithms include: Problems in combinatorial optimization , such as: The vehicle routing problem , a form of shortest path problem \uff08\u6700\u77ed\u8def\u5f84\u95ee\u9898\uff09 The knapsack problem : Given a set of items, each with a weight and a value, determine the number of each item to include in a collection so that the total weight is less than or equal to a given limit and the total value is as large as possible. The nurse scheduling problem Problems in constraint satisfaction , such as: The map coloring problem Filling in a sudoku or crossword puzzle In game theory and especially combinatorial game theory , choosing the best move to make next (such as with the minmax algorithm) Finding a combination or password from the whole set of possibilities Factoring an integer (an important problem in cryptography ) Optimizing an industrial process, such as a chemical reaction , by changing the parameters of the process (like temperature, pressure, and pH) Retrieving a record from a database Finding the maximum or minimum value in a list or array Checking to see if a given value is present in a set of values NOTE : Search algorithm \u7684\u4e00\u4e2a\u4e3b\u8981\u95ee\u9898\u5c31\u662f\u786e\u5b9a search space \uff0c\u672c\u6587\u4e2d search space \u6240\u94fe\u63a5\u7684\u6587\u7ae0\u6240\u63cf\u8ff0\u7684\u5176\u5b9e\u662f\u53ef\u884c\u57df\uff0c\u53ef\u884c\u57df\u7684\u542b\u4e49\u53ef\u80fd\u6709\u4e9b\u9650\u5236\uff0c\u4f46\u662f\u4e5f\u53ef\u4ee5\u7528\uff1b search space \u53ef\u80fd\u662fvirtual spaces\uff08\u5982backtracing\u5728\u89e3\u7a7a\u95f4\u4e2d\u641c\u7d22\uff09\u4e5f\u53ef\u80fd\u662f\u7684\u786e\u5b58\u5728\u4e00\u4e2adata structure\uff08\u5982\u4e8c\u5206\u641c\u7d22\u5728\u4e00\u4e2asorted array\u4e2d\u8fdb\u884c\u641c\u7d22\uff09\uff1b\u6b63\u5982\u4e0b\u9762\u4f1a\u8fdb\u884c\u5206\u7c7b\uff1a - For virtual search spaces - For sub-structures of a given structure - Search for the maximum of a function The classic search problems described above and web search are both problems in information retrieval , but are generally studied as separate subfields and are solved and evaluated differently. are generally focused on filtering and that find documents most relevant to human queries. Classic search algorithms are typically evaluated on how fast they can find a solution, and whether or not that solution is guaranteed to be optimal. Though information retrieval algorithms must be fast, the quality of ranking is more important, as is whether or not good results have been left out and bad results included. The appropriate search algorithm often depends on the data structure being searched, and may also include prior knowledge about the data. Some database structures are specially constructed to make search algorithms faster or more efficient, such as a search tree , hash map , or a database index . [ 1] [ 2] -2) Search algorithms can be classified based on their mechanism of searching. Linear search algorithms check every record for the one associated with a target key in a linear fashion.[ 3] -3) Binary, or half interval searches , repeatedly target the center of the search structure and divide the search space in half. Comparison search algorithms improve on linear searching by successively eliminating records based on comparisons of the keys until the target record is found, and can be applied on data structures with a defined order.[ 4] -4) Digital search algorithms work based on the properties of digits in data structures that use numerical keys.[ 5] Finally, hashing directly maps keys to records based on a hash function .[ 6] Searches outside a linear search require that the data be sorted in some way. Algorithms are often evaluated by their computational complexity , or maximum theoretical run time. Binary search functions, for example, have a maximum complexity of O (log n ), or logarithmic time. This means that the maximum number of operations needed to find the search target is a logarithmic function of the size of the search space . Classes # For virtual search spaces # Algorithms for searching virtual spaces are used in the constraint satisfaction problem , where the goal is to find a set of value assignments to certain variables that will satisfy specific mathematical equations and inequations / equalities. They are also used when the goal is to find a variable assignment that will maximize or minimize a certain function of those variables. Algorithms for these problems include the basic brute-force search (also called \"na\u00efve\" or \"uninformed\" search), and a variety of heuristics that try to exploit partial knowledge about the structure of this space, such as linear relaxation, constraint generation, and constraint propagation . An important subclass are the local search methods, that view the elements of the search space as the vertices \uff08\u70b9\uff09 of a graph, with edges defined by a set of heuristics applicable to the case; and scan the space by moving from item to item along the edges, for example according to the steepest descent or best-first criterion, or in a stochastic search . This category includes a great variety of general metaheuristic methods, such as simulated annealing , tabu search , A-teams, and genetic programming , that combine arbitrary heuristics in specific ways. This class also includes various tree search algorithms , that view the elements as vertices of a tree , and traverse that tree in some special order. Examples of the latter include the exhaustive methods such as depth-first search and breadth-first search , as well as various heuristic-based search tree pruning methods such as backtracking and branch and bound . Unlike general metaheuristics, which at best work only in a probabilistic sense, many of these tree-search methods are guaranteed to find the exact or optimal solution, if given enough time. This is called \" completeness \". Another important sub-class consists of algorithms for exploring the game tree of multiple-player games, such as chess or backgammon \uff08\u53cc\u9646\u68cb\uff09, whose nodes consist of all possible game situations that could result from the current situation. The goal in these problems is to find the move that provides the best chance of a win, taking into account all possible moves of the opponent(s). Similar problems occur when humans or machines have to make successive decisions whose outcomes are not entirely under one's control, such as in robot guidance or in marketing , financial , or military strategy planning. This kind of problem \u2014 combinatorial search \uff08\u7ec4\u5408\u641c\u7d22\uff09 \u2014 has been extensively studied in the context of artificial intelligence . Examples of algorithms for this class are the minimax algorithm , alpha\u2013beta pruning , * Informational search [ 7] and the A* algorithm . For sub-structures of a given structure # The name \"combinatorial search\"\uff08\u7ec4\u5408\u641c\u7d22\uff09 is generally used for algorithms that look for a specific sub-structure of a given discrete structure , such as a graph, a string , a finite group , and so on. The term combinatorial optimization is typically used when the goal is to find a sub-structure with a maximum (or minimum) value of some parameter. (Since the sub-structure is usually represented in the computer by a set of integer variables with constraints, these problems can be viewed as special cases of constraint satisfaction or discrete optimization; but they are usually formulated and solved in a more abstract setting where the internal representation is not explicitly mentioned.) An important and extensively studied subclass are the graph algorithms , in particular graph traversal algorithms, for finding specific sub-structures in a given graph \u2014 such as subgraphs , paths , circuits, and so on. Examples include Dijkstra's algorithm , Kruskal's algorithm , the nearest neighbour algorithm , and Prim's algorithm . Another important subclass of this category are the string searching algorithms , that search for patterns within strings. Two famous examples are the Boyer\u2013Moore and Knuth\u2013Morris\u2013Pratt algorithms , and several algorithms based on the suffix tree data structure. Search for the maximum of a function # In 1953, American statistician Jack Kiefer devised Fibonacci search which can be used to find the maximum of a unimodal\uff08\u5355\u5cf0\uff09 function and has many other applications in computer science. For quantum computers # There are also search methods designed for quantum computers , like Grover's algorithm , that are theoretically faster than linear or brute-force search even without the help of data structures or heuristics. See also # Backward induction Content-addressable memory hardware Dual-phase evolution Linear search problem No free lunch in search and optimization Recommender system , also use statistical methods to rank results in very large data sets Search engine (computing) Search game Selection algorithm Solver Sorting algorithm \u2013 An algorithm that arranges lists in order, necessary for executing certain search algorithms Web search engine \u2013 Software system that is designed to search for information on the World Wide Web Categories: Category:Search algorithms Bibliography # Books # Knuth, Donald (1998). Sorting and Searching . The Art of Computer Programming . 3 (2nd ed.). Reading, MA: Addison-Wesley Professional. d","title":"Search-algorithm"},{"location":"Application/search/Search-algorithm/#search_algorithm","text":"NOTE: \u8fd9\u7bc7\u6587\u7ae0\u5bf9search algorithm\u7684\u63cf\u8ff0\u662f\u975e\u5e38\u597d\u7684\uff0c\u5c24\u5176\u662f\u5bf9\u95ee\u9898\u7684\u5206\u7c7b In computer science , a search algorithm is any algorithm which solves the search problem , namely, to retrieve information stored within some data structure, or calculated in the search space \uff08\u53ef\u884c\u57df\u3001\u89e3\u7a7a\u95f4\uff09 of a problem domain , either with discrete or continuous values . Specific applications of search algorithms include: Problems in combinatorial optimization , such as: The vehicle routing problem , a form of shortest path problem \uff08\u6700\u77ed\u8def\u5f84\u95ee\u9898\uff09 The knapsack problem : Given a set of items, each with a weight and a value, determine the number of each item to include in a collection so that the total weight is less than or equal to a given limit and the total value is as large as possible. The nurse scheduling problem Problems in constraint satisfaction , such as: The map coloring problem Filling in a sudoku or crossword puzzle In game theory and especially combinatorial game theory , choosing the best move to make next (such as with the minmax algorithm) Finding a combination or password from the whole set of possibilities Factoring an integer (an important problem in cryptography ) Optimizing an industrial process, such as a chemical reaction , by changing the parameters of the process (like temperature, pressure, and pH) Retrieving a record from a database Finding the maximum or minimum value in a list or array Checking to see if a given value is present in a set of values NOTE : Search algorithm \u7684\u4e00\u4e2a\u4e3b\u8981\u95ee\u9898\u5c31\u662f\u786e\u5b9a search space \uff0c\u672c\u6587\u4e2d search space \u6240\u94fe\u63a5\u7684\u6587\u7ae0\u6240\u63cf\u8ff0\u7684\u5176\u5b9e\u662f\u53ef\u884c\u57df\uff0c\u53ef\u884c\u57df\u7684\u542b\u4e49\u53ef\u80fd\u6709\u4e9b\u9650\u5236\uff0c\u4f46\u662f\u4e5f\u53ef\u4ee5\u7528\uff1b search space \u53ef\u80fd\u662fvirtual spaces\uff08\u5982backtracing\u5728\u89e3\u7a7a\u95f4\u4e2d\u641c\u7d22\uff09\u4e5f\u53ef\u80fd\u662f\u7684\u786e\u5b58\u5728\u4e00\u4e2adata structure\uff08\u5982\u4e8c\u5206\u641c\u7d22\u5728\u4e00\u4e2asorted array\u4e2d\u8fdb\u884c\u641c\u7d22\uff09\uff1b\u6b63\u5982\u4e0b\u9762\u4f1a\u8fdb\u884c\u5206\u7c7b\uff1a - For virtual search spaces - For sub-structures of a given structure - Search for the maximum of a function The classic search problems described above and web search are both problems in information retrieval , but are generally studied as separate subfields and are solved and evaluated differently. are generally focused on filtering and that find documents most relevant to human queries. Classic search algorithms are typically evaluated on how fast they can find a solution, and whether or not that solution is guaranteed to be optimal. Though information retrieval algorithms must be fast, the quality of ranking is more important, as is whether or not good results have been left out and bad results included. The appropriate search algorithm often depends on the data structure being searched, and may also include prior knowledge about the data. Some database structures are specially constructed to make search algorithms faster or more efficient, such as a search tree , hash map , or a database index . [ 1] [ 2] -2) Search algorithms can be classified based on their mechanism of searching. Linear search algorithms check every record for the one associated with a target key in a linear fashion.[ 3] -3) Binary, or half interval searches , repeatedly target the center of the search structure and divide the search space in half. Comparison search algorithms improve on linear searching by successively eliminating records based on comparisons of the keys until the target record is found, and can be applied on data structures with a defined order.[ 4] -4) Digital search algorithms work based on the properties of digits in data structures that use numerical keys.[ 5] Finally, hashing directly maps keys to records based on a hash function .[ 6] Searches outside a linear search require that the data be sorted in some way. Algorithms are often evaluated by their computational complexity , or maximum theoretical run time. Binary search functions, for example, have a maximum complexity of O (log n ), or logarithmic time. This means that the maximum number of operations needed to find the search target is a logarithmic function of the size of the search space .","title":"Search algorithm"},{"location":"Application/search/Search-algorithm/#classes","text":"","title":"Classes"},{"location":"Application/search/Search-algorithm/#for_virtual_search_spaces","text":"Algorithms for searching virtual spaces are used in the constraint satisfaction problem , where the goal is to find a set of value assignments to certain variables that will satisfy specific mathematical equations and inequations / equalities. They are also used when the goal is to find a variable assignment that will maximize or minimize a certain function of those variables. Algorithms for these problems include the basic brute-force search (also called \"na\u00efve\" or \"uninformed\" search), and a variety of heuristics that try to exploit partial knowledge about the structure of this space, such as linear relaxation, constraint generation, and constraint propagation . An important subclass are the local search methods, that view the elements of the search space as the vertices \uff08\u70b9\uff09 of a graph, with edges defined by a set of heuristics applicable to the case; and scan the space by moving from item to item along the edges, for example according to the steepest descent or best-first criterion, or in a stochastic search . This category includes a great variety of general metaheuristic methods, such as simulated annealing , tabu search , A-teams, and genetic programming , that combine arbitrary heuristics in specific ways. This class also includes various tree search algorithms , that view the elements as vertices of a tree , and traverse that tree in some special order. Examples of the latter include the exhaustive methods such as depth-first search and breadth-first search , as well as various heuristic-based search tree pruning methods such as backtracking and branch and bound . Unlike general metaheuristics, which at best work only in a probabilistic sense, many of these tree-search methods are guaranteed to find the exact or optimal solution, if given enough time. This is called \" completeness \". Another important sub-class consists of algorithms for exploring the game tree of multiple-player games, such as chess or backgammon \uff08\u53cc\u9646\u68cb\uff09, whose nodes consist of all possible game situations that could result from the current situation. The goal in these problems is to find the move that provides the best chance of a win, taking into account all possible moves of the opponent(s). Similar problems occur when humans or machines have to make successive decisions whose outcomes are not entirely under one's control, such as in robot guidance or in marketing , financial , or military strategy planning. This kind of problem \u2014 combinatorial search \uff08\u7ec4\u5408\u641c\u7d22\uff09 \u2014 has been extensively studied in the context of artificial intelligence . Examples of algorithms for this class are the minimax algorithm , alpha\u2013beta pruning , * Informational search [ 7] and the A* algorithm .","title":"For virtual search spaces"},{"location":"Application/search/Search-algorithm/#for_sub-structures_of_a_given_structure","text":"The name \"combinatorial search\"\uff08\u7ec4\u5408\u641c\u7d22\uff09 is generally used for algorithms that look for a specific sub-structure of a given discrete structure , such as a graph, a string , a finite group , and so on. The term combinatorial optimization is typically used when the goal is to find a sub-structure with a maximum (or minimum) value of some parameter. (Since the sub-structure is usually represented in the computer by a set of integer variables with constraints, these problems can be viewed as special cases of constraint satisfaction or discrete optimization; but they are usually formulated and solved in a more abstract setting where the internal representation is not explicitly mentioned.) An important and extensively studied subclass are the graph algorithms , in particular graph traversal algorithms, for finding specific sub-structures in a given graph \u2014 such as subgraphs , paths , circuits, and so on. Examples include Dijkstra's algorithm , Kruskal's algorithm , the nearest neighbour algorithm , and Prim's algorithm . Another important subclass of this category are the string searching algorithms , that search for patterns within strings. Two famous examples are the Boyer\u2013Moore and Knuth\u2013Morris\u2013Pratt algorithms , and several algorithms based on the suffix tree data structure.","title":"For sub-structures of a given structure"},{"location":"Application/search/Search-algorithm/#search_for_the_maximum_of_a_function","text":"In 1953, American statistician Jack Kiefer devised Fibonacci search which can be used to find the maximum of a unimodal\uff08\u5355\u5cf0\uff09 function and has many other applications in computer science.","title":"Search for the maximum of a function"},{"location":"Application/search/Search-algorithm/#for_quantum_computers","text":"There are also search methods designed for quantum computers , like Grover's algorithm , that are theoretically faster than linear or brute-force search even without the help of data structures or heuristics.","title":"For quantum computers"},{"location":"Application/search/Search-algorithm/#see_also","text":"Backward induction Content-addressable memory hardware Dual-phase evolution Linear search problem No free lunch in search and optimization Recommender system , also use statistical methods to rank results in very large data sets Search engine (computing) Search game Selection algorithm Solver Sorting algorithm \u2013 An algorithm that arranges lists in order, necessary for executing certain search algorithms Web search engine \u2013 Software system that is designed to search for information on the World Wide Web Categories: Category:Search algorithms","title":"See also"},{"location":"Application/search/Search-algorithm/#bibliography","text":"","title":"Bibliography"},{"location":"Application/search/Search-algorithm/#books","text":"Knuth, Donald (1998). Sorting and Searching . The Art of Computer Programming . 3 (2nd ed.). Reading, MA: Addison-Wesley Professional. d","title":"Books"},{"location":"Application/search/Graph-and-tree-search-algorithm/A-star-search-algorithm/","text":"","title":"A-star-search-algorithm"},{"location":"Application/search/Graph-and-tree-search-algorithm/B-star-search-algorithm/","text":"","title":"B-star-search-algorithm"},{"location":"Application/search/Graph-and-tree-search-algorithm/Beam-search/","text":"","title":"Beam-search"},{"location":"Application/search/Graph-and-tree-search-algorithm/Iterative-deepening-depth-first-search/","text":"","title":"Iterative-deepening-depth-first-search"},{"location":"Application/sorting/","text":"\u5173\u4e8e\u672c\u7ae0 # \u672c\u7ae0\u63cf\u8ff0\u6392\u5e8f\u7b97\u6cd5\u3002","title":"Introduction"},{"location":"Application/sorting/#_1","text":"\u672c\u7ae0\u63cf\u8ff0\u6392\u5e8f\u7b97\u6cd5\u3002","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"Application/sorting/merge-sorts/Merge-algorithm/","text":"Merge algorithm Application Merging two lists K-way merging Merge algorithm # Merge algorithms are a family of algorithms that take multiple sorted lists as input and produce a single list as output, containing all the elements of the inputs lists in sorted order. These algorithms are used as subroutines in various sorting algorithms , most famously merge sort . Application # The merge algorithm plays a critical role in the merge sort algorithm, a comparison-based sorting algorithm . Conceptually, merge sort algorithm consists of two steps: Recursively divide the list into sublists of (roughly) equal length, until each sublist contains only one element, or in the case of iterative (bottom up) merge sort, consider a list of n elements as n sub-lists of size 1. A list containing a single element is, by definition, sorted. Repeatedly merge sublists to create a new sorted sublist until the single list contains all elements. The single list is the sorted list. The merge algorithm is used repeatedly in the merge sort algorithm . An example merge sort is given in the illustration. It starts with an unsorted array of 7 integers. The array is divided into 7 partitions; each partition contains 1 element and is sorted. The sorted partitions are then merged to produce larger, sorted, partitions, until 1 partition, the sorted array, is left. Merging two lists # Merging two sorted lists into one can be done in linear time and linear space. The following pseudocode demonstrates an algorithm that merges input lists (either linked lists or arrays ) A and B into a new list C .[ 1] [ 2] :104 The function head yields the first element of a list; \"dropping\" an element means removing it from its list, typically by incrementing a pointer or index. algorithm merge(A, B) is inputs A, B : list returns list C := new empty list while A is not empty and B is not empty do if head(A) \u2264 head(B) then append head(A) to C drop the head of A else append head(B) to C drop the head of B // By now, either A or B is empty. It remains to empty the other input list. while A is not empty do append head(A) to C drop the head of A while B is not empty do append head(B) to C drop the head of B return C When the inputs are linked lists, this algorithm can be implemented to use only a constant amount of working space; the pointers in the lists' nodes can be reused for bookkeeping and for constructing the final merged list. In the merge sort algorithm, this subroutine is typically used to merge two sub-arrays A[lo..mid], A[mid..hi] of a single array A. This can be done by copying the sub-arrays into a temporary array, then applying the merge algorithm above.[ 1] The allocation of a temporary array can be avoided, but at the expense of speed and programming ease. Various in-place merge algorithms have been devised,[ 3] sometimes sacrificing the linear-time bound to produce an O ( n log n ) algorithm;[ 4] see Merge sort \u00a7 Variants for discussion. K-way merging # Main article: K-way merge algorithm k -way merging generalizes binary merging to an arbitrary number k of sorted input lists. Applications of k -way merging arise in various sorting algorithms, including patience sorting [ 5] and an external sorting algorithm that divides its input into k = 1/ M \u2212 1 blocks that fit in memory, sorts these one by one, then merges these blocks.[ 2] :119\u2013120 Several solutions to this problem exist. A naive solution is to do a loop over the k lists to pick off the minimum element each time, and repeat this loop until all lists are empty: Input: a list of k lists. While any of the lists is non-empty: Loop over the lists to find the one with the minimum first element. Output the minimum element and remove it from its list. In the worst case , this algorithm performs ( k \u22121)( n \u2212 k /2) element comparisons to perform its work if there are a total of n elements in the lists.[ 6] It can be improved by storing the lists in a priority queue ( min-heap ) keyed by their first element: Build a min-heap h of the k lists, using the first element as the key. While any of the lists is non-empty: Let i = find-min( h ). Output the first element of list i and remove it from its list. Re-heapify h .","title":"Merge-algorithm"},{"location":"Application/sorting/merge-sorts/Merge-algorithm/#merge_algorithm","text":"Merge algorithms are a family of algorithms that take multiple sorted lists as input and produce a single list as output, containing all the elements of the inputs lists in sorted order. These algorithms are used as subroutines in various sorting algorithms , most famously merge sort .","title":"Merge algorithm"},{"location":"Application/sorting/merge-sorts/Merge-algorithm/#application","text":"The merge algorithm plays a critical role in the merge sort algorithm, a comparison-based sorting algorithm . Conceptually, merge sort algorithm consists of two steps: Recursively divide the list into sublists of (roughly) equal length, until each sublist contains only one element, or in the case of iterative (bottom up) merge sort, consider a list of n elements as n sub-lists of size 1. A list containing a single element is, by definition, sorted. Repeatedly merge sublists to create a new sorted sublist until the single list contains all elements. The single list is the sorted list. The merge algorithm is used repeatedly in the merge sort algorithm . An example merge sort is given in the illustration. It starts with an unsorted array of 7 integers. The array is divided into 7 partitions; each partition contains 1 element and is sorted. The sorted partitions are then merged to produce larger, sorted, partitions, until 1 partition, the sorted array, is left.","title":"Application"},{"location":"Application/sorting/merge-sorts/Merge-algorithm/#merging_two_lists","text":"Merging two sorted lists into one can be done in linear time and linear space. The following pseudocode demonstrates an algorithm that merges input lists (either linked lists or arrays ) A and B into a new list C .[ 1] [ 2] :104 The function head yields the first element of a list; \"dropping\" an element means removing it from its list, typically by incrementing a pointer or index. algorithm merge(A, B) is inputs A, B : list returns list C := new empty list while A is not empty and B is not empty do if head(A) \u2264 head(B) then append head(A) to C drop the head of A else append head(B) to C drop the head of B // By now, either A or B is empty. It remains to empty the other input list. while A is not empty do append head(A) to C drop the head of A while B is not empty do append head(B) to C drop the head of B return C When the inputs are linked lists, this algorithm can be implemented to use only a constant amount of working space; the pointers in the lists' nodes can be reused for bookkeeping and for constructing the final merged list. In the merge sort algorithm, this subroutine is typically used to merge two sub-arrays A[lo..mid], A[mid..hi] of a single array A. This can be done by copying the sub-arrays into a temporary array, then applying the merge algorithm above.[ 1] The allocation of a temporary array can be avoided, but at the expense of speed and programming ease. Various in-place merge algorithms have been devised,[ 3] sometimes sacrificing the linear-time bound to produce an O ( n log n ) algorithm;[ 4] see Merge sort \u00a7 Variants for discussion.","title":"Merging two lists"},{"location":"Application/sorting/merge-sorts/Merge-algorithm/#k-way_merging","text":"Main article: K-way merge algorithm k -way merging generalizes binary merging to an arbitrary number k of sorted input lists. Applications of k -way merging arise in various sorting algorithms, including patience sorting [ 5] and an external sorting algorithm that divides its input into k = 1/ M \u2212 1 blocks that fit in memory, sorts these one by one, then merges these blocks.[ 2] :119\u2013120 Several solutions to this problem exist. A naive solution is to do a loop over the k lists to pick off the minimum element each time, and repeat this loop until all lists are empty: Input: a list of k lists. While any of the lists is non-empty: Loop over the lists to find the one with the minimum first element. Output the minimum element and remove it from its list. In the worst case , this algorithm performs ( k \u22121)( n \u2212 k /2) element comparisons to perform its work if there are a total of n elements in the lists.[ 6] It can be improved by storing the lists in a priority queue ( min-heap ) keyed by their first element: Build a min-heap h of the k lists, using the first element as the key. While any of the lists is non-empty: Let i = find-min( h ). Output the first element of list i and remove it from its list. Re-heapify h .","title":"K-way merging"},{"location":"Application/sorting/merge-sorts/Merge-sort/","text":"Merge sort Merge sort #","title":"Merge-sort"},{"location":"Application/sorting/merge-sorts/Merge-sort/#merge_sort","text":"","title":"Merge sort"},{"location":"Application/sorting/merge-sorts/k-way-merge-algorithm/","text":"","title":"K-way-merge-algorithm"},{"location":"Application/wikipedia-Parsing-algorithms/parsing-algorithm/","text":"Parsing algorithm doc and code # Bottom-up # parser doc code LL parser Recursive descent parser Top-down #","title":"Parsing algorithm doc and code"},{"location":"Application/wikipedia-Parsing-algorithms/parsing-algorithm/#parsing_algorithm_doc_and_code","text":"","title":"Parsing algorithm doc and code"},{"location":"Application/wikipedia-Parsing-algorithms/parsing-algorithm/#bottom-up","text":"parser doc code LL parser Recursive descent parser","title":"Bottom-up"},{"location":"Application/wikipedia-Parsing-algorithms/parsing-algorithm/#top-down","text":"","title":"Top-down"},{"location":"Application/wikipedia-Parsing-algorithms/wikipedia-Parsing/","text":"Parsing Types of parsers Parsing # Parsing , syntax analysis , or syntactic analysis is the process of analysing a string of symbols , either in natural language , computer languages or data structures , conforming to the rules of a formal grammar . The term parsing comes from Latin pars ( orationis ), meaning part (of speech) .[ 1] SUMMARY : syntax analysis \u66f4\u52a0\u80fd\u591f\u8bf4\u660e Parsing \u7684\u542b\u4e49\uff1b The term has slightly different meanings in different branches of linguistics and computer science . Traditional sentence parsing is often performed as a method of understanding the exact meaning of a sentence or word, sometimes with the aid of devices such as sentence diagrams . It usually emphasizes the importance of grammatical divisions such as subject and predicate . Within computational linguistics the term is used to refer to the formal analysis by a computer of a sentence or other string of words into its constituents, resulting in a parse tree showing their syntactic relation to each other, which may also contain semantic and other information.[ citation needed ] Some parsing algorithms may generate a parse forest or list of parse trees for a syntactically ambiguous input.[ 2] NOTE: Compiler principle can also be classified into computational linguistics . The term is also used in psycholinguistics when describing language comprehension. In this context, parsing refers to the way that human beings analyze a sentence or phrase (in spoken language or text) \"in terms of grammatical constituents, identifying the parts of speech, syntactic relations, etc.\"[ 1] This term is especially common when discussing what linguistic cues help speakers to interpret garden-path sentences . Within computer science, the term is used in the analysis of computer languages , referring to the syntactic analysis of the input code into its component parts in order to facilitate the writing of compilers and interpreters . The term may also be used to describe a split or separation. Types of parsers # The task of the parser is essentially to determine if and how the input can be derived from the start symbol of the grammar. This can be done in essentially two ways: Top-down parsing - Top-down parsing can be viewed as an attempt to find left-most derivations of an input-stream by searching for parse trees using a top-down expansion of the given formal grammar rules. Tokens are consumed from left to right. Inclusive choice is used to accommodate ambiguity by expanding all alternative right-hand-sides of grammar rules. Bottom-up parsing - A parser can start with the input and attempt to rewrite it to the start symbol. Intuitively, the parser attempts to locate the most basic elements, then the elements containing these, and so on. LR parsers are examples of bottom-up parsers. Another term used for this type of parser is Shift-Reduce parsing. LL parsers and recursive-descent parser are examples of top-down parsers which cannot accommodate left recursive production rules . Although it has been believed that simple implementations of top-down parsing cannot accommodate direct and indirect left-recursion and may require exponential time and space complexity while parsing ambiguous context-free grammars , more sophisticated algorithms for top-down parsing have been created by Frost, Hafiz, and Callaghan[ 11] [ 12] which accommodate ambiguity and left recursion in polynomial time and which generate polynomial-size representations of the potentially exponential number of parse trees. Their algorithm is able to produce both left-most and right-most derivations of an input with regard to a given context-free grammar . An important distinction with regard to parsers is whether a parser generates a leftmost derivation or a rightmost derivation (see context-free grammar ). LL parsers will generate a leftmost derivation and LR parsers will generate a rightmost derivation (although usually in reverse).[ 10] Some graphical parsing algorithms have been designed for visual programming languages .[ 13] [ 14] Parsers for visual languages are sometimes based on graph grammars .[ 15] Adaptive parsing algorithms have been used to construct \"self-extending\" natural language user interfaces .[ 16]","title":"wikipedia Parsing"},{"location":"Application/wikipedia-Parsing-algorithms/wikipedia-Parsing/#parsing","text":"Parsing , syntax analysis , or syntactic analysis is the process of analysing a string of symbols , either in natural language , computer languages or data structures , conforming to the rules of a formal grammar . The term parsing comes from Latin pars ( orationis ), meaning part (of speech) .[ 1] SUMMARY : syntax analysis \u66f4\u52a0\u80fd\u591f\u8bf4\u660e Parsing \u7684\u542b\u4e49\uff1b The term has slightly different meanings in different branches of linguistics and computer science . Traditional sentence parsing is often performed as a method of understanding the exact meaning of a sentence or word, sometimes with the aid of devices such as sentence diagrams . It usually emphasizes the importance of grammatical divisions such as subject and predicate . Within computational linguistics the term is used to refer to the formal analysis by a computer of a sentence or other string of words into its constituents, resulting in a parse tree showing their syntactic relation to each other, which may also contain semantic and other information.[ citation needed ] Some parsing algorithms may generate a parse forest or list of parse trees for a syntactically ambiguous input.[ 2] NOTE: Compiler principle can also be classified into computational linguistics . The term is also used in psycholinguistics when describing language comprehension. In this context, parsing refers to the way that human beings analyze a sentence or phrase (in spoken language or text) \"in terms of grammatical constituents, identifying the parts of speech, syntactic relations, etc.\"[ 1] This term is especially common when discussing what linguistic cues help speakers to interpret garden-path sentences . Within computer science, the term is used in the analysis of computer languages , referring to the syntactic analysis of the input code into its component parts in order to facilitate the writing of compilers and interpreters . The term may also be used to describe a split or separation.","title":"Parsing"},{"location":"Application/wikipedia-Parsing-algorithms/wikipedia-Parsing/#types_of_parsers","text":"The task of the parser is essentially to determine if and how the input can be derived from the start symbol of the grammar. This can be done in essentially two ways: Top-down parsing - Top-down parsing can be viewed as an attempt to find left-most derivations of an input-stream by searching for parse trees using a top-down expansion of the given formal grammar rules. Tokens are consumed from left to right. Inclusive choice is used to accommodate ambiguity by expanding all alternative right-hand-sides of grammar rules. Bottom-up parsing - A parser can start with the input and attempt to rewrite it to the start symbol. Intuitively, the parser attempts to locate the most basic elements, then the elements containing these, and so on. LR parsers are examples of bottom-up parsers. Another term used for this type of parser is Shift-Reduce parsing. LL parsers and recursive-descent parser are examples of top-down parsers which cannot accommodate left recursive production rules . Although it has been believed that simple implementations of top-down parsing cannot accommodate direct and indirect left-recursion and may require exponential time and space complexity while parsing ambiguous context-free grammars , more sophisticated algorithms for top-down parsing have been created by Frost, Hafiz, and Callaghan[ 11] [ 12] which accommodate ambiguity and left recursion in polynomial time and which generate polynomial-size representations of the potentially exponential number of parse trees. Their algorithm is able to produce both left-most and right-most derivations of an input with regard to a given context-free grammar . An important distinction with regard to parsers is whether a parser generates a leftmost derivation or a rightmost derivation (see context-free grammar ). LL parsers will generate a leftmost derivation and LR parsers will generate a rightmost derivation (although usually in reverse).[ 10] Some graphical parsing algorithms have been designed for visual programming languages .[ 13] [ 14] Parsers for visual languages are sometimes based on graph grammars .[ 15] Adaptive parsing algorithms have been used to construct \"self-extending\" natural language user interfaces .[ 16]","title":"Types of parsers"},{"location":"Application/wikipedia-Parsing-algorithms/Bottom-up/blog-Equation(expression)parser-with-precedence/","text":"Equation (expression) parser with precedence? # Simple Guide to Mathematical Expression Parsing #","title":"[Equation (expression) parser with precedence?](https://stackoverflow.com/questions/28256/equation-expression-parser-with-precedence)"},{"location":"Application/wikipedia-Parsing-algorithms/Bottom-up/blog-Equation(expression)parser-with-precedence/#equation_expression_parser_with_precedence","text":"","title":"Equation (expression) parser with precedence?"},{"location":"Application/wikipedia-Parsing-algorithms/Bottom-up/blog-Equation(expression)parser-with-precedence/#simple_guide_to_mathematical_expression_parsing","text":"","title":"Simple Guide to Mathematical Expression Parsing"},{"location":"Application/wikipedia-Parsing-algorithms/Bottom-up/wikipedia-Bottom-up-parsing/","text":"Bottom-up parsing Bottom-up Versus Top-down Examples Bottom-up parsing # In computer science , parsing reveals the grammatical structure of linear input text, as a first step in working out its meaning. Bottom-up parsing recognizes the text's lowest-level small details first, before its mid-level structures, and leaving the highest-level overall structure to last. Bottom-up Versus Top-down # The bottom-up name comes from the concept of a parse tree , in which the most detailed parts are at the bottom of the (upside-down) tree, and larger structures composed from them are in successively higher layers, until at the top or \"root\" of the tree a single unit describes the entire input stream. A bottom-up parse discovers and processes that tree starting from the bottom left end , and incrementally works its way upwards and rightwards.[ 2] A parser may act on the structure hierarchy's low, mid, and highest levels without ever creating an actual data tree; the tree is then merely implicit in the parser's actions. Bottom-up parsing patiently waits until it has scanned and parsed all parts of some construct before committing to what the combined construct is. The opposite of this is top-down parsing , in which the input's overall structure is decided (or guessed at) first, before dealing with mid-level parts, leaving completion of all lowest-level details to last. A top-down parser discovers and processes the hierarchical tree starting from the top, and incrementally works its way first downwards and then rightwards. Top-down parsing eagerly decides what a construct is much earlier, when it has only scanned the leftmost symbol of that construct and has not yet parsed any of its parts. Left corner parsing is a hybrid method which works bottom-up along the left edges of each subtree, and top-down on the rest of the parse tree. If a language grammar has multiple rules that may start with the same leftmost symbols but have different endings, then that grammar can be efficiently handled by a deterministic bottom-up parse but cannot be handled top-down without guesswork and backtracking . So bottom-up parsers handle a somewhat larger range of computer language grammars than do deterministic top-down parsers. Bottom-up parsing is sometimes done by backtracking . But much more commonly, bottom-up parsing is done by a shift-reduce parser such as a LALR parser . Examples # Some of the parsers that use bottom-up parsing include: Precedence parser Simple precedence parser Operator-precedence parser Bounded-context parser (BC) LR parser ( L eft-to-right, R ightmost derivation in reverse) Simple LR parser (SLR) LALR parser (Look-Ahead) Canonical LR parser (LR(1)) GLR parser (Generalized)[ 3] CYK parser (Cocke\u2013Younger\u2013Kasami) Recursive ascent parser Packrat parser Shift-reduce parser","title":"wikipedia Bottom up parsing"},{"location":"Application/wikipedia-Parsing-algorithms/Bottom-up/wikipedia-Bottom-up-parsing/#bottom-up_parsing","text":"In computer science , parsing reveals the grammatical structure of linear input text, as a first step in working out its meaning. Bottom-up parsing recognizes the text's lowest-level small details first, before its mid-level structures, and leaving the highest-level overall structure to last.","title":"Bottom-up parsing"},{"location":"Application/wikipedia-Parsing-algorithms/Bottom-up/wikipedia-Bottom-up-parsing/#bottom-up_versus_top-down","text":"The bottom-up name comes from the concept of a parse tree , in which the most detailed parts are at the bottom of the (upside-down) tree, and larger structures composed from them are in successively higher layers, until at the top or \"root\" of the tree a single unit describes the entire input stream. A bottom-up parse discovers and processes that tree starting from the bottom left end , and incrementally works its way upwards and rightwards.[ 2] A parser may act on the structure hierarchy's low, mid, and highest levels without ever creating an actual data tree; the tree is then merely implicit in the parser's actions. Bottom-up parsing patiently waits until it has scanned and parsed all parts of some construct before committing to what the combined construct is. The opposite of this is top-down parsing , in which the input's overall structure is decided (or guessed at) first, before dealing with mid-level parts, leaving completion of all lowest-level details to last. A top-down parser discovers and processes the hierarchical tree starting from the top, and incrementally works its way first downwards and then rightwards. Top-down parsing eagerly decides what a construct is much earlier, when it has only scanned the leftmost symbol of that construct and has not yet parsed any of its parts. Left corner parsing is a hybrid method which works bottom-up along the left edges of each subtree, and top-down on the rest of the parse tree. If a language grammar has multiple rules that may start with the same leftmost symbols but have different endings, then that grammar can be efficiently handled by a deterministic bottom-up parse but cannot be handled top-down without guesswork and backtracking . So bottom-up parsers handle a somewhat larger range of computer language grammars than do deterministic top-down parsers. Bottom-up parsing is sometimes done by backtracking . But much more commonly, bottom-up parsing is done by a shift-reduce parser such as a LALR parser .","title":"Bottom-up Versus Top-down"},{"location":"Application/wikipedia-Parsing-algorithms/Bottom-up/wikipedia-Bottom-up-parsing/#examples","text":"Some of the parsers that use bottom-up parsing include: Precedence parser Simple precedence parser Operator-precedence parser Bounded-context parser (BC) LR parser ( L eft-to-right, R ightmost derivation in reverse) Simple LR parser (SLR) LALR parser (Look-Ahead) Canonical LR parser (LR(1)) GLR parser (Generalized)[ 3] CYK parser (Cocke\u2013Younger\u2013Kasami) Recursive ascent parser Packrat parser Shift-reduce parser","title":"Examples"},{"location":"Application/wikipedia-Parsing-algorithms/Bottom-up/wikipedia-CYK-algorithm/","text":"CYK algorithm #","title":"[CYK algorithm](https://en.wikipedia.org/wiki/CYK_algorithm)"},{"location":"Application/wikipedia-Parsing-algorithms/Bottom-up/wikipedia-CYK-algorithm/#cyk_algorithm","text":"","title":"CYK algorithm"},{"location":"Application/wikipedia-Parsing-algorithms/Bottom-up/wikipedia-LR-parser/","text":"LR parser Overview Bottom-up parse tree for example A*2 + 1 Shift and reduce actions LR parser # In computer science , LR parsers are a type of bottom-up parser that analyses deterministic context-free languages in linear time . There are several variants of LR parsers: SLR parsers , LALR parsers , Canonical LR(1) parsers , Minimal LR(1) parsers , GLR parsers . LR parsers can be generated by a parser generator from a formal grammar defining the syntax of the language to be parsed. They are widely used for the processing of computer languages . An LR parser ( L eft-to-right, R ightmost derivation in reverse) reads input text from left to right without backing up (this is true for most parsers), and produces a rightmost derivation in reverse: it does a bottom-up parse - not a top-down LL parse or ad-hoc parse. The name LR is often followed by a numeric qualifier, as in LR(1) or sometimes LR( k ) . To avoid backtracking or guessing, the LR parser is allowed to peek ahead at k lookahead input symbols before deciding how to parse earlier symbols. Typically k is 1 and is not mentioned. The name LR is often preceded by other qualifiers, as in SLR and LALR . The LR( k ) condition for a grammar was suggested by Knuth to stand for \"translatable from left to right with bound k .\" LR parsers are deterministic; they produce a single correct parse without guesswork or backtracking, in linear time. This is ideal for computer languages, but LR parsers are not suited for human languages which need more flexible but inevitably slower methods. Some methods which can parse arbitrary context-free languages (e.g., Cocke\u2013Younger\u2013Kasami , Earley , GLR ) have worst-case performance of O( n 3) time. Other methods which backtrack or yield multiple parses may even take exponential time when they guess badly. The above properties of L , R , and k are actually shared by all shift-reduce parsers , including precedence parsers . But by convention, the LR name stands for the form of parsing invented by Donald Knuth , and excludes the earlier, less powerful precedence methods (for example Operator-precedence parser ).[ 1] LR parsers can handle a larger range of languages and grammars than precedence parsers or top-down LL parsing .[ 3] This is because the LR parser waits until it has seen an entire instance of some grammar pattern before committing to what it has found. An LL parser has to decide or guess what it is seeing much sooner, when it has only seen the leftmost input symbol of that pattern. Overview # Bottom-up parse tree for example A*2 + 1 # An LR parser scans and parses the input text in one forward pass over the text. The parser builds up the parse tree incrementally, bottom up, and left to right, without guessing or backtracking. At every point in this pass, the parser has accumulated a list of subtrees or phrases of the input text that have been already parsed. Those subtrees are not yet joined together because the parser has not yet reached the right end of the syntax pattern that will combine them. At step 6 in an example parse, only \"A 2\" has been parsed, incompletely. Only the shaded lower-left corner of the parse tree exists. None of the parse tree nodes numbered 7 and above exist yet. Nodes 3, 4, and 6 are the roots of isolated subtrees for variable A, operator , and number 2, respectively. These three root nodes are temporarily held in a parse stack . The remaining unparsed portion of the input stream is \"+ 1\". Bottom-up parse tree built in numbered steps Shift and reduce actions # As with other shift-reduce parsers, an LR parser works by doing some combination of Shift steps and Reduce steps. A Shift step advances in the input stream by one symbol. That shifted symbol becomes a new single-node parse tree. A Reduce step applies a completed grammar rule to some of the recent parse trees, joining them together as one tree with a new root symbol. If the input has no syntax errors, the parser continues with these steps until all of the input has been consumed and all of the parse trees have been reduced to a single tree representing an entire legal input. LR parsers differ from other shift-reduce parsers in how they decide when to reduce, and how to pick between rules with similar endings. But the final decisions and the sequence of shift or reduce steps are the same. Much of the LR parser's efficiency is from being deterministic. To avoid guessing, the LR parser often looks ahead (rightwards) at the next scanned symbol, before deciding what to do with previously scanned symbols. The lexical scanner works one or more symbols ahead of the parser. The lookahead symbols are the 'right-hand context' for the parsing decision.","title":"wikipedia LR parser"},{"location":"Application/wikipedia-Parsing-algorithms/Bottom-up/wikipedia-LR-parser/#lr_parser","text":"In computer science , LR parsers are a type of bottom-up parser that analyses deterministic context-free languages in linear time . There are several variants of LR parsers: SLR parsers , LALR parsers , Canonical LR(1) parsers , Minimal LR(1) parsers , GLR parsers . LR parsers can be generated by a parser generator from a formal grammar defining the syntax of the language to be parsed. They are widely used for the processing of computer languages . An LR parser ( L eft-to-right, R ightmost derivation in reverse) reads input text from left to right without backing up (this is true for most parsers), and produces a rightmost derivation in reverse: it does a bottom-up parse - not a top-down LL parse or ad-hoc parse. The name LR is often followed by a numeric qualifier, as in LR(1) or sometimes LR( k ) . To avoid backtracking or guessing, the LR parser is allowed to peek ahead at k lookahead input symbols before deciding how to parse earlier symbols. Typically k is 1 and is not mentioned. The name LR is often preceded by other qualifiers, as in SLR and LALR . The LR( k ) condition for a grammar was suggested by Knuth to stand for \"translatable from left to right with bound k .\" LR parsers are deterministic; they produce a single correct parse without guesswork or backtracking, in linear time. This is ideal for computer languages, but LR parsers are not suited for human languages which need more flexible but inevitably slower methods. Some methods which can parse arbitrary context-free languages (e.g., Cocke\u2013Younger\u2013Kasami , Earley , GLR ) have worst-case performance of O( n 3) time. Other methods which backtrack or yield multiple parses may even take exponential time when they guess badly. The above properties of L , R , and k are actually shared by all shift-reduce parsers , including precedence parsers . But by convention, the LR name stands for the form of parsing invented by Donald Knuth , and excludes the earlier, less powerful precedence methods (for example Operator-precedence parser ).[ 1] LR parsers can handle a larger range of languages and grammars than precedence parsers or top-down LL parsing .[ 3] This is because the LR parser waits until it has seen an entire instance of some grammar pattern before committing to what it has found. An LL parser has to decide or guess what it is seeing much sooner, when it has only seen the leftmost input symbol of that pattern.","title":"LR parser"},{"location":"Application/wikipedia-Parsing-algorithms/Bottom-up/wikipedia-LR-parser/#overview","text":"","title":"Overview"},{"location":"Application/wikipedia-Parsing-algorithms/Bottom-up/wikipedia-LR-parser/#bottom-up_parse_tree_for_example_a2_1","text":"An LR parser scans and parses the input text in one forward pass over the text. The parser builds up the parse tree incrementally, bottom up, and left to right, without guessing or backtracking. At every point in this pass, the parser has accumulated a list of subtrees or phrases of the input text that have been already parsed. Those subtrees are not yet joined together because the parser has not yet reached the right end of the syntax pattern that will combine them. At step 6 in an example parse, only \"A 2\" has been parsed, incompletely. Only the shaded lower-left corner of the parse tree exists. None of the parse tree nodes numbered 7 and above exist yet. Nodes 3, 4, and 6 are the roots of isolated subtrees for variable A, operator , and number 2, respectively. These three root nodes are temporarily held in a parse stack . The remaining unparsed portion of the input stream is \"+ 1\". Bottom-up parse tree built in numbered steps","title":"Bottom-up parse tree for example A*2 + 1"},{"location":"Application/wikipedia-Parsing-algorithms/Bottom-up/wikipedia-LR-parser/#shift_and_reduce_actions","text":"As with other shift-reduce parsers, an LR parser works by doing some combination of Shift steps and Reduce steps. A Shift step advances in the input stream by one symbol. That shifted symbol becomes a new single-node parse tree. A Reduce step applies a completed grammar rule to some of the recent parse trees, joining them together as one tree with a new root symbol. If the input has no syntax errors, the parser continues with these steps until all of the input has been consumed and all of the parse trees have been reduced to a single tree representing an entire legal input. LR parsers differ from other shift-reduce parsers in how they decide when to reduce, and how to pick between rules with similar endings. But the final decisions and the sequence of shift or reduce steps are the same. Much of the LR parser's efficiency is from being deterministic. To avoid guessing, the LR parser often looks ahead (rightwards) at the next scanned symbol, before deciding what to do with previously scanned symbols. The lexical scanner works one or more symbols ahead of the parser. The lookahead symbols are the 'right-hand context' for the parsing decision.","title":"Shift and reduce actions"},{"location":"Application/wikipedia-Parsing-algorithms/Bottom-up/Precedence/wikipedia-Operator-precedence-parser/","text":"","title":"wikipedia Operator precedence parser"},{"location":"Application/wikipedia-Parsing-algorithms/Bottom-up/Precedence/wikipedia-Shunting-yard-algorithm/","text":"","title":"wikipedia Shunting yard algorithm"},{"location":"Application/wikipedia-Parsing-algorithms/Top-down/LR-parsers-Versus-recursive-descent-parsers/","text":"Recursive descent parser use backtracking to match. LL parsers are table-based parsers, they use parsing table. Parsing table is constructed from the grammar and acts as a transformation function. Using parsing table and k tokens of lookahead , a LL parser can become predictive parser and avoid backtracking .","title":"LR parsers Versus recursive descent parsers"},{"location":"Application/wikipedia-Parsing-algorithms/Top-down/andrewbegel-LL(1)-Parsing/","text":"LL(1) Parsing left-recursion elimination LL(1) parsing FIRST FOLLOW LL(1) Parsing # The following grammar comes from Written Assignment 2. E -> E + T | T T -> T * F | F F -> (E) | int This is a grammar for arithmetic. There are several orders of precedence here: () 's beat * , and * beats + . We'd like to parse sentences using this grammar using a top-down, recursive descent algorithm. left-recursion elimination # This algorithm traverses the grammar looking for matches between terminals ( * , + , ( , ) , and int ) and the input sentence. We do this search depth-first, which presents a problem. If we start at the starting production E , and derive the production E + T , we have E still on the left. In recursive-descent parsing, we can only expand the left-most non-terminal at each step! We're going to infinitely loop if we try to parse sentences using this grammar. How do we fix it? We use a technique known as left-recursion elimination to get rid of the non-terminals on the left-hand side of each production that cause us to infinitely loop. (Note: not all grammars have left recursion. You can identify the ones that have immediate left recursion by looking at all of the productions -- if the non-terminal on the left side of the arrow is the same as the non-terminal in the left-most position of any phrase on the right side of the arrow, then this grammar is left-recursive. There are other forms of left recursion that can show up if you were to \"recurse\" down multiple rules in the grammar. If you eventually will cause any infinite loop, the grammar is left-recursive.) Let's take a look at the first one: E -> E + T | T What we do is this. For each production where the non-terminal on the left ( E ) of the arrow is the same as the left-side of a production on the right-hand side of the arrow ( E + T ), we take the part of the production without the E ( +T ) and move it down into its own new production (we'll call it E' ). E' -> + T We're not done yet. Now, after each of the new productions, add E' to the end. E' -> + T E' Nope, still not done yet. Now add an extra production to epsilon. E' -> + T E' | epsilon Good. Now we must fix up the original E productions. Here, we take all of the right-hand sides that didn't start with E , and add E' to the end of them. E -> T E' If we do this for the T productions as well, we get the following grammar: E -> T E' E' -> + T E' | epsilon T -> F T' T' -> * F T' | epsilon F -> (E) | int Note, the F production didn't get changed at all. That's because F didn't appear on the leftmost position of any of the productions on the right-hand side of the arrow. LL(1) parsing # Once we have performed left-recursion elimination on our grammar, we need to construct our parser. We're going to use an algorithm called LL(1) parsing. This is an algorithm that utilizes a lookup table to parse an expression. On top, we list all of the terminals , and on the left, we list the non-terminals (we include $ to signify end-of-file). + * ( ) int $ E E' T T' F Our LL(1) parser also contains a stack of non-terminals . Initially, we'll only put the starting state ( E ) on the stack. As we parse, we'll be putting non-terminals on and popping them off this stack. When they're all gone, and our stack is empty (and there is no more input), we're done. At each step, there is a grammar symbol at the top of the stack. When we see an input token from the lexer, we look in the table to see what to do. Each cell in the table is going to tell our LL(1) parser what to do when it sees the terminal on the top when the non-terminal on the left is at the top of the stack. Right now, our table is empty. Let's fill it up. We do this by computing two functions called FIRST and FOLLOW . FIRST # FIRST is a function on each non-terminal ( E , E' , T , T' , and F ) that tells us which terminals (tokens from the lexer) can appear as the first part of one of these non-terminals . Epsilon (neither a terminal nor a non-terminal) may also be included in this FIRST function. ( FIRST is also defined for terminals, but its value is just equal to the terminal itself, so we won't talk about it more.) What this means is that the parser is going to invoke some of these productions in the grammar. We need to know which one to pick when we see a particular token in the input stream. NOTE: To be predictive. So, let's start computing. What is the FIRST ( E )? What are the terminals that can appear at the beginning of the stream when we're looking for an E ? Well, E -> T E' , so whatever occurs at the beginning of E will be the same as what happens at the beginning of T . FIRST(E) => FIRST(T) How about FIRST ( E' )? This one is easy, we have the terminal + , and epsilon. FIRST(E') = { +, epsilon } And we'll continue with the others: FIRST(T) => FIRST(F) FIRST(T') = { *, epsilon } FIRST(F) = { (, int } See? FIRST ( F ) is just the set of terminals that are at the beginnings of its productions. So, to sum up: FIRST(E) = { (, int } FIRST(E') = { +, epsilon } FIRST(T) = { (, int } FIRST(T') = { *, epsilon } FIRST(F) = { (, int } FOLLOW # Now, let's do FOLLOW . Just as FIRST shows us the terminals that can be at the beginning of a derived non-terminal, FOLLOW shows us the terminals that can come after a derived non-terminal. Note, this does not mean the last terminal derived from a non-terminal. It's the set of terminals that can come after it. We define FOLLOW for all the non-terminals in the grammar. How do we figure out FOLLOW ? Instead of looking at the first terminal for each phrase on the right side of the arrow, we find every place our non-terminal is located on the right side of any of the arrows. Then we look for some terminals . As we go through our example, you'll see almost all of the different ways we figure out the FOLLOW of a non-terminal. First, however, let's pretend that our grammar starts with a unique starting production (it's not really part of the grammar): S -> E We start our journey at S , but rewrite it a bit to reflect the EOF that can be at the end. In parser-land, EOF is represented by $ . So our production is really: S -> E $ What is FOLLOW ( E )? (Note: We don't really care about FOLLOW ( S ) because it's just imaginary.) Look on all of the right-hand sides (after the arrow) of all of the productions in the grammar. What terminals appear on the right of the E ? Well, I see a $ and a ). FOLLOW(E) = { $, ) }","title":"andrewbegel LL(1) Parsing"},{"location":"Application/wikipedia-Parsing-algorithms/Top-down/andrewbegel-LL(1)-Parsing/#ll1_parsing","text":"The following grammar comes from Written Assignment 2. E -> E + T | T T -> T * F | F F -> (E) | int This is a grammar for arithmetic. There are several orders of precedence here: () 's beat * , and * beats + . We'd like to parse sentences using this grammar using a top-down, recursive descent algorithm.","title":"LL(1) Parsing"},{"location":"Application/wikipedia-Parsing-algorithms/Top-down/andrewbegel-LL(1)-Parsing/#left-recursion_elimination","text":"This algorithm traverses the grammar looking for matches between terminals ( * , + , ( , ) , and int ) and the input sentence. We do this search depth-first, which presents a problem. If we start at the starting production E , and derive the production E + T , we have E still on the left. In recursive-descent parsing, we can only expand the left-most non-terminal at each step! We're going to infinitely loop if we try to parse sentences using this grammar. How do we fix it? We use a technique known as left-recursion elimination to get rid of the non-terminals on the left-hand side of each production that cause us to infinitely loop. (Note: not all grammars have left recursion. You can identify the ones that have immediate left recursion by looking at all of the productions -- if the non-terminal on the left side of the arrow is the same as the non-terminal in the left-most position of any phrase on the right side of the arrow, then this grammar is left-recursive. There are other forms of left recursion that can show up if you were to \"recurse\" down multiple rules in the grammar. If you eventually will cause any infinite loop, the grammar is left-recursive.) Let's take a look at the first one: E -> E + T | T What we do is this. For each production where the non-terminal on the left ( E ) of the arrow is the same as the left-side of a production on the right-hand side of the arrow ( E + T ), we take the part of the production without the E ( +T ) and move it down into its own new production (we'll call it E' ). E' -> + T We're not done yet. Now, after each of the new productions, add E' to the end. E' -> + T E' Nope, still not done yet. Now add an extra production to epsilon. E' -> + T E' | epsilon Good. Now we must fix up the original E productions. Here, we take all of the right-hand sides that didn't start with E , and add E' to the end of them. E -> T E' If we do this for the T productions as well, we get the following grammar: E -> T E' E' -> + T E' | epsilon T -> F T' T' -> * F T' | epsilon F -> (E) | int Note, the F production didn't get changed at all. That's because F didn't appear on the leftmost position of any of the productions on the right-hand side of the arrow.","title":"left-recursion elimination"},{"location":"Application/wikipedia-Parsing-algorithms/Top-down/andrewbegel-LL(1)-Parsing/#ll1_parsing_1","text":"Once we have performed left-recursion elimination on our grammar, we need to construct our parser. We're going to use an algorithm called LL(1) parsing. This is an algorithm that utilizes a lookup table to parse an expression. On top, we list all of the terminals , and on the left, we list the non-terminals (we include $ to signify end-of-file). + * ( ) int $ E E' T T' F Our LL(1) parser also contains a stack of non-terminals . Initially, we'll only put the starting state ( E ) on the stack. As we parse, we'll be putting non-terminals on and popping them off this stack. When they're all gone, and our stack is empty (and there is no more input), we're done. At each step, there is a grammar symbol at the top of the stack. When we see an input token from the lexer, we look in the table to see what to do. Each cell in the table is going to tell our LL(1) parser what to do when it sees the terminal on the top when the non-terminal on the left is at the top of the stack. Right now, our table is empty. Let's fill it up. We do this by computing two functions called FIRST and FOLLOW .","title":"LL(1) parsing"},{"location":"Application/wikipedia-Parsing-algorithms/Top-down/andrewbegel-LL(1)-Parsing/#first","text":"FIRST is a function on each non-terminal ( E , E' , T , T' , and F ) that tells us which terminals (tokens from the lexer) can appear as the first part of one of these non-terminals . Epsilon (neither a terminal nor a non-terminal) may also be included in this FIRST function. ( FIRST is also defined for terminals, but its value is just equal to the terminal itself, so we won't talk about it more.) What this means is that the parser is going to invoke some of these productions in the grammar. We need to know which one to pick when we see a particular token in the input stream. NOTE: To be predictive. So, let's start computing. What is the FIRST ( E )? What are the terminals that can appear at the beginning of the stream when we're looking for an E ? Well, E -> T E' , so whatever occurs at the beginning of E will be the same as what happens at the beginning of T . FIRST(E) => FIRST(T) How about FIRST ( E' )? This one is easy, we have the terminal + , and epsilon. FIRST(E') = { +, epsilon } And we'll continue with the others: FIRST(T) => FIRST(F) FIRST(T') = { *, epsilon } FIRST(F) = { (, int } See? FIRST ( F ) is just the set of terminals that are at the beginnings of its productions. So, to sum up: FIRST(E) = { (, int } FIRST(E') = { +, epsilon } FIRST(T) = { (, int } FIRST(T') = { *, epsilon } FIRST(F) = { (, int }","title":"FIRST"},{"location":"Application/wikipedia-Parsing-algorithms/Top-down/andrewbegel-LL(1)-Parsing/#follow","text":"Now, let's do FOLLOW . Just as FIRST shows us the terminals that can be at the beginning of a derived non-terminal, FOLLOW shows us the terminals that can come after a derived non-terminal. Note, this does not mean the last terminal derived from a non-terminal. It's the set of terminals that can come after it. We define FOLLOW for all the non-terminals in the grammar. How do we figure out FOLLOW ? Instead of looking at the first terminal for each phrase on the right side of the arrow, we find every place our non-terminal is located on the right side of any of the arrows. Then we look for some terminals . As we go through our example, you'll see almost all of the different ways we figure out the FOLLOW of a non-terminal. First, however, let's pretend that our grammar starts with a unique starting production (it's not really part of the grammar): S -> E We start our journey at S , but rewrite it a bit to reflect the EOF that can be at the end. In parser-land, EOF is represented by $ . So our production is really: S -> E $ What is FOLLOW ( E )? (Note: We don't really care about FOLLOW ( S ) because it's just imaginary.) Look on all of the right-hand sides (after the arrow) of all of the productions in the grammar. What terminals appear on the right of the E ? Well, I see a $ and a ). FOLLOW(E) = { $, ) }","title":"FOLLOW"},{"location":"Application/wikipedia-Parsing-algorithms/Top-down/code/","text":"C compiler # https://github.com/rui314/8cc https://github.com/rui314/9cc https://github.com/rui314/chibicc https://www.sigbus.info/ https://www.sigbus.info/how-i-wrote-a-self-hosting-c-compiler-in-40-days https://github.com/zakirullin/tiny-compiler https://github.com/jamiebuilds/the-super-tiny-compiler recursive descend parser # https://github.com/codeplea/tinyexpr https://github.com/axilmar/parserlib LL parser # https://github.com/sid24rane/LL1-parser https://github.com/scottfrazer/hermes https://github.com/dxhj/predictive-parser LR parser # https://github.com/erezsh/plyplus https://github.com/lark-parser/lark https://github.com/igordejanovic/parglare https://github.com/bajdcc/clibparser https://github.com/igordejanovic/parglare automaton # https://github.com/WojciechMula/pyahocorasick state machine # https://github.com/pytransitions/transitions https://github.com/microsoft/BlingFire https://github.com/viewflow/django-fsm https://github.com/jtushman/state_machine https://github.com/boost-experimental/sml https://github.com/dabeaz/ply/tree/master/ply https://github.com/dabeaz/sly","title":"C compiler"},{"location":"Application/wikipedia-Parsing-algorithms/Top-down/code/#c_compiler","text":"https://github.com/rui314/8cc https://github.com/rui314/9cc https://github.com/rui314/chibicc https://www.sigbus.info/ https://www.sigbus.info/how-i-wrote-a-self-hosting-c-compiler-in-40-days https://github.com/zakirullin/tiny-compiler https://github.com/jamiebuilds/the-super-tiny-compiler","title":"C compiler"},{"location":"Application/wikipedia-Parsing-algorithms/Top-down/code/#recursive_descend_parser","text":"https://github.com/codeplea/tinyexpr https://github.com/axilmar/parserlib","title":"recursive descend parser"},{"location":"Application/wikipedia-Parsing-algorithms/Top-down/code/#ll_parser","text":"https://github.com/sid24rane/LL1-parser https://github.com/scottfrazer/hermes https://github.com/dxhj/predictive-parser","title":"LL parser"},{"location":"Application/wikipedia-Parsing-algorithms/Top-down/code/#lr_parser","text":"https://github.com/erezsh/plyplus https://github.com/lark-parser/lark https://github.com/igordejanovic/parglare https://github.com/bajdcc/clibparser https://github.com/igordejanovic/parglare","title":"LR parser"},{"location":"Application/wikipedia-Parsing-algorithms/Top-down/code/#automaton","text":"https://github.com/WojciechMula/pyahocorasick","title":"automaton"},{"location":"Application/wikipedia-Parsing-algorithms/Top-down/code/#state_machine","text":"https://github.com/pytransitions/transitions https://github.com/microsoft/BlingFire https://github.com/viewflow/django-fsm https://github.com/jtushman/state_machine https://github.com/boost-experimental/sml https://github.com/dabeaz/ply/tree/master/ply https://github.com/dabeaz/sly","title":"state machine"},{"location":"Application/wikipedia-Parsing-algorithms/Top-down/wikipedia-LL-parser/","text":"LL parser Overview Parser Concrete example Set up Parsing procedure implementation Remarks Constructing an LL(1) parsing table LL parser # In computer science , an LL parser ( L eft-to-right, L eftmost derivation) is a top-down parser for a subset of context-free languages . It parses the input from L eft to right, performing L eftmost derivation of the sentence. NOTE: Follow the L eftmost derivation , you will find an detailed explanation of leftmost derivation. An LL parser is called an LL( k ) parser if it uses k tokens of lookahead when parsing a sentence. A grammar is called an LL( k ) grammar if an LL( k ) parser can be constructed from it. A formal language is called an LL( k ) language if it has an LL( k ) grammar. The set of LL( k ) languages is properly contained in that of LL( k +1) languages, for each k \u2265 0. A corollary of this is that not all context-free languages can be recognized by an LL( k ) parser. An LL parser is called an LL( * ), or LL-regular, parser if it is not restricted to a finite number k of tokens of lookahead, but can make parsing decisions by recognizing whether the following tokens belong to a regular language (for example by means of a deterministic finite automaton ). LL grammars, particularly LL(1) grammars, are of great practical interest, as parsers for these grammars are easy to construct, and many computer languages are designed to be LL(1) for this reason.[ 3] LL parsers are table-based parsers, similar to LR parsers . LL grammars can also be parsed by recursive descent parsers . According to Waite and Goos (1984), LL( k ) grammars were introduced by Stearns and Lewis (1969). Overview # For a given context-free grammar , the parser attempts to find the leftmost derivation . Given an example grammar $ G $: $ S\\to E $ $ E\\to (E+E) $ $ E\\to i $ the leftmost derivation for $ w=((i+i)+i) $ is: $ S\\ {\\overset {(1)}{\\Rightarrow }}\\ E\\ {\\overset {(2)}{\\Rightarrow }}\\ (E+E)\\ {\\overset {(2)}{\\Rightarrow }}\\ ((E+E)+E)\\ {\\overset {(3)}{\\Rightarrow }}\\ ((i+E)+E)\\ {\\overset {(3)}{\\Rightarrow }}\\ ((i+i)+E)\\ {\\overset {(3)}{\\Rightarrow }}\\ ((i+i)+i) $ Generally, there are multiple possibilities when selecting a rule to expand the leftmost non-terminal . In step 2 of the previous example, the parser must choose whether to apply rule 2 or rule 3: $ S\\ {\\overset {(1)}{\\Rightarrow }}\\ E\\ {\\overset {(?)}{\\Rightarrow }}\\ ? $ To be efficient, the parser must be able to make this choice deterministically when possible, without backtracking . For some grammars, it can do this by peeking on the unread input (without reading). In our example, if the parser knows that the next unread symbol is $ ( $ , the only correct rule that can be used is 2. NOTE: Peeking makes the parser predictive to choose an deterministic production and avoid choosing by backtracking . As you will see later, it is not enough to achieve predictability just by peeking , parsing table is also needed to make the parser predictive to avoid backtracking . Generally, an $ LL(k) $ parser can look ahead at $ k $ symbols. However, given a grammar, the problem of determining if there exists a $ LL(k) $ parser for some $ k $ that recognizes it is undecidable. For each $ k $, there is a language that cannot be recognized by an $ LL(k) $ parser, but can be by an $ LL(k+1) $. We can use the above analysis to give the following formal definition: Let $ G $ be a context-free grammar and $ k\\geq 1 $. We say that $ G $ is $ LL(k) $, if and only if for any two leftmost derivations: $ S\\ \\Rightarrow \\ \\dots \\ \\Rightarrow \\ wA\\alpha \\ \\Rightarrow \\ \\dots \\ \\Rightarrow \\ w\\beta \\alpha \\ \\Rightarrow \\ \\dots \\ \\Rightarrow \\ wu $ $ S\\ \\Rightarrow \\ \\dots \\ \\Rightarrow \\ wA\\alpha \\ \\Rightarrow \\ \\dots \\ \\Rightarrow \\ w\\gamma \\alpha \\ \\Rightarrow \\ \\dots \\ \\Rightarrow \\ wv $ the following condition holds: the prefix of the string $ u $ of length $ k $ equals the prefix of the string $ v $ of length $ k $ implies $ \\beta \\ =\\ \\gamma $. Do not understand. Parser # The $ LL(k) $ parser is a deterministic pushdown automaton with the ability to peek on the next $ k $ input symbols without reading. This peek capability can be emulated by storing the lookahead buffer contents in the finite state space , since both buffer and input alphabet are finite in size. As a result, this does not make the automaton more powerful, but is a convenient abstraction. The stack alphabet is $ \\Gamma =N\\cup \\Sigma $, where: $ N $ is the set of non-terminals; $ \\Sigma $ the set of terminal (input) symbols with a special end-of-input (EOI) symbol $ \\$ $. The parser stack initially contains the starting symbol above the EOI: $ [\\ S\\ \\$\\ ] $. During operation, the parser repeatedly replaces the symbol $ X $ on top of the stack: with some $ \\alpha $, if $ X\\in N $ and there is a rule $ X\\to \\alpha $ ($X$ is a non-terminal); with $ \\epsilon $ (in some notations $ \\lambda $), i.e. $ X $ is popped off the stack, if $ X\\in \\Sigma $. In this case, an input symbol $ x $ is read and if $ x\\neq X $, the parser rejects the input ($X$ is a terminal, which means $X$ should equal to $x$). NOTE: At first step, $X$ is $S$. If the last symbol to be removed from the stack is the EOI, the parsing is successful; the automaton accepts via an empty stack. The states and the transition function are not explicitly given; they are specified (generated) using a more convenient parse table instead. The table provides the following mapping: row: top-of-stack symbol $ X $ column: $ |w|\\leq k $ lookahead buffer contents cell: rule number for $ X\\to \\alpha $ or $ \\epsilon $ If the parser cannot perform a valid transition, the input is rejected (empty cells). To make the table more compact, only the non-terminal rows are commonly displayed, since the action is the same for terminals. NOTE: The LL parser consists of three parts parsing table parser stack input stream The following image is from a Wikipedia entry pushdown automaton The $ LL(k) $ parser is a deterministic pushdown automaton . Concrete example # Set up # To explain an LL(1) parser's workings we will consider the following small LL(1) grammar: S \u2192 F S \u2192 ( S + F ) F \u2192 a and parse the following input: ( a + a ) An LL(1) parsing table for a grammar has a row for each of the non-terminals and a column for each terminal (including the special terminal, represented here as $ , that is used to indicate the end of the input stream). Each cell of the table may point to at most one rule of the grammar (identified by its number). For example, in the parsing table for the above grammar, the cell for the non-terminal 'S' and terminal '(' points to the rule number 2: ( ) a + $ S 2 - 1 - - F - - 3 - - The algorithm to construct a parsing table is described in a later section, but first let's see how the parser uses the parsing table to process its input. Parsing procedure # In each step, the parser reads the next-available symbol from the input stream, and the top-most symbol from the stack. If the input symbol and the stack-top symbol match, the parser discards them both, leaving only the unmatched symbols in the input stream and on the stack. Thus, in its first step, the parser reads the input symbol ( and the stack-top symbol S . The parsing table instruction comes from the column headed by the input symbol ( and the row headed by the stack-top symbol S ; this cell contains 2 , which instructs the parser to apply rule (2). The parser has to rewrite S to ( S + F ) on the stack by removing S from stack and pushing ) , F , + , S , ( onto the stack, and this writes the rule number 2 to the output. The stack then becomes: [ (, S, +, F, ), $ ] In the second step, the parser removes the ( from its input stream and from its stack , since they now match. The stack now becomes: [ S, +, F, ), $ ] Now the parser has an a on its input stream and an S as its stack top. The parsing table instructs it to apply rule (1) from the grammar and write the rule number 1 to the output stream. The stack becomes: [ F, +, F, ), $ ] The parser now has an ' a' on its input stream and an 'F' as its stack top. The parsing table instructs it to apply rule (3) from the grammar and write the rule number 3 to the output stream. The stack becomes: [ a, +, F, ), $ ] The parser now has an ' a' on the input stream and an 'a' at its stack top. Because they are the same, it removes it from the input stream and pops it from the top of the stack. The parser then has an ' +' on the input stream and '+' is at the top of the stack meaning, like with 'a', it is popped from the stack and removed from the input stream. This results in: [ F, ), $ ] In the next three steps the parser will replace ' F' on the stack by ' a' , write the rule number 3 to the output stream and remove the ' a' and ' )' from both the stack and the input stream. The parser thus ends with ' $' on both its stack and its input stream. In this case the parser will report that it has accepted the input string and write the following list of rule numbers to the output stream: [ 2, 1, 3, 3 ] This is indeed a list of rules for a leftmost derivation of the input string, which is: S \u2192 ( S + F ) \u2192 ( F + F ) \u2192 ( a + F ) \u2192 ( a + a ) implementation # cpp python Remarks # As can be seen from the example, the parser performs three types of steps depending on whether the top of the stack is a nonterminal, a terminal or the special symbol $ : If the top is a nonterminal then the parser looks up in the parsing table, on the basis of this nonterminal and the symbol on the input stream, which rule of the grammar it should use to replace nonterminal on the stack. The number of the rule is written to the output stream. If the parsing table indicates that there is no such rule then the parser reports an error and stops. If the top is a terminal then the parser compares it to the symbol on the input stream and if they are equal they are both removed. If they are not equal the parser reports an error and stops. If the top is $ and on the input stream there is also a $ then the parser reports that it has successfully parsed the input, otherwise it reports an error. In both cases the parser will stop. These steps are repeated until the parser stops, and then it will have either completely parsed the input and written a leftmost derivation to the output stream or it will have reported an error. Constructing an LL(1) parsing table # In order to fill the parsing table , we have to establish what grammar rule the parser should choose if it sees a nonterminal A on the top of its stack and a symbol a on its input stream. It is easy to see that such a rule should be of the form A \u2192 w and that the language corresponding to w should have at least one string starting with a . For this purpose we define the First-set of w , written here as Fi (w) , as the set of terminals that can be found at the start of some string in w , plus $\\epsilon$ if the empty string also belongs to w . Given a grammar with the rules $A_1 \\to w_1, \\dots, A_n \\to w_n$, we can compute the Fi (wi) and Fi (Ai) for every rule as follows: initialize every Fi(Ai) with the empty set add Fi(wi) to Fi (wi) for every rule $A_i \\to w_i$, where Fi is defined as follows: Fi(aw) = { a } for every terminal a Unfortunately, the First-sets are not sufficient to compute the parsing table . This is because a right-hand side w of a rule might ultimately be rewritten to the empty string. So the parser should also use the rule A \u2192 w if \u03b5 is in Fi ( w ) and it sees on the input stream a symbol that could follow A . Therefore, we also need the Follow-set of A , written as Fo ( A ) here, which is defined as the set of terminals a such that there is a string of symbols \u03b1Aa\u03b2 that can be derived from the start symbol. We use $ as a special terminal indicating end of input stream, and S as start symbol. In an LL(1) parser, the parser works by maintaining a workspace initially seeded to the start symbol followed by the end-of-string marker (usually denoted $ ). At each step, it does one of the following: If the first symbol of the workspace is a terminal, it matches it against the next token of input (or reports an error if it doesn't match.) If the first symbol of the workspace is a nonterminal, it predicts what production to replace that nonterminal with. The predict step is where FIRST and FOLLOW show up. The parser needs to be able to guess, based purely on the current nonterminal and the next token of input, which production to use. The question is how to do this. Let's suppose that the current nonterminal is A and the next token of input is t . If you know the productions of A , which one would you choose to apply? There's one simple case to consider: if there's a production of the form A \u2192 t\u03c9 , where \u03c9 is some arbitrary string, then you should pick that production because the t you're looking at as input will match the t at the front of the production. There are also some complex cases to consider. Suppose you have a production of the form A \u2192 B\u03c9 , where B is a nonterminal and \u03c9 is some string. Under what circumstances would you want to guess this production? Well, if you know that the next terminal symbol is a t , you wouldn't want to guess this production unless you knew that B can expand to a string that starts with the terminal t (there's another case that we'll talk about in a second). This is where FIRST sets come in. In grammars without \u03b5 productions, the set FIRST(X) for some nonterminal X is the set of all terminals that can potentially appear at the start of some string derived from X . If you have a production of the form A \u2192 B\u03c9 and you see the nonterminal t , you'd guess to use that production precisely when t \u2208 FIRST(B) ; that is, B can derive some string that starts with t . If B doesn't derive anything starting with t , then there's no reason to choose it, and if B does derive something starting with t , you'd want to make this choice so that you could eventually match the t against it. Things get a bit trickier when \u03b5 productions are introduced. Now, let's suppose that you have a production of the form A \u2192 BC\u03c9 , where B and C are nonterminals and \u03c9 is a string. Let's also suppose the next token of input is t . If t \u2208 FIRST(B) , then we'd choose this production, as mentioned above. However, what happens if t \u2209 FIRST(B) ? If there are \u03b5 productions in the grammar, we might still want to choose this production if B can derive \u03b5 and t \u2208 FIRST(C) . Why is this? If this happens, it means that we might be able to match the t by producing BC\u03c9 , then producing \u03b5 from B , leaving C\u03c9 against which to match the t . This is one context where we might have to \"look through\" a nonterminal. Fortunately, this is handled by FIRST sets. If a nonterminal X can produce \u03b5 , then \u03b5 \u2208 FIRST(X) . Therefore, we can use FIRST sets to check whether we need to \"look through\" a nonterminal by seeing whether \u03b5 \u2208 FIRST(X) . So far we haven't talked about FOLLOW sets. Where do they come in? Well, suppose that we're processing the nonterminal A , we see the terminal t , but none of the productions for A can actually consume the t . What do we do then? It turns out there's still a way that we can eat up that t . Remember that LL(1) parsers work by maintaining a workspace with a string in it. It's possible that the t we're looking at is not supposed to be matched against the current nonterminal A at all, and instead we're supposed to have A produce \u03b5 and then let some later nonterminal in the workspace match against the t . This is where FOLLOW sets come in. The FOLLOW set of a nonterminal X , denoted FOLLOW(X), is the set of all terminal symbols that can appear immediately after X in some derivation. When choosing which production to choose for A, we add in a final rule - if the terminal symbol t is in the FOLLOW set of A, we choose some production that ultimately will produce \u03b5 . That way, the A will \"disappear\" and we can match the t against some character that appears after the A nonterminal. This isn't a complete introduction to LL(1) parsing, but I hope it helps you see why we need FIRST and FOLLOW sets. For more information, pick up a book on parsing (I recommend Parsing Techniques: A Practical Guide by Grune and Jacobs) or take a course on compilers. As a totally shameless plug, I taught a compilers course in Summer 2012-2013 and all of the lecture slides are available online . Hope this helps!","title":"wikipedia LL parser"},{"location":"Application/wikipedia-Parsing-algorithms/Top-down/wikipedia-LL-parser/#ll_parser","text":"In computer science , an LL parser ( L eft-to-right, L eftmost derivation) is a top-down parser for a subset of context-free languages . It parses the input from L eft to right, performing L eftmost derivation of the sentence. NOTE: Follow the L eftmost derivation , you will find an detailed explanation of leftmost derivation. An LL parser is called an LL( k ) parser if it uses k tokens of lookahead when parsing a sentence. A grammar is called an LL( k ) grammar if an LL( k ) parser can be constructed from it. A formal language is called an LL( k ) language if it has an LL( k ) grammar. The set of LL( k ) languages is properly contained in that of LL( k +1) languages, for each k \u2265 0. A corollary of this is that not all context-free languages can be recognized by an LL( k ) parser. An LL parser is called an LL( * ), or LL-regular, parser if it is not restricted to a finite number k of tokens of lookahead, but can make parsing decisions by recognizing whether the following tokens belong to a regular language (for example by means of a deterministic finite automaton ). LL grammars, particularly LL(1) grammars, are of great practical interest, as parsers for these grammars are easy to construct, and many computer languages are designed to be LL(1) for this reason.[ 3] LL parsers are table-based parsers, similar to LR parsers . LL grammars can also be parsed by recursive descent parsers . According to Waite and Goos (1984), LL( k ) grammars were introduced by Stearns and Lewis (1969).","title":"LL parser"},{"location":"Application/wikipedia-Parsing-algorithms/Top-down/wikipedia-LL-parser/#overview","text":"For a given context-free grammar , the parser attempts to find the leftmost derivation . Given an example grammar $ G $: $ S\\to E $ $ E\\to (E+E) $ $ E\\to i $ the leftmost derivation for $ w=((i+i)+i) $ is: $ S\\ {\\overset {(1)}{\\Rightarrow }}\\ E\\ {\\overset {(2)}{\\Rightarrow }}\\ (E+E)\\ {\\overset {(2)}{\\Rightarrow }}\\ ((E+E)+E)\\ {\\overset {(3)}{\\Rightarrow }}\\ ((i+E)+E)\\ {\\overset {(3)}{\\Rightarrow }}\\ ((i+i)+E)\\ {\\overset {(3)}{\\Rightarrow }}\\ ((i+i)+i) $ Generally, there are multiple possibilities when selecting a rule to expand the leftmost non-terminal . In step 2 of the previous example, the parser must choose whether to apply rule 2 or rule 3: $ S\\ {\\overset {(1)}{\\Rightarrow }}\\ E\\ {\\overset {(?)}{\\Rightarrow }}\\ ? $ To be efficient, the parser must be able to make this choice deterministically when possible, without backtracking . For some grammars, it can do this by peeking on the unread input (without reading). In our example, if the parser knows that the next unread symbol is $ ( $ , the only correct rule that can be used is 2. NOTE: Peeking makes the parser predictive to choose an deterministic production and avoid choosing by backtracking . As you will see later, it is not enough to achieve predictability just by peeking , parsing table is also needed to make the parser predictive to avoid backtracking . Generally, an $ LL(k) $ parser can look ahead at $ k $ symbols. However, given a grammar, the problem of determining if there exists a $ LL(k) $ parser for some $ k $ that recognizes it is undecidable. For each $ k $, there is a language that cannot be recognized by an $ LL(k) $ parser, but can be by an $ LL(k+1) $. We can use the above analysis to give the following formal definition: Let $ G $ be a context-free grammar and $ k\\geq 1 $. We say that $ G $ is $ LL(k) $, if and only if for any two leftmost derivations: $ S\\ \\Rightarrow \\ \\dots \\ \\Rightarrow \\ wA\\alpha \\ \\Rightarrow \\ \\dots \\ \\Rightarrow \\ w\\beta \\alpha \\ \\Rightarrow \\ \\dots \\ \\Rightarrow \\ wu $ $ S\\ \\Rightarrow \\ \\dots \\ \\Rightarrow \\ wA\\alpha \\ \\Rightarrow \\ \\dots \\ \\Rightarrow \\ w\\gamma \\alpha \\ \\Rightarrow \\ \\dots \\ \\Rightarrow \\ wv $ the following condition holds: the prefix of the string $ u $ of length $ k $ equals the prefix of the string $ v $ of length $ k $ implies $ \\beta \\ =\\ \\gamma $. Do not understand.","title":"Overview"},{"location":"Application/wikipedia-Parsing-algorithms/Top-down/wikipedia-LL-parser/#parser","text":"The $ LL(k) $ parser is a deterministic pushdown automaton with the ability to peek on the next $ k $ input symbols without reading. This peek capability can be emulated by storing the lookahead buffer contents in the finite state space , since both buffer and input alphabet are finite in size. As a result, this does not make the automaton more powerful, but is a convenient abstraction. The stack alphabet is $ \\Gamma =N\\cup \\Sigma $, where: $ N $ is the set of non-terminals; $ \\Sigma $ the set of terminal (input) symbols with a special end-of-input (EOI) symbol $ \\$ $. The parser stack initially contains the starting symbol above the EOI: $ [\\ S\\ \\$\\ ] $. During operation, the parser repeatedly replaces the symbol $ X $ on top of the stack: with some $ \\alpha $, if $ X\\in N $ and there is a rule $ X\\to \\alpha $ ($X$ is a non-terminal); with $ \\epsilon $ (in some notations $ \\lambda $), i.e. $ X $ is popped off the stack, if $ X\\in \\Sigma $. In this case, an input symbol $ x $ is read and if $ x\\neq X $, the parser rejects the input ($X$ is a terminal, which means $X$ should equal to $x$). NOTE: At first step, $X$ is $S$. If the last symbol to be removed from the stack is the EOI, the parsing is successful; the automaton accepts via an empty stack. The states and the transition function are not explicitly given; they are specified (generated) using a more convenient parse table instead. The table provides the following mapping: row: top-of-stack symbol $ X $ column: $ |w|\\leq k $ lookahead buffer contents cell: rule number for $ X\\to \\alpha $ or $ \\epsilon $ If the parser cannot perform a valid transition, the input is rejected (empty cells). To make the table more compact, only the non-terminal rows are commonly displayed, since the action is the same for terminals. NOTE: The LL parser consists of three parts parsing table parser stack input stream The following image is from a Wikipedia entry pushdown automaton The $ LL(k) $ parser is a deterministic pushdown automaton .","title":"Parser"},{"location":"Application/wikipedia-Parsing-algorithms/Top-down/wikipedia-LL-parser/#concrete_example","text":"","title":"Concrete example"},{"location":"Application/wikipedia-Parsing-algorithms/Top-down/wikipedia-LL-parser/#set_up","text":"To explain an LL(1) parser's workings we will consider the following small LL(1) grammar: S \u2192 F S \u2192 ( S + F ) F \u2192 a and parse the following input: ( a + a ) An LL(1) parsing table for a grammar has a row for each of the non-terminals and a column for each terminal (including the special terminal, represented here as $ , that is used to indicate the end of the input stream). Each cell of the table may point to at most one rule of the grammar (identified by its number). For example, in the parsing table for the above grammar, the cell for the non-terminal 'S' and terminal '(' points to the rule number 2: ( ) a + $ S 2 - 1 - - F - - 3 - - The algorithm to construct a parsing table is described in a later section, but first let's see how the parser uses the parsing table to process its input.","title":"Set up"},{"location":"Application/wikipedia-Parsing-algorithms/Top-down/wikipedia-LL-parser/#parsing_procedure","text":"In each step, the parser reads the next-available symbol from the input stream, and the top-most symbol from the stack. If the input symbol and the stack-top symbol match, the parser discards them both, leaving only the unmatched symbols in the input stream and on the stack. Thus, in its first step, the parser reads the input symbol ( and the stack-top symbol S . The parsing table instruction comes from the column headed by the input symbol ( and the row headed by the stack-top symbol S ; this cell contains 2 , which instructs the parser to apply rule (2). The parser has to rewrite S to ( S + F ) on the stack by removing S from stack and pushing ) , F , + , S , ( onto the stack, and this writes the rule number 2 to the output. The stack then becomes: [ (, S, +, F, ), $ ] In the second step, the parser removes the ( from its input stream and from its stack , since they now match. The stack now becomes: [ S, +, F, ), $ ] Now the parser has an a on its input stream and an S as its stack top. The parsing table instructs it to apply rule (1) from the grammar and write the rule number 1 to the output stream. The stack becomes: [ F, +, F, ), $ ] The parser now has an ' a' on its input stream and an 'F' as its stack top. The parsing table instructs it to apply rule (3) from the grammar and write the rule number 3 to the output stream. The stack becomes: [ a, +, F, ), $ ] The parser now has an ' a' on the input stream and an 'a' at its stack top. Because they are the same, it removes it from the input stream and pops it from the top of the stack. The parser then has an ' +' on the input stream and '+' is at the top of the stack meaning, like with 'a', it is popped from the stack and removed from the input stream. This results in: [ F, ), $ ] In the next three steps the parser will replace ' F' on the stack by ' a' , write the rule number 3 to the output stream and remove the ' a' and ' )' from both the stack and the input stream. The parser thus ends with ' $' on both its stack and its input stream. In this case the parser will report that it has accepted the input string and write the following list of rule numbers to the output stream: [ 2, 1, 3, 3 ] This is indeed a list of rules for a leftmost derivation of the input string, which is: S \u2192 ( S + F ) \u2192 ( F + F ) \u2192 ( a + F ) \u2192 ( a + a )","title":"Parsing procedure"},{"location":"Application/wikipedia-Parsing-algorithms/Top-down/wikipedia-LL-parser/#implementation","text":"cpp python","title":"implementation"},{"location":"Application/wikipedia-Parsing-algorithms/Top-down/wikipedia-LL-parser/#remarks","text":"As can be seen from the example, the parser performs three types of steps depending on whether the top of the stack is a nonterminal, a terminal or the special symbol $ : If the top is a nonterminal then the parser looks up in the parsing table, on the basis of this nonterminal and the symbol on the input stream, which rule of the grammar it should use to replace nonterminal on the stack. The number of the rule is written to the output stream. If the parsing table indicates that there is no such rule then the parser reports an error and stops. If the top is a terminal then the parser compares it to the symbol on the input stream and if they are equal they are both removed. If they are not equal the parser reports an error and stops. If the top is $ and on the input stream there is also a $ then the parser reports that it has successfully parsed the input, otherwise it reports an error. In both cases the parser will stop. These steps are repeated until the parser stops, and then it will have either completely parsed the input and written a leftmost derivation to the output stream or it will have reported an error.","title":"Remarks"},{"location":"Application/wikipedia-Parsing-algorithms/Top-down/wikipedia-LL-parser/#constructing_an_ll1_parsing_table","text":"In order to fill the parsing table , we have to establish what grammar rule the parser should choose if it sees a nonterminal A on the top of its stack and a symbol a on its input stream. It is easy to see that such a rule should be of the form A \u2192 w and that the language corresponding to w should have at least one string starting with a . For this purpose we define the First-set of w , written here as Fi (w) , as the set of terminals that can be found at the start of some string in w , plus $\\epsilon$ if the empty string also belongs to w . Given a grammar with the rules $A_1 \\to w_1, \\dots, A_n \\to w_n$, we can compute the Fi (wi) and Fi (Ai) for every rule as follows: initialize every Fi(Ai) with the empty set add Fi(wi) to Fi (wi) for every rule $A_i \\to w_i$, where Fi is defined as follows: Fi(aw) = { a } for every terminal a Unfortunately, the First-sets are not sufficient to compute the parsing table . This is because a right-hand side w of a rule might ultimately be rewritten to the empty string. So the parser should also use the rule A \u2192 w if \u03b5 is in Fi ( w ) and it sees on the input stream a symbol that could follow A . Therefore, we also need the Follow-set of A , written as Fo ( A ) here, which is defined as the set of terminals a such that there is a string of symbols \u03b1Aa\u03b2 that can be derived from the start symbol. We use $ as a special terminal indicating end of input stream, and S as start symbol. In an LL(1) parser, the parser works by maintaining a workspace initially seeded to the start symbol followed by the end-of-string marker (usually denoted $ ). At each step, it does one of the following: If the first symbol of the workspace is a terminal, it matches it against the next token of input (or reports an error if it doesn't match.) If the first symbol of the workspace is a nonterminal, it predicts what production to replace that nonterminal with. The predict step is where FIRST and FOLLOW show up. The parser needs to be able to guess, based purely on the current nonterminal and the next token of input, which production to use. The question is how to do this. Let's suppose that the current nonterminal is A and the next token of input is t . If you know the productions of A , which one would you choose to apply? There's one simple case to consider: if there's a production of the form A \u2192 t\u03c9 , where \u03c9 is some arbitrary string, then you should pick that production because the t you're looking at as input will match the t at the front of the production. There are also some complex cases to consider. Suppose you have a production of the form A \u2192 B\u03c9 , where B is a nonterminal and \u03c9 is some string. Under what circumstances would you want to guess this production? Well, if you know that the next terminal symbol is a t , you wouldn't want to guess this production unless you knew that B can expand to a string that starts with the terminal t (there's another case that we'll talk about in a second). This is where FIRST sets come in. In grammars without \u03b5 productions, the set FIRST(X) for some nonterminal X is the set of all terminals that can potentially appear at the start of some string derived from X . If you have a production of the form A \u2192 B\u03c9 and you see the nonterminal t , you'd guess to use that production precisely when t \u2208 FIRST(B) ; that is, B can derive some string that starts with t . If B doesn't derive anything starting with t , then there's no reason to choose it, and if B does derive something starting with t , you'd want to make this choice so that you could eventually match the t against it. Things get a bit trickier when \u03b5 productions are introduced. Now, let's suppose that you have a production of the form A \u2192 BC\u03c9 , where B and C are nonterminals and \u03c9 is a string. Let's also suppose the next token of input is t . If t \u2208 FIRST(B) , then we'd choose this production, as mentioned above. However, what happens if t \u2209 FIRST(B) ? If there are \u03b5 productions in the grammar, we might still want to choose this production if B can derive \u03b5 and t \u2208 FIRST(C) . Why is this? If this happens, it means that we might be able to match the t by producing BC\u03c9 , then producing \u03b5 from B , leaving C\u03c9 against which to match the t . This is one context where we might have to \"look through\" a nonterminal. Fortunately, this is handled by FIRST sets. If a nonterminal X can produce \u03b5 , then \u03b5 \u2208 FIRST(X) . Therefore, we can use FIRST sets to check whether we need to \"look through\" a nonterminal by seeing whether \u03b5 \u2208 FIRST(X) . So far we haven't talked about FOLLOW sets. Where do they come in? Well, suppose that we're processing the nonterminal A , we see the terminal t , but none of the productions for A can actually consume the t . What do we do then? It turns out there's still a way that we can eat up that t . Remember that LL(1) parsers work by maintaining a workspace with a string in it. It's possible that the t we're looking at is not supposed to be matched against the current nonterminal A at all, and instead we're supposed to have A produce \u03b5 and then let some later nonterminal in the workspace match against the t . This is where FOLLOW sets come in. The FOLLOW set of a nonterminal X , denoted FOLLOW(X), is the set of all terminal symbols that can appear immediately after X in some derivation. When choosing which production to choose for A, we add in a final rule - if the terminal symbol t is in the FOLLOW set of A, we choose some production that ultimately will produce \u03b5 . That way, the A will \"disappear\" and we can match the t against some character that appears after the A nonterminal. This isn't a complete introduction to LL(1) parsing, but I hope it helps you see why we need FIRST and FOLLOW sets. For more information, pick up a book on parsing (I recommend Parsing Techniques: A Practical Guide by Grune and Jacobs) or take a course on compilers. As a totally shameless plug, I taught a compilers course in Summer 2012-2013 and all of the lecture slides are available online . Hope this helps!","title":"Constructing an LL(1) parsing table"},{"location":"Application/wikipedia-Parsing-algorithms/Top-down/wikipedia-Recursive-descent-parser/","text":"","title":"wikipedia Recursive descent parser"},{"location":"Application/wikipedia-Parsing-algorithms/Top-down/wikipedia-Top-down-parsing/","text":"Top-down parsing Top-down parsing # In computer science , top-down parsing is a parsing strategy where one first looks at the highest level of the parse tree and works down the parse tree by using the rewriting rules of a formal grammar .[ 1] LL parsers are a type of parser that uses a top-down parsing strategy. NOTE: Rewriting rules means expanding, substituting an nonterminal symbol with production body . Top-down parsing is a strategy of analyzing unknown data relationships by hypothesizing general parse tree structures and then considering whether the known fundamental structures are compatible with the hypothesis. It occurs in the analysis of both natural languages and computer languages . Top-down parsing can be viewed as an attempt to find left-most derivations of an input-stream by searching for parse-trees using a top-down expansion of the given formal grammar rules. Inclusive choice is used to accommodate ambiguity by expanding all alternative right-hand-sides of grammar rules.[ 2] Simple implementations of top-down parsing do not terminate for left-recursive grammars, and top-down parsing with backtracking may have exponential time complexity with respect to the length of the input for ambiguous CFGs .[ 3] However, more sophisticated top-down parsers have been created by Frost, Hafiz, and Callaghan [ 4] [ 5] which do accommodate ambiguity and left recursion in polynomial time and which generate polynomial-sized representations of the potentially exponential number of parse trees.","title":"wikipedia Top down parsing"},{"location":"Application/wikipedia-Parsing-algorithms/Top-down/wikipedia-Top-down-parsing/#top-down_parsing","text":"In computer science , top-down parsing is a parsing strategy where one first looks at the highest level of the parse tree and works down the parse tree by using the rewriting rules of a formal grammar .[ 1] LL parsers are a type of parser that uses a top-down parsing strategy. NOTE: Rewriting rules means expanding, substituting an nonterminal symbol with production body . Top-down parsing is a strategy of analyzing unknown data relationships by hypothesizing general parse tree structures and then considering whether the known fundamental structures are compatible with the hypothesis. It occurs in the analysis of both natural languages and computer languages . Top-down parsing can be viewed as an attempt to find left-most derivations of an input-stream by searching for parse-trees using a top-down expansion of the given formal grammar rules. Inclusive choice is used to accommodate ambiguity by expanding all alternative right-hand-sides of grammar rules.[ 2] Simple implementations of top-down parsing do not terminate for left-recursive grammars, and top-down parsing with backtracking may have exponential time complexity with respect to the length of the input for ambiguous CFGs .[ 3] However, more sophisticated top-down parsers have been created by Frost, Hafiz, and Callaghan [ 4] [ 5] which do accommodate ambiguity and left recursion in polynomial time and which generate polynomial-sized representations of the potentially exponential number of parse trees.","title":"Top-down parsing"},{"location":"Application/wikipedia-Parsing-algorithms/others/wikipedia-Chart-parser/","text":"Chart parser # In computer science , a chart parser is a type of parser suitable for ambiguous grammars (including grammars of natural languages ). It uses the dynamic programming approach\u2014partial hypothesized results are stored in a structure called a chart and can be re-used. This eliminates backtracking and prevents a combinatorial explosion . Chart parsing is generally credited to Martin Kay . Types of chart parsers # A common approach is to use a variant of the Viterbi algorithm . The Earley parser is a type of chart parser mainly used for parsing in computational linguistics , named for its inventor. Another chart parsing algorithm is the Cocke-Younger-Kasami (CYK) algorithm. Chart parsers can also be used for parsing computer languages. Earley parsers in particular have been used in compiler compilers where their ability to parse using arbitrary Context-free grammars eases the task of writing the grammar for a particular language. However their lower efficiency has led to people avoiding them for most compiler work. In bidirectional chart parsing, edges of the chart are marked with a direction, either forwards or backwards, and rules are enforced on the direction in which edges must point in order to be combined into further edges. In incremental chart parsing, the chart is constructed incrementally as the text is edited by the user, with each change to the text resulting in the minimal possible corresponding change to the chart. Chart parsers are distinguished between top-down and bottom-up , as well as active and passive.","title":"[Chart parser](https://en.wikipedia.org/wiki/Chart_parser)"},{"location":"Application/wikipedia-Parsing-algorithms/others/wikipedia-Chart-parser/#chart_parser","text":"In computer science , a chart parser is a type of parser suitable for ambiguous grammars (including grammars of natural languages ). It uses the dynamic programming approach\u2014partial hypothesized results are stored in a structure called a chart and can be re-used. This eliminates backtracking and prevents a combinatorial explosion . Chart parsing is generally credited to Martin Kay .","title":"Chart parser"},{"location":"Application/wikipedia-Parsing-algorithms/others/wikipedia-Chart-parser/#types_of_chart_parsers","text":"A common approach is to use a variant of the Viterbi algorithm . The Earley parser is a type of chart parser mainly used for parsing in computational linguistics , named for its inventor. Another chart parsing algorithm is the Cocke-Younger-Kasami (CYK) algorithm. Chart parsers can also be used for parsing computer languages. Earley parsers in particular have been used in compiler compilers where their ability to parse using arbitrary Context-free grammars eases the task of writing the grammar for a particular language. However their lower efficiency has led to people avoiding them for most compiler work. In bidirectional chart parsing, edges of the chart are marked with a direction, either forwards or backwards, and rules are enforced on the direction in which edges must point in order to be combined into further edges. In incremental chart parsing, the chart is constructed incrementally as the text is edited by the user, with each change to the text resulting in the minimal possible corresponding change to the chart. Chart parsers are distinguished between top-down and bottom-up , as well as active and passive.","title":"Types of chart parsers"},{"location":"Application/wikipedia-Parsing-algorithms/others/wikipedia-Earley-parser/","text":"Earley parser #","title":"[Earley parser](https://en.wikipedia.org/wiki/Earley_parser)"},{"location":"Application/wikipedia-Parsing-algorithms/others/wikipedia-Earley-parser/#earley_parser","text":"","title":"Earley parser"},{"location":"Application/wikipedia-String/","text":"\u5173\u4e8e\u672c\u7ae0 # \u672c\u7ae0\u8bb2\u8ff0\u4e0e\u5b57\u7b26\u4e32\u76f8\u5173\u7684\u7b97\u6cd5\u3002 algorithm to judge whether two string has substring in common # Finding all the common substrings of given two strings Check if two strings have a common substring longest common substring problem suffix tree suffix tree # Ukkonen's suffix tree algorithm in plain English https://stackoverflow.com/questions/tagged/suffix-tree?tab=Active \u4f7f\u7528\u7f16\u8f91\u8ddd\u79bb # \u4e00\u79cdpython\u5b9e\u73b0 # def get_num_of_word_in_common(word1, word2): \"\"\"\u516c\u5171\u8bcd\u7684\u4e2a\u6570\"\"\" return len(Counter(word1) & Counter(word2))","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"Application/wikipedia-String/#_1","text":"\u672c\u7ae0\u8bb2\u8ff0\u4e0e\u5b57\u7b26\u4e32\u76f8\u5173\u7684\u7b97\u6cd5\u3002","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"Application/wikipedia-String/#algorithm_to_judge_whether_two_string_has_substring_in_common","text":"Finding all the common substrings of given two strings Check if two strings have a common substring longest common substring problem suffix tree","title":"algorithm to judge whether two string has substring in common"},{"location":"Application/wikipedia-String/#suffix_tree","text":"Ukkonen's suffix tree algorithm in plain English https://stackoverflow.com/questions/tagged/suffix-tree?tab=Active","title":"suffix tree"},{"location":"Application/wikipedia-String/#_2","text":"","title":"\u4f7f\u7528\u7f16\u8f91\u8ddd\u79bb"},{"location":"Application/wikipedia-String/#python","text":"def get_num_of_word_in_common(word1, word2): \"\"\"\u516c\u5171\u8bcd\u7684\u4e2a\u6570\"\"\" return len(Counter(word1) & Counter(word2))","title":"\u4e00\u79cdpython\u5b9e\u73b0"},{"location":"Application/wikipedia-String/String(computer-science)/","text":"","title":"String(computer science)"},{"location":"Application/wikipedia-String/Regular-expression/wikipedia-Powerset-construction/","text":"Powerset construction # In the theory of computation and automata theory , the powerset construction or subset construction is a standard method for converting a nondeterministic finite automaton (NFA) into a deterministic finite automaton (DFA) which recognizes the same formal language . It is important in theory because it establishes that NFAs, despite their additional flexibility, are unable to recognize any language that cannot be recognized by some DFA. It is also important in practice for converting easier-to-construct NFAs into more efficiently executable DFAs. However, if the NFA has n states, the resulting DFA may have up to $2^n$ states, an exponentially larger number, which sometimes makes the construction impractical for large NFAs. The construction, sometimes called the Rabin\u2013Scott powerset construction (or subset construction) to distinguish it from similar constructions for other types of automata, was first published by Michael O. Rabin and Dana Scott in 1959.[ 1] Intuition # To simulate the operation of a DFA on a given input string, one needs to keep track of a single state at any time: the state that the automaton will reach after seeing a prefix of the input. In contrast, to simulate an NFA, one needs to keep track of a set of states : all of the states that the automaton could reach after seeing the same prefix of the input, according to the nondeterministic choices made by the automaton. If, after a certain prefix of the input, a set S of states can be reached, then after the next input symbol x the set of reachable states is a deterministic function of S and x . Therefore, the sets of reachable NFA states play the same role in the NFA simulation as single DFA states play in the DFA simulation, and in fact the sets of NFA states appearing in this simulation may be re-interpreted as being states of a DFA.[ 2] NOTE: The difference between DFA and NFA can help to understand why a single state in DFA versus a set of states in NFA, below is come from Finite-state machine : In a deterministic automaton, every state has exactly one transition for each possible input. In a non-deterministic automaton, an input can lead to one, more than one, or no transition for a given state. The powerset construction algorithm can transform any nondeterministic automaton into a (usually more complex) deterministic automaton with identical functionality.","title":"[Powerset construction](https://en.wikipedia.org/wiki/Powerset_construction)"},{"location":"Application/wikipedia-String/Regular-expression/wikipedia-Powerset-construction/#powerset_construction","text":"In the theory of computation and automata theory , the powerset construction or subset construction is a standard method for converting a nondeterministic finite automaton (NFA) into a deterministic finite automaton (DFA) which recognizes the same formal language . It is important in theory because it establishes that NFAs, despite their additional flexibility, are unable to recognize any language that cannot be recognized by some DFA. It is also important in practice for converting easier-to-construct NFAs into more efficiently executable DFAs. However, if the NFA has n states, the resulting DFA may have up to $2^n$ states, an exponentially larger number, which sometimes makes the construction impractical for large NFAs. The construction, sometimes called the Rabin\u2013Scott powerset construction (or subset construction) to distinguish it from similar constructions for other types of automata, was first published by Michael O. Rabin and Dana Scott in 1959.[ 1]","title":"Powerset construction"},{"location":"Application/wikipedia-String/Regular-expression/wikipedia-Powerset-construction/#intuition","text":"To simulate the operation of a DFA on a given input string, one needs to keep track of a single state at any time: the state that the automaton will reach after seeing a prefix of the input. In contrast, to simulate an NFA, one needs to keep track of a set of states : all of the states that the automaton could reach after seeing the same prefix of the input, according to the nondeterministic choices made by the automaton. If, after a certain prefix of the input, a set S of states can be reached, then after the next input symbol x the set of reachable states is a deterministic function of S and x . Therefore, the sets of reachable NFA states play the same role in the NFA simulation as single DFA states play in the DFA simulation, and in fact the sets of NFA states appearing in this simulation may be re-interpreted as being states of a DFA.[ 2] NOTE: The difference between DFA and NFA can help to understand why a single state in DFA versus a set of states in NFA, below is come from Finite-state machine : In a deterministic automaton, every state has exactly one transition for each possible input. In a non-deterministic automaton, an input can lead to one, more than one, or no transition for a given state. The powerset construction algorithm can transform any nondeterministic automaton into a (usually more complex) deterministic automaton with identical functionality.","title":"Intuition"},{"location":"Application/wikipedia-String/String-metric/wikipedia-Edit-distance/","text":"Edit distance Edit distance #","title":"wikipedia Edit distance"},{"location":"Application/wikipedia-String/String-metric/wikipedia-Edit-distance/#edit_distance","text":"","title":"Edit distance"},{"location":"Application/wikipedia-String/String-metric/wikipedia-String-metric/","text":"String metric #","title":"[String metric](https://en.wikipedia.org/wiki/String_metric)"},{"location":"Application/wikipedia-String/String-metric/wikipedia-String-metric/#string_metric","text":"","title":"String metric"},{"location":"Application/wikipedia-String/string-searching/wikipedia-String-searching-algorithm/","text":"String-searching algorithm Basic classification of search algorithms Single-pattern algorithms Algorithms using a finite set of patterns Algorithms using an infinite number of patterns Other classification Na\u00efve string search Finite-state-automaton-based search Stubs Index methods Other variants String-searching algorithm # Basic classification of search algorithms # The various algorithms can be classified by the number of patterns each uses. Single-pattern algorithms # Let m be the length of the pattern, n be the length of the searchable text and k = |\u03a3| be the size of the alphabet. Algorithm Preprocessing time Matching time[ 1] Space Na\u00efve string-search algorithm none \u0398(nm) none Rabin\u2013Karp algorithm \u0398(m) average \u0398(n + m), worst \u0398((n\u2212m)m) O(1) Knuth\u2013Morris\u2013Pratt algorithm \u0398(m) \u0398(n) \u0398(m) Boyer\u2013Moore string-search algorithm \u0398(m + k) best \u03a9(n/m), worst O(mn) \u0398(k) Bitap algorithm ( shift-or , shift-and , Baeza\u2013Yates\u2013Gonnet ; fuzzy; agrep) \u0398(m + k) O(mn) Two-way string-matching algorithm (glibc memmem/strstr)[ 3] \u0398(m) O(n+m) O(1) BNDM (Backward Non-Deterministic DAWG Matching) (fuzzy + regex; nrgrep)[ 4] O(m) O(n) BOM (Backward Oracle Matching)[ 5] O(m) O(mn) 1. ^ Asymptotic times are expressed using O, \u03a9, and \u0398 notation . The Boyer\u2013Moore string-search algorithm has been the standard benchmark for the practical string-search literature.[ 6] Algorithms using a finite set of patterns # Aho\u2013Corasick string matching algorithm (extension of Knuth-Morris-Pratt) Commentz-Walter algorithm (extension of Boyer-Moore) Set-BOM (extension of Backward Oracle Matching) Rabin\u2013Karp string search algorithm Algorithms using an infinite number of patterns # Naturally, the patterns can not be enumerated finitely in this case. They are represented usually by a regular grammar or regular expression . Other classification # Other classification approaches are possible. One of the most common uses preprocessing as main criteria. Classes of string searching algorithms [ 7] Text not preprocessed Text preprocessed Patterns not preprocessed Elementary algorithms Index methods Patterns preprocessed Constructed search engines Signature methods :[ 8] Another one classifies the algorithms by their matching strategy:[ 9] Match the prefix first (Knuth-Morris-Pratt, Shift-And, Aho-Corasick) Match the suffix first (Boyer-Moore and variants, Commentz-Walter) Match the best factor first (BNDM, BOM, Set-BOM) Other strategy (Naive, Rabin-Karp) Na\u00efve string search # A simple and inefficient way to see where one string occurs inside another is to check each place it could be, one by one, to see if it's there. So first we see if there's a copy of the needle in the first character of the haystack; if not, we look to see if there's a copy of the needle starting at the second character of the haystack; if not, we look starting at the third character, and so forth. In the normal case, we only have to look at one or two characters for each wrong position to see that it is a wrong position, so in the average case, this takes O ( n + m ) steps, where n is the length of the haystack and m is the length of the needle; but in the worst case, searching for a string like \"aaaab\" in a string like \"aaaaaaaaab\", it takes O ( nm ) Finite-state-automaton-based search # In this approach, we avoid backtracking by constructing a deterministic finite automaton (DFA) that recognizes stored search string. These are expensive to construct\u2014they are usually created using the powerset construction \u2014but are very quick to use. For example, the DFA shown to the right recognizes the word \"MOMMY\". This approach is frequently generalized in practice to search for arbitrary regular expressions . Stubs # Knuth\u2013Morris\u2013Pratt computes a DFA that recognizes inputs with the string to search for as a suffix, Boyer\u2013Moore starts searching from the end of the needle, so it can usually jump ahead a whole needle-length at each step. Baeza\u2013Yates keeps track of whether the previous j characters were a prefix of the search string, and is therefore adaptable to fuzzy string searching . The bitap algorithm is an application of Baeza\u2013Yates' approach. Index methods # Faster search algorithms preprocess the text. After building a substring index , for example a suffix tree or suffix array , the occurrences of a pattern can be found quickly. As an example, a suffix tree can be built in $ \\Theta (n) $ time, and all $ z $ occurrences of a pattern can be found in $ O(m) $ time under the assumption that the alphabet has a constant size and all inner nodes in the suffix tree know what leaves are underneath them. The latter can be accomplished by running a DFS algorithm from the root of the suffix tree. NOTE: https://github.com/fxsjy/jieba Other variants # Some search methods, for instance trigram search , are intended to find a \"closeness\" score between the search string and the text rather than a \"match/non-match\". These are sometimes called \"fuzzy\" searches .","title":"wikipedia String searching algorithm"},{"location":"Application/wikipedia-String/string-searching/wikipedia-String-searching-algorithm/#string-searching_algorithm","text":"","title":"String-searching algorithm"},{"location":"Application/wikipedia-String/string-searching/wikipedia-String-searching-algorithm/#basic_classification_of_search_algorithms","text":"The various algorithms can be classified by the number of patterns each uses.","title":"Basic classification of search algorithms"},{"location":"Application/wikipedia-String/string-searching/wikipedia-String-searching-algorithm/#single-pattern_algorithms","text":"Let m be the length of the pattern, n be the length of the searchable text and k = |\u03a3| be the size of the alphabet. Algorithm Preprocessing time Matching time[ 1] Space Na\u00efve string-search algorithm none \u0398(nm) none Rabin\u2013Karp algorithm \u0398(m) average \u0398(n + m), worst \u0398((n\u2212m)m) O(1) Knuth\u2013Morris\u2013Pratt algorithm \u0398(m) \u0398(n) \u0398(m) Boyer\u2013Moore string-search algorithm \u0398(m + k) best \u03a9(n/m), worst O(mn) \u0398(k) Bitap algorithm ( shift-or , shift-and , Baeza\u2013Yates\u2013Gonnet ; fuzzy; agrep) \u0398(m + k) O(mn) Two-way string-matching algorithm (glibc memmem/strstr)[ 3] \u0398(m) O(n+m) O(1) BNDM (Backward Non-Deterministic DAWG Matching) (fuzzy + regex; nrgrep)[ 4] O(m) O(n) BOM (Backward Oracle Matching)[ 5] O(m) O(mn) 1. ^ Asymptotic times are expressed using O, \u03a9, and \u0398 notation . The Boyer\u2013Moore string-search algorithm has been the standard benchmark for the practical string-search literature.[ 6]","title":"Single-pattern algorithms"},{"location":"Application/wikipedia-String/string-searching/wikipedia-String-searching-algorithm/#algorithms_using_a_finite_set_of_patterns","text":"Aho\u2013Corasick string matching algorithm (extension of Knuth-Morris-Pratt) Commentz-Walter algorithm (extension of Boyer-Moore) Set-BOM (extension of Backward Oracle Matching) Rabin\u2013Karp string search algorithm","title":"Algorithms using a finite set of patterns"},{"location":"Application/wikipedia-String/string-searching/wikipedia-String-searching-algorithm/#algorithms_using_an_infinite_number_of_patterns","text":"Naturally, the patterns can not be enumerated finitely in this case. They are represented usually by a regular grammar or regular expression .","title":"Algorithms using an infinite number of patterns"},{"location":"Application/wikipedia-String/string-searching/wikipedia-String-searching-algorithm/#other_classification","text":"Other classification approaches are possible. One of the most common uses preprocessing as main criteria. Classes of string searching algorithms [ 7] Text not preprocessed Text preprocessed Patterns not preprocessed Elementary algorithms Index methods Patterns preprocessed Constructed search engines Signature methods :[ 8] Another one classifies the algorithms by their matching strategy:[ 9] Match the prefix first (Knuth-Morris-Pratt, Shift-And, Aho-Corasick) Match the suffix first (Boyer-Moore and variants, Commentz-Walter) Match the best factor first (BNDM, BOM, Set-BOM) Other strategy (Naive, Rabin-Karp)","title":"Other classification"},{"location":"Application/wikipedia-String/string-searching/wikipedia-String-searching-algorithm/#naive_string_search","text":"A simple and inefficient way to see where one string occurs inside another is to check each place it could be, one by one, to see if it's there. So first we see if there's a copy of the needle in the first character of the haystack; if not, we look to see if there's a copy of the needle starting at the second character of the haystack; if not, we look starting at the third character, and so forth. In the normal case, we only have to look at one or two characters for each wrong position to see that it is a wrong position, so in the average case, this takes O ( n + m ) steps, where n is the length of the haystack and m is the length of the needle; but in the worst case, searching for a string like \"aaaab\" in a string like \"aaaaaaaaab\", it takes O ( nm )","title":"Na\u00efve string search"},{"location":"Application/wikipedia-String/string-searching/wikipedia-String-searching-algorithm/#finite-state-automaton-based_search","text":"In this approach, we avoid backtracking by constructing a deterministic finite automaton (DFA) that recognizes stored search string. These are expensive to construct\u2014they are usually created using the powerset construction \u2014but are very quick to use. For example, the DFA shown to the right recognizes the word \"MOMMY\". This approach is frequently generalized in practice to search for arbitrary regular expressions .","title":"Finite-state-automaton-based search"},{"location":"Application/wikipedia-String/string-searching/wikipedia-String-searching-algorithm/#stubs","text":"Knuth\u2013Morris\u2013Pratt computes a DFA that recognizes inputs with the string to search for as a suffix, Boyer\u2013Moore starts searching from the end of the needle, so it can usually jump ahead a whole needle-length at each step. Baeza\u2013Yates keeps track of whether the previous j characters were a prefix of the search string, and is therefore adaptable to fuzzy string searching . The bitap algorithm is an application of Baeza\u2013Yates' approach.","title":"Stubs"},{"location":"Application/wikipedia-String/string-searching/wikipedia-String-searching-algorithm/#index_methods","text":"Faster search algorithms preprocess the text. After building a substring index , for example a suffix tree or suffix array , the occurrences of a pattern can be found quickly. As an example, a suffix tree can be built in $ \\Theta (n) $ time, and all $ z $ occurrences of a pattern can be found in $ O(m) $ time under the assumption that the alphabet has a constant size and all inner nodes in the suffix tree know what leaves are underneath them. The latter can be accomplished by running a DFS algorithm from the root of the suffix tree. NOTE: https://github.com/fxsjy/jieba","title":"Index methods"},{"location":"Application/wikipedia-String/string-searching/wikipedia-String-searching-algorithm/#other_variants","text":"Some search methods, for instance trigram search , are intended to find a \"closeness\" score between the search string and the text rather than a \"match/non-match\". These are sometimes called \"fuzzy\" searches .","title":"Other variants"},{"location":"Application/wikipedia-String/string-searching/Aho-Corasick-algorithm/Aho-Corasick-algorithm/","text":"Aho-Corasick algorithm Construction of the trie Construction of an automaton Aho-Corasick algorithm # Let there be a set of strings with the total length mm (sum of all lengths). The Aho-Corasick algorithm constructs a data structure similar to a trie with some additional links, and then constructs a finite state machine (automaton) in $O(mk)$ time, where $k$ is the size of the used alphabet. The algorithm was proposed by Alfred Aho and Margaret Corasick in 1975. Construction of the trie # Formally a trie is a rooted tree, where each edge of the tree is labeled by some letter. All outgoing edge from one vertex mush have different labels. Consider any path in the trie from the root to any vertex. If we write out the labels of all edges on the path, we get a string that corresponds to this path. For any vertex in the trie we will associate the string from the root to the vertex. Each vertex will also have a flag leaf which will be true, if any string from the given set corresponds to this vertex. Accordingly to build a trie for a set of strings means to build a trie such that each leaf vertex will correspond to one string from the set, and conversely that each string of the set corresponds to one leaf vertex. We now describe how to construct a trie for a given set of strings in linear time with respect to their total length. We introduce a structure for the vertices of the tree. const int K = 26; struct Vertex { int next[K]; bool leaf = false; Vertex() { fill(begin(next), end(next), -1); } }; vector<Vertex> trie(1); Here we store the trie as an array of Vertex . Each Vertex contains the flag leaf , and the edges in the form of ans array next[] , where next[i] is the index to the vertex that we reach by following the character i , or \u22121, if there is no such edge. Initially the trie consists of only one vertex - the root - with the index 0. Now we implement a function that will add a string s to the trie. The implementation is extremely simple: we start at the root node, and as long as there are edges corresponding to the characters of s we follow them. If there is no edge for one character, we simply generate a new vertex and connect it via an edge. At the end of the process we mark the last vertex with flag leaf . void add_string(string const& s) { int v = 0; for (char ch : s) { int c = ch - 'a'; if (trie[v].next[c] == -1) { trie[v].next[c] = trie.size(); trie.emplace_back(); } v = trie[v].next[c]; } trie[v].leaf = true; } The implementation obviously runs in linear time. And since every vertex store k links, it will use O(mk) memory. It is possible to decrease the memory consumption to O(m) by using a map instead of an array in each vertex. However this will increase the complexity to O(nlogk) . Construction of an automaton # Suppose we have built a trie for the given set of strings. Now let's look at it from a different side. If we look at any vertex. The string that corresponds to it is a prefix of one or more strings in the set, thus each vertex of the trie can be interpreted as a position in one or more strings from the set. In fact the trie vertices can be interpreted as states in a finite deterministic automaton . From any state we can transition - using some input letter - to other states, i.e. to another position in the set of strings. For example, if there is only one string in the trie abc , and we are standing at vertex 2 (which corresponds to the string ab ), then using the letter c we can transition to the state 3 . Thus we can understand the edges of the trie as transitions in an automaton according to the corresponding letter. However for an automaton we cannot restrict the possible transitions for each state. If we try to perform a transition using a letter, and there is no corresponding edge in the trie, then we nevertheless must go into some state. Thus we can understand the edges of the trie as transitions in an automaton according to the corresponding letter. However for an automaton we cannot restrict the possible transitions for each state. If we try to perform a transition using a letter, and there is no corresponding edge in the trie, then we nevertheless must go into some state.","title":"Aho Corasick algorithm"},{"location":"Application/wikipedia-String/string-searching/Aho-Corasick-algorithm/Aho-Corasick-algorithm/#aho-corasick_algorithm","text":"Let there be a set of strings with the total length mm (sum of all lengths). The Aho-Corasick algorithm constructs a data structure similar to a trie with some additional links, and then constructs a finite state machine (automaton) in $O(mk)$ time, where $k$ is the size of the used alphabet. The algorithm was proposed by Alfred Aho and Margaret Corasick in 1975.","title":"Aho-Corasick algorithm"},{"location":"Application/wikipedia-String/string-searching/Aho-Corasick-algorithm/Aho-Corasick-algorithm/#construction_of_the_trie","text":"Formally a trie is a rooted tree, where each edge of the tree is labeled by some letter. All outgoing edge from one vertex mush have different labels. Consider any path in the trie from the root to any vertex. If we write out the labels of all edges on the path, we get a string that corresponds to this path. For any vertex in the trie we will associate the string from the root to the vertex. Each vertex will also have a flag leaf which will be true, if any string from the given set corresponds to this vertex. Accordingly to build a trie for a set of strings means to build a trie such that each leaf vertex will correspond to one string from the set, and conversely that each string of the set corresponds to one leaf vertex. We now describe how to construct a trie for a given set of strings in linear time with respect to their total length. We introduce a structure for the vertices of the tree. const int K = 26; struct Vertex { int next[K]; bool leaf = false; Vertex() { fill(begin(next), end(next), -1); } }; vector<Vertex> trie(1); Here we store the trie as an array of Vertex . Each Vertex contains the flag leaf , and the edges in the form of ans array next[] , where next[i] is the index to the vertex that we reach by following the character i , or \u22121, if there is no such edge. Initially the trie consists of only one vertex - the root - with the index 0. Now we implement a function that will add a string s to the trie. The implementation is extremely simple: we start at the root node, and as long as there are edges corresponding to the characters of s we follow them. If there is no edge for one character, we simply generate a new vertex and connect it via an edge. At the end of the process we mark the last vertex with flag leaf . void add_string(string const& s) { int v = 0; for (char ch : s) { int c = ch - 'a'; if (trie[v].next[c] == -1) { trie[v].next[c] = trie.size(); trie.emplace_back(); } v = trie[v].next[c]; } trie[v].leaf = true; } The implementation obviously runs in linear time. And since every vertex store k links, it will use O(mk) memory. It is possible to decrease the memory consumption to O(m) by using a map instead of an array in each vertex. However this will increase the complexity to O(nlogk) .","title":"Construction of the trie"},{"location":"Application/wikipedia-String/string-searching/Aho-Corasick-algorithm/Aho-Corasick-algorithm/#construction_of_an_automaton","text":"Suppose we have built a trie for the given set of strings. Now let's look at it from a different side. If we look at any vertex. The string that corresponds to it is a prefix of one or more strings in the set, thus each vertex of the trie can be interpreted as a position in one or more strings from the set. In fact the trie vertices can be interpreted as states in a finite deterministic automaton . From any state we can transition - using some input letter - to other states, i.e. to another position in the set of strings. For example, if there is only one string in the trie abc , and we are standing at vertex 2 (which corresponds to the string ab ), then using the letter c we can transition to the state 3 . Thus we can understand the edges of the trie as transitions in an automaton according to the corresponding letter. However for an automaton we cannot restrict the possible transitions for each state. If we try to perform a transition using a letter, and there is no corresponding edge in the trie, then we nevertheless must go into some state. Thus we can understand the edges of the trie as transitions in an automaton according to the corresponding letter. However for an automaton we cannot restrict the possible transitions for each state. If we try to perform a transition using a letter, and there is no corresponding edge in the trie, then we nevertheless must go into some state.","title":"Construction of an automaton"},{"location":"Application/wikipedia-String/string-searching/Aho-Corasick-algorithm/geeksforgeeks-Aho-Corasick-Algorithm/","text":"Aho-Corasick Algorithm for Pattern Searching Prepocessing : 1. Go To 2. Failure 3. Output Matching Preprocessing: Aho-Corasick Algorithm for Pattern Searching # Given an input text and an array of k words, arr[] , find all occurrences of all words in the input text. Let n be the length of text and m be the total number characters in all words, i.e. m = length(arr[0]) + length(arr[1]) + \u2026 + length(arr[k-1]) . Here k is total numbers of input words. Example: Input: text = \"ahishers\" arr[] = {\"he\", \"she\", \"hers\", \"his\"} Output: Word his appears from 1 to 3 Word he appears from 4 to 5 Word she appears from 3 to 5 Word hers appears from 4 to 7 If we use a linear time searching algorithm like KMP , then we need to one by one search all words in text[]. This gives us total time complexity as O(n + length(word[0]) + O(n + length(word[1]) + O(n + length(word[2]) + \u2026 O(n + length(word[k-1]) . This time complexity can be written as O(n*k + m) . Aho-Corasick Algorithm finds all words in O(n + m + z) time where z is total number of occurrences of words in text. The Aho\u2013Corasick string matching algorithm formed the basis of the original Unix command fgrep. Prepocessing : # Build an automaton of all words in arr[] The automaton has mainly three functions: 1. Go To # This function simply follows edges of Trie of all words in arr[] . It is represented as 2D array g[][] where we store next state for current state and character. 2. Failure # This function stores all edges that are followed when current character doesn't have edge in Trie. It is represented as 1D array f[] where we store next state for current state. 3. Output # Stores indexes of all words that end at current state. It is represented as 1D array o[] where we store indexes of all matching words as a bitmap for current state. Matching # Traverse the given text over built automaton to find all matching words. Preprocessing: # We first Build a Trie (or Keyword Tree) of all words. NOTE: The figure above has a mistake that edge i should start from node after h . This part fills entries in goto g[][] and output o[] . Next we extend Trie into an automaton to support linear time matching. This part fills entries in failure f[] and output o[] . Go to : We build Trie . And for all characters which don\u2019t have an edge at root, we add an edge back to root. Failure : For a state s , we find the longest proper suffix which is a proper prefix of some pattern. This is done using Breadth First Traversal of Trie. Output : For a state s , indexes of all words ending at s are stored. These indexes are stored as bitwise map (by doing bitwise OR of values). This is also computing using Breadth First Traversal with Failure. Below is C++ implementation of Aho-Corasick Algorithm","title":"geeksforgeeks Aho Corasick Algorithm"},{"location":"Application/wikipedia-String/string-searching/Aho-Corasick-algorithm/geeksforgeeks-Aho-Corasick-Algorithm/#aho-corasick_algorithm_for_pattern_searching","text":"Given an input text and an array of k words, arr[] , find all occurrences of all words in the input text. Let n be the length of text and m be the total number characters in all words, i.e. m = length(arr[0]) + length(arr[1]) + \u2026 + length(arr[k-1]) . Here k is total numbers of input words. Example: Input: text = \"ahishers\" arr[] = {\"he\", \"she\", \"hers\", \"his\"} Output: Word his appears from 1 to 3 Word he appears from 4 to 5 Word she appears from 3 to 5 Word hers appears from 4 to 7 If we use a linear time searching algorithm like KMP , then we need to one by one search all words in text[]. This gives us total time complexity as O(n + length(word[0]) + O(n + length(word[1]) + O(n + length(word[2]) + \u2026 O(n + length(word[k-1]) . This time complexity can be written as O(n*k + m) . Aho-Corasick Algorithm finds all words in O(n + m + z) time where z is total number of occurrences of words in text. The Aho\u2013Corasick string matching algorithm formed the basis of the original Unix command fgrep.","title":"Aho-Corasick Algorithm for Pattern Searching"},{"location":"Application/wikipedia-String/string-searching/Aho-Corasick-algorithm/geeksforgeeks-Aho-Corasick-Algorithm/#prepocessing","text":"Build an automaton of all words in arr[] The automaton has mainly three functions:","title":"Prepocessing :"},{"location":"Application/wikipedia-String/string-searching/Aho-Corasick-algorithm/geeksforgeeks-Aho-Corasick-Algorithm/#1_go_to","text":"This function simply follows edges of Trie of all words in arr[] . It is represented as 2D array g[][] where we store next state for current state and character.","title":"1. Go To"},{"location":"Application/wikipedia-String/string-searching/Aho-Corasick-algorithm/geeksforgeeks-Aho-Corasick-Algorithm/#2_failure","text":"This function stores all edges that are followed when current character doesn't have edge in Trie. It is represented as 1D array f[] where we store next state for current state.","title":"2. Failure"},{"location":"Application/wikipedia-String/string-searching/Aho-Corasick-algorithm/geeksforgeeks-Aho-Corasick-Algorithm/#3_output","text":"Stores indexes of all words that end at current state. It is represented as 1D array o[] where we store indexes of all matching words as a bitmap for current state.","title":"3. Output"},{"location":"Application/wikipedia-String/string-searching/Aho-Corasick-algorithm/geeksforgeeks-Aho-Corasick-Algorithm/#matching","text":"Traverse the given text over built automaton to find all matching words.","title":"Matching"},{"location":"Application/wikipedia-String/string-searching/Aho-Corasick-algorithm/geeksforgeeks-Aho-Corasick-Algorithm/#preprocessing","text":"We first Build a Trie (or Keyword Tree) of all words. NOTE: The figure above has a mistake that edge i should start from node after h . This part fills entries in goto g[][] and output o[] . Next we extend Trie into an automaton to support linear time matching. This part fills entries in failure f[] and output o[] . Go to : We build Trie . And for all characters which don\u2019t have an edge at root, we add an edge back to root. Failure : For a state s , we find the longest proper suffix which is a proper prefix of some pattern. This is done using Breadth First Traversal of Trie. Output : For a state s , indexes of all words ending at s are stored. These indexes are stored as bitwise map (by doing bitwise OR of values). This is also computing using Breadth First Traversal with Failure. Below is C++ implementation of Aho-Corasick Algorithm","title":"Preprocessing:"},{"location":"Application/wikipedia-String/string-searching/Aho-Corasick-algorithm/wikipedia-Aho-Corasick-algorithm/","text":"Aho\u2013Corasick algorithm Example Implementation Aho\u2013Corasick algorithm # In computer science , the Aho\u2013Corasick algorithm is a string-searching algorithm invented by Alfred V. Aho and Margaret J. Corasick.[ 1] It is a kind of dictionary-matching algorithm that locates elements of a finite set of strings (the \"dictionary\") within an input text. It matches all strings simultaneously. The complexity of the algorithm is linear in the length of the strings plus the length of the searched text plus the number of output matches. Note that because all matches are found, there can be a quadratic number of matches if every substring matches (e.g. dictionary = a , aa , aaa , aaaa and input string is aaaa ). Informally, the algorithm constructs a finite-state machine that resembles a trie with additional links between the various internal nodes. These extra internal links allow fast transitions between failed string matches (e.g. a search for cat in a trie that does not contain cat , but contains cart , and thus would fail at the node prefixed by ca ), to other branches of the trie that share a common prefix (e.g., in the previous case, a branch for attribute might be the best lateral transition). This allows the automaton to transition between string matches without the need for backtracking. When the string dictionary is known in advance (e.g. a computer virus database), the construction of the automaton can be performed once off-line and the compiled automaton stored for later use. In this case, its run time is linear in the length of the input plus the number of matched entries. The Aho\u2013Corasick string-matching algorithm formed the basis of the original Unix command fgrep . Example # In this example, we will consider a dictionary consisting of the following words: { a , ab , bab , bc , bca , c , caa }. The graph below is the Aho\u2013Corasick data structure constructed from the specified dictionary, with each row in the table representing a node in the trie, with the column path indicating the (unique) sequence of characters from the root to the node. A visualization of the trie for the dictionary on the right. Suffix links are in blue; dictionary suffix links in green. Nodes corresponding to dictionary entries are highlighted in blue. The data structure has one node for every prefix of every string in the dictionary. So if ( bca ) is in the dictionary, then there will be nodes for ( bca ), ( bc ), ( b ), and (). If a node is in the dictionary then it is a blue node . Otherwise it is a grey node . There is a black directed \"child\" arc from each node to a node whose name is found by appending one character. So there is a black arc from ( bc ) to ( bca ). There is a blue directed \"suffix\" arc from each node to the node that is the longest possible strict suffix of it in the graph . For example, for node ( caa ), its strict suffixes are ( aa ) and ( a ) and (). The longest of these that exists in the graph is ( a ). So there is a blue arc from ( caa ) to ( a ). The blue arcs can be computed in linear time by repeatedly traversing the blue arcs of a node's parent until the traversing node has a child matching the character of the target node. There is a green \"dictionary suffix\" arc from each node to the next node in the dictionary that can be reached by following blue arcs . For example, there is a green arc from ( bca ) to ( a ) because ( a ) is the first node in the dictionary (i.e. a blue node) that is reached when following the blue arcs to ( ca ) and then on to (a). The green arcs can be computed in linear time by repeatedly traversing blue arcs until a filled in node is found, and memoizing this information. Dictionary {a, ab, bab, bc, bca, c, caa} Path In dictionary Suffix link Dict suffix link () \u2013 (a) + () (ab) + (b) (b) \u2013 () (ba) \u2013 (a) (a) (bab) + (ab) (ab) (bc) + (c) (c) (bca) + (ca) (a) (c) + () (ca) \u2013 (a) (a) (caa) + (a) (a) At each step, the current node is extended by finding its child, and if that doesn't exist, finding its suffix's child, and if that doesn't work, finding its suffix's suffix's child, and so on, finally ending in the root node if nothing's seen before. When the algorithm reaches a node, it outputs all the dictionary entries that end at the current character position in the input text. This is done by printing every node reached by following the dictionary suffix links, starting from that node, and continuing until it reaches a node with no dictionary suffix link. In addition, the node itself is printed, if it is a dictionary entry. Execution on input string abccab yields the following steps: Implementation # https://github.com/WojciechMula/pyahocorasick","title":"wikipedia Aho Corasick algorithm"},{"location":"Application/wikipedia-String/string-searching/Aho-Corasick-algorithm/wikipedia-Aho-Corasick-algorithm/#ahocorasick_algorithm","text":"In computer science , the Aho\u2013Corasick algorithm is a string-searching algorithm invented by Alfred V. Aho and Margaret J. Corasick.[ 1] It is a kind of dictionary-matching algorithm that locates elements of a finite set of strings (the \"dictionary\") within an input text. It matches all strings simultaneously. The complexity of the algorithm is linear in the length of the strings plus the length of the searched text plus the number of output matches. Note that because all matches are found, there can be a quadratic number of matches if every substring matches (e.g. dictionary = a , aa , aaa , aaaa and input string is aaaa ). Informally, the algorithm constructs a finite-state machine that resembles a trie with additional links between the various internal nodes. These extra internal links allow fast transitions between failed string matches (e.g. a search for cat in a trie that does not contain cat , but contains cart , and thus would fail at the node prefixed by ca ), to other branches of the trie that share a common prefix (e.g., in the previous case, a branch for attribute might be the best lateral transition). This allows the automaton to transition between string matches without the need for backtracking. When the string dictionary is known in advance (e.g. a computer virus database), the construction of the automaton can be performed once off-line and the compiled automaton stored for later use. In this case, its run time is linear in the length of the input plus the number of matched entries. The Aho\u2013Corasick string-matching algorithm formed the basis of the original Unix command fgrep .","title":"Aho\u2013Corasick algorithm"},{"location":"Application/wikipedia-String/string-searching/Aho-Corasick-algorithm/wikipedia-Aho-Corasick-algorithm/#example","text":"In this example, we will consider a dictionary consisting of the following words: { a , ab , bab , bc , bca , c , caa }. The graph below is the Aho\u2013Corasick data structure constructed from the specified dictionary, with each row in the table representing a node in the trie, with the column path indicating the (unique) sequence of characters from the root to the node. A visualization of the trie for the dictionary on the right. Suffix links are in blue; dictionary suffix links in green. Nodes corresponding to dictionary entries are highlighted in blue. The data structure has one node for every prefix of every string in the dictionary. So if ( bca ) is in the dictionary, then there will be nodes for ( bca ), ( bc ), ( b ), and (). If a node is in the dictionary then it is a blue node . Otherwise it is a grey node . There is a black directed \"child\" arc from each node to a node whose name is found by appending one character. So there is a black arc from ( bc ) to ( bca ). There is a blue directed \"suffix\" arc from each node to the node that is the longest possible strict suffix of it in the graph . For example, for node ( caa ), its strict suffixes are ( aa ) and ( a ) and (). The longest of these that exists in the graph is ( a ). So there is a blue arc from ( caa ) to ( a ). The blue arcs can be computed in linear time by repeatedly traversing the blue arcs of a node's parent until the traversing node has a child matching the character of the target node. There is a green \"dictionary suffix\" arc from each node to the next node in the dictionary that can be reached by following blue arcs . For example, there is a green arc from ( bca ) to ( a ) because ( a ) is the first node in the dictionary (i.e. a blue node) that is reached when following the blue arcs to ( ca ) and then on to (a). The green arcs can be computed in linear time by repeatedly traversing blue arcs until a filled in node is found, and memoizing this information. Dictionary {a, ab, bab, bc, bca, c, caa} Path In dictionary Suffix link Dict suffix link () \u2013 (a) + () (ab) + (b) (b) \u2013 () (ba) \u2013 (a) (a) (bab) + (ab) (ab) (bc) + (c) (c) (bca) + (ca) (a) (c) + () (ca) \u2013 (a) (a) (caa) + (a) (a) At each step, the current node is extended by finding its child, and if that doesn't exist, finding its suffix's child, and if that doesn't work, finding its suffix's suffix's child, and so on, finally ending in the root node if nothing's seen before. When the algorithm reaches a node, it outputs all the dictionary entries that end at the current character position in the input text. This is done by printing every node reached by following the dictionary suffix links, starting from that node, and continuing until it reaches a node with no dictionary suffix link. In addition, the node itself is printed, if it is a dictionary entry. Execution on input string abccab yields the following steps:","title":"Example"},{"location":"Application/wikipedia-String/string-searching/Aho-Corasick-algorithm/wikipedia-Aho-Corasick-algorithm/#implementation","text":"https://github.com/WojciechMula/pyahocorasick","title":"Implementation"},{"location":"Application/wikipedia-String/string-searching/Knuth\u2013Morris\u2013Pratt-algorithm/Knuth\u2013Morris\u2013Pratt-algorithm/","text":"Introduction 1. Knuth\u2013Morris\u2013Pratt algorithm 2. \u8be6\u89e3KMP\u7b97\u6cd5 \u6c42\u89e3next\u6570\u7ec4 \u5f53P[k] == P[j]\u65f6 \u5f53P[k] != P[j]\u65f6, 3. Computing the KMP failure function (f(k)) definition of f(k) Naive way to find f(k): Relating f(k) to f(k\u22121) Fact between f(k) and f(k\u22121) Computation trick 1 Prelude to computation trick 2 Computation trick #2 Algorithm to compute KMP failure function KMP\u5b9e\u73b0\u5206\u6790 \u8ba1\u7b97KMP failure function\u7684\u9012\u5f52\u516c\u5f0f \u8ba1\u7b97KMP failure function\u7684python\u5b9e\u73b0 \u8ba1\u7b97KMP failure function \u548c dynamic programming KMP\u7684\u5b9e\u73b0 KMP in leetcode Introduction # It takes me some effort to master KMP algorithm. Here are three articles that helped me solve the mystery as I learned. 1. Knuth\u2013Morris\u2013Pratt algorithm # In computer science , the Knuth\u2013Morris\u2013Pratt string-searching algorithm (or KMP algorithm ) searches for occurrences of a \"word\" W within a main \"text string\" S by employing the observation that when a mismatch occurs, the word itself embodies sufficient information to determine where the next match could begin, thus bypassing re-examination of previously matched characters. The algorithm was conceived by James H. Morris and independently discovered by Donald Knuth \"a few weeks later\" from automata theory.[ 1] [ 2] Morris and Vaughan Pratt published a technical report in 1970.[ 3] The three also published the algorithm jointly in 1977.[ 1] Independently, in 1969, Matiyasevich [ 4] [ 5] discovered a similar algorithm, coded by a two-dimensional Turing machine, while studying a string-pattern-matching recognition problem over a binary alphabet. This was the first linear-time algorithm for string matching. 2. \u8be6\u89e3KMP\u7b97\u6cd5 # KMP\u7b97\u6cd5\u8981\u89e3\u51b3\u7684\u95ee\u9898\u5c31\u662f\u5728\u5b57\u7b26\u4e32\uff08\u4e5f\u53eb\u4e3b\u4e32\uff09\u4e2d\u7684\u6a21\u5f0f\uff08pattern\uff09\u5b9a\u4f4d\u95ee\u9898\u3002\u8bf4\u7b80\u5355\u70b9\u5c31\u662f\u6211\u4eec\u5e73\u65f6\u5e38\u8bf4\u7684\u5173\u952e\u5b57\u641c\u7d22\u3002\u6a21\u5f0f\u4e32\u5c31\u662f\u5173\u952e\u5b57\uff08\u63a5\u4e0b\u6765\u79f0\u5b83\u4e3a P \uff09\uff0c\u5982\u679c\u5b83\u5728\u4e00\u4e2a\u4e3b\u4e32\uff08\u63a5\u4e0b\u6765\u79f0\u4e3a T \uff09\u4e2d\u51fa\u73b0\uff0c\u5c31\u8fd4\u56de\u5b83\u7684\u5177\u4f53\u4f4d\u7f6e\uff0c\u5426\u5219\u8fd4\u56de -1 \uff08\u5e38\u7528\u624b\u6bb5\uff09\u3002 \u9996\u5148\uff0c\u5bf9\u4e8e\u8fd9\u4e2a\u95ee\u9898\u6709\u4e00\u4e2a\u5f88\u5355\u7eaf\u7684\u60f3\u6cd5\uff1a\u4ece\u5de6\u5230\u53f3\u4e00\u4e2a\u4e2a\u5339\u914d\uff0c\u5982\u679c\u8fd9\u4e2a\u8fc7\u7a0b\u4e2d\u6709\u67d0\u4e2a\u5b57\u7b26\u4e0d\u5339\u914d\uff0c\u5c31\u8df3\u56de\u53bb\uff0c\u5c06\u6a21\u5f0f\u4e32\u5411\u53f3\u79fb\u52a8\u4e00\u4f4d\u3002\u8fd9\u6709\u4ec0\u4e48\u96be\u7684\uff1f \u6211\u4eec\u53ef\u4ee5\u8fd9\u6837\u521d\u59cb\u5316\uff1a \u4e4b\u540e\u6211\u4eec\u53ea\u9700\u8981\u6bd4\u8f83 i \u6307\u9488 \u6307\u5411\u7684\u5b57\u7b26\u548c j \u6307\u9488 \u6307\u5411\u7684\u5b57\u7b26\u662f\u5426\u4e00\u81f4\u3002\u5982\u679c\u4e00\u81f4\u5c31\u90fd\u5411\u540e\u79fb\u52a8\uff0c\u5982\u679c\u4e0d\u4e00\u81f4\uff0c\u5982\u4e0b\u56fe\uff1a A \u548c E \u4e0d\u76f8\u7b49\uff0c\u90a3\u5c31\u628a i \u6307\u9488 \u79fb\u56de\u7b2c1\u4f4d\uff08\u5047\u8bbe\u4e0b\u6807\u4ece0\u5f00\u59cb\uff09\uff0c j \u79fb\u52a8\u5230\u6a21\u5f0f\u4e32\u7684\u7b2c0\u4f4d\uff0c\u7136\u540e\u53c8\u91cd\u65b0\u5f00\u59cb\u8fd9\u4e2a\u6b65\u9aa4\uff1a \u57fa\u4e8e\u8fd9\u4e2a\u60f3\u6cd5\u6211\u4eec\u53ef\u4ee5\u5f97\u5230\u4ee5\u4e0b\u7684\u7a0b\u5e8f\uff1a /** * \u66b4\u529b\u7834\u89e3\u6cd5 * @param ts \u4e3b\u4e32 * @param ps \u6a21\u5f0f\u4e32 * @return \u5982\u679c\u627e\u5230\uff0c\u8fd4\u56de\u5728\u4e3b\u4e32\u4e2d\u7b2c\u4e00\u4e2a\u5b57\u7b26\u51fa\u73b0\u7684\u4e0b\u6807\uff0c\u5426\u5219\u4e3a-1 */ public static int bf(String ts, String ps) { char[] t = ts.toCharArray(); char[] p = ps.toCharArray(); int i = 0; // \u4e3b\u4e32\u7684\u4f4d\u7f6e int j = 0; // \u6a21\u5f0f\u4e32\u7684\u4f4d\u7f6e while (i < t.length && j < p.length) { if (t[i] == p[j]) { // \u5f53\u4e24\u4e2a\u5b57\u7b26\u76f8\u540c\uff0c\u5c31\u6bd4\u8f83\u4e0b\u4e00\u4e2a i++; j++; } else { i = i - j + 1; // \u4e00\u65e6\u4e0d\u5339\u914d\uff0ci\u540e\u9000 j = 0; // j\u5f520 } } if (j == p.length) { return i - j; } else { return -1; } } \u4e0a\u9762\u7684\u7a0b\u5e8f\u662f\u6ca1\u6709\u95ee\u9898\u7684\uff0c\u4f46\u4e0d\u591f\u597d\uff01 NOTE: geeksforgeeks\u7684\u6587\u7ae0 Naive algorithm for Pattern Searching \u4e2d\u7ed9\u51fa\u7684\u4ee3\u7801\u662f\u6bd4\u4e0a\u8ff0\u4ee3\u7801\u66f4\u52a0\u5bb9\u6613\u7406\u89e3\u7684\u3002 \u5982\u679c\u662f\u4eba\u4e3a\u6765\u5bfb\u627e\u7684\u8bdd\uff0c\u80af\u5b9a\u4e0d\u4f1a\u518d\u628a i \u79fb\u52a8\u56de\u7b2c1\u4f4d\uff0c \u56e0\u4e3a\u4e3b\u4e32\u5339\u914d\u5931\u8d25\u7684\u4f4d\u7f6e\u524d\u9762\u9664\u4e86\u7b2c\u4e00\u4e2a A \u4e4b\u5916\u518d\u4e5f\u6ca1\u6709 A \u4e86\uff0c\u6211\u4eec\u4e3a\u4ec0\u4e48\u80fd\u77e5\u9053\u4e3b\u4e32\u524d\u9762\u53ea\u6709\u4e00\u4e2a A \uff1f \u56e0\u4e3a\u6211\u4eec\u5df2\u7ecf\u77e5\u9053\u524d\u9762\u4e09\u4e2a\u5b57\u7b26\u90fd\u662f\u5339\u914d\u7684\uff01\uff08\u8fd9\u5f88\u91cd\u8981\uff09 \u3002\u79fb\u52a8\u8fc7\u53bb\u80af\u5b9a\u4e5f\u662f\u4e0d\u5339\u914d\u7684\uff01\u6709\u4e00\u4e2a\u60f3\u6cd5\uff0c i \u53ef\u4ee5\u4e0d\u52a8\uff0c\u6211\u4eec\u53ea\u9700\u8981\u79fb\u52a8 j \u5373\u53ef\uff0c\u5982\u4e0b\u56fe\uff1a \u4e0a\u9762\u7684\u8fd9\u79cd\u60c5\u51b5\u8fd8\u662f\u6bd4\u8f83\u7406\u60f3\u7684\u60c5\u51b5\uff0c\u6211\u4eec\u6700\u591a\u4e5f\u5c31\u591a\u6bd4\u8f83\u4e86\u4e24\u6b21\u3002\u4f46\u5047\u5982\u662f\u5728\u4e3b\u4e32 SSSSSSSSSSSSSA \u4e2d\u67e5\u627e SSSSB \uff0c\u6bd4\u8f83\u5230\u6700\u540e\u4e00\u4e2a\u624d\u77e5\u9053\u4e0d\u5339\u914d\uff0c\u7136\u540e i \u56de\u6eaf \uff0c\u8fd9\u4e2a\u7684\u6548\u7387\u662f\u663e\u7136\u662f\u6700\u4f4e\u7684\u3002 NOTE: \u5173\u4e8e\u56de\u6eaf\uff0c\u53c2\u89c1 Backtracking \u5927\u725b\u4eec\u662f\u65e0\u6cd5\u5fcd\u53d7\u201c\u66b4\u529b\u7834\u89e3\u201d\u8fd9\u79cd\u4f4e\u6548\u7684\u624b\u6bb5\u7684\uff0c\u4e8e\u662f\u4ed6\u4eec\u4e09\u4e2a\u7814\u7a76\u51fa\u4e86KMP\u7b97\u6cd5\u3002\u5176\u601d\u60f3\u5c31\u5982\u540c\u6211\u4eec\u4e0a\u8fb9\u6240\u770b\u5230\u7684\u4e00\u6837\uff1a\u201c \u5229\u7528\u5df2\u7ecf\u90e8\u5206\u5339\u914d\u8fd9\u4e2a\u6709\u6548\u4fe1\u606f\uff0c\u4fdd\u6301 i \u6307\u9488\u4e0d\u56de\u6eaf\uff0c\u901a\u8fc7\u4fee\u6539 j \u6307\u9488\uff0c\u8ba9\u6a21\u5f0f\u4e32\u5c3d\u91cf\u5730\u79fb\u52a8\u5230\u6709\u6548\u7684\u4f4d\u7f6e \u3002\u201d NOTE: \u63d0\u9192\u4f60\u6ce8\u610f \u5c3d\u91cf\u5730 \u8fd9\u4e2a\u4fee\u9970\u8bed\uff0c\u7b49\u4f60\u5b8c\u5168\u7406\u89e3\u4e86KMP\u7b97\u6cd5\uff0c\u4f60\u5c31\u5e61\u7136\u9192\u609f\u8fd9\u4e2a\u4fee\u9970\u8bed\u662f\u975e\u5e38\u5999\u7684\u3002\u5176\u5b9e\u5728\u8fd9\u91cc\uff0c\u6211\u662f\u53ef\u4ee5\u5411\u4f60\u63d0\u524d\u900f\u9732\u7684\uff0c\u65e2\u7136\u8bf4\u662f\u5c3d\u91cf\uff0c\u90a3\u4e48\u4e5f\u5c31\u662f\u8bf4\u79fb\u52a8\u5230\u7684\u4f4d\u7f6e\u4e0d\u4e00\u5b9a\u662f\u6700\u6700\u6709\u6548\u7684\u4f4d\u7f6e\uff0c\u800c\u662f\u4e00\u4e2a\u76f8\u5bf9\u6709\u6548\u7684\u4f4d\u7f6e\uff0c\u53ef\u80fd\u9700\u8981\u7ecf\u8fc7\u591a\u6b21\u79fb\u52a8\u624d\u80fd\u591f\u5230\u8fbe\u6b63\u786e\u7684\u4f4d\u7f6e\uff0c\u6bd5\u7adf\u8ba1\u7b97\u673a\u4e0d\u662f\u50cf\u6211\u4eec\u4eba\u8fd9\u6837\u7684\u667a\u80fd\u3002 \u6240\u4ee5\uff0c\u6574\u4e2aKMP\u7684\u91cd\u70b9\u5c31\u5728\u4e8e \u5f53\u67d0\u4e00\u4e2a\u5b57\u7b26\u4e0e\u4e3b\u4e32\u4e0d\u5339\u914d\u65f6\uff0c\u6211\u4eec\u5e94\u8be5\u77e5\u9053 j \u6307\u9488\u8981\u79fb\u52a8\u5230\u54ea \uff1f \u63a5\u4e0b\u6765\u6211\u4eec\u81ea\u5df1\u6765\u53d1\u73b0 j \u7684\u79fb\u52a8\u89c4\u5f8b\uff1a \u5982\u56fe\uff1a C \u548c D \u4e0d\u5339\u914d\u4e86\uff0c\u6211\u4eec\u8981\u628a j \u79fb\u52a8\u5230\u54ea\uff1f\u663e\u7136\u662f\u7b2c1\u4f4d\u3002\u4e3a\u4ec0\u4e48\uff1f\u56e0\u4e3a\u524d\u9762\u6709\u4e00\u4e2a A \u76f8\u540c\u554a\uff1a \u5982\u4e0b\u56fe\u4e5f\u662f\u4e00\u6837\u7684\u60c5\u51b5\uff1a \u53ef\u4ee5\u628a j \u6307\u9488\u79fb\u52a8\u5230\u7b2c2\u4f4d\uff0c\u56e0\u4e3a\u524d\u9762\u6709\u4e24\u4e2a\u5b57\u6bcd\u662f\u4e00\u6837\u7684\uff1a \u81f3\u6b64\u6211\u4eec\u53ef\u4ee5\u5927\u6982\u770b\u51fa\u4e00\u70b9\u7aef\u502a\uff0c\u5f53\u5339\u914d\u5931\u8d25\u65f6\uff0c j \u8981\u79fb\u52a8\u7684\u4e0b\u4e00\u4e2a\u4f4d\u7f6e k \u3002\u5b58\u5728\u7740\u8fd9\u6837\u7684\u6027\u8d28\uff1a \u6700\u524d\u9762\u7684 k \u5b57\u7b26\u548c j \u4e4b\u524d\u7684\u6700\u540e k \u4e2a\u5b57\u7b26\u662f\u4e00\u6837\u7684 \u3002 \u5982\u679c\u7528\u6570\u5b66\u516c\u5f0f\u6765\u8868\u793a\u662f\u8fd9\u6837\u7684 P[0 ~ k-1] == P[j-k ~ j-1] \u8fd9\u4e2a\u76f8\u5f53\u91cd\u8981\uff0c\u5982\u679c\u89c9\u5f97\u4e0d\u597d\u8bb0\u7684\u8bdd\uff0c\u53ef\u4ee5\u901a\u8fc7\u4e0b\u56fe\u6765\u7406\u89e3\uff1a \u5f04\u660e\u767d\u4e86\u8fd9\u4e2a\u5c31\u5e94\u8be5\u53ef\u80fd\u660e\u767d\u4e3a\u4ec0\u4e48\u53ef\u4ee5\u76f4\u63a5\u5c06 j \u79fb\u52a8\u5230 k \u4f4d\u7f6e\u4e86\u3002 \u56e0\u4e3a: \u5f53 T[i] != P[j] \u65f6 \u6709 T[i-j ~ i-1] == P[0 ~ j-1] \u7531 P[0 ~ k-1] == P[j-k ~ j-1] \u5fc5\u7136\uff1a T[i-k ~ i-1] == P[0 ~ k-1] NOTE: \u4e0a\u8ff0\u516c\u5f0f\u5176\u5b9e\u5c31\u662fa==b, b==c,\u5219a==c \u516c\u5f0f\u5f88\u65e0\u804a\uff0c\u80fd\u770b\u660e\u767d\u5c31\u884c\u4e86\uff0c\u4e0d\u9700\u8981\u8bb0\u4f4f\u3002 NOTE: \u4f5c\u8005\u8fd9\u91cc\u7684\u603b\u7ed3\u4e0d\u591f\u76f4\u63a5\uff0c\u4e0b\u9762\u662f\u6458\u81ea\u767e\u5ea6\u767e\u79d1 kmp\u7b97\u6cd5 \u4e2d\u5bf9\u8fd9\u4e2a\u7ed3\u8bba\u7684\u603b\u7ed3\uff0c\u5b83\u975e\u5e38\u76f4\u63a5\uff1a \u7528\u66b4\u529b\u7b97\u6cd5\u5339\u914d\u5b57\u7b26\u4e32\u8fc7\u7a0b\u4e2d\uff0c\u6211\u4eec\u4f1a\u628a T[0] \u8ddf W[0] \u5339\u914d\uff0c\u5982\u679c\u76f8\u540c\u5219\u5339\u914d\u4e0b\u4e00\u4e2a\u5b57\u7b26\uff0c\u76f4\u5230\u51fa\u73b0\u4e0d\u76f8\u540c\u7684\u60c5\u51b5\uff0c\u6b64\u65f6\u6211\u4eec\u4f1a\u4e22\u5f03\u524d\u9762\u7684\u5339\u914d\u4fe1\u606f\uff0c\u7136\u540e\u628a T[1] \u8ddf W[0] \u5339\u914d\uff0c\u5faa\u73af\u8fdb\u884c\uff0c\u76f4\u5230\u4e3b\u4e32\u7ed3\u675f\uff0c\u6216\u8005\u51fa\u73b0\u5339\u914d\u6210\u529f\u7684\u60c5\u51b5\u3002\u8fd9\u79cd\u4e22\u5f03\u524d\u9762\u7684\u5339\u914d\u4fe1\u606f\u7684\u65b9\u6cd5\uff0c\u6781\u5927\u5730\u964d\u4f4e\u4e86\u5339\u914d\u6548\u7387\u3002 \u800c\u5728KMP\u7b97\u6cd5\u4e2d\uff0c\u5bf9\u4e8e\u6bcf\u4e00\u4e2a\u6a21\u5f0f\u4e32\u6211\u4eec\u4f1a\u4e8b\u5148\u8ba1\u7b97\u51fa\u6a21\u5f0f\u4e32\u7684\u5185\u90e8\u5339\u914d\u4fe1\u606f\uff0c\u5728\u5339\u914d\u5931\u8d25\u65f6\u6700\u5927\u7684\u79fb\u52a8\u6a21\u5f0f\u4e32\uff0c\u4ee5\u51cf\u5c11\u5339\u914d\u6b21\u6570\u3002 \u6bd4\u5982\uff0c\u5728\u7b80\u5355\u7684\u4e00\u6b21\u5339\u914d\u5931\u8d25\u540e\uff0c\u6211\u4eec\u4f1a\u60f3\u5c06\u6a21\u5f0f\u4e32\u5c3d\u91cf\u7684\u53f3\u79fb\u548c\u4e3b\u4e32\u8fdb\u884c\u5339\u914d\u3002\u53f3\u79fb\u7684\u8ddd\u79bb\u5728KMP\u7b97\u6cd5\u4e2d\u662f\u5982\u6b64\u8ba1\u7b97\u7684\uff1a\u5728 \u5df2\u7ecf\u5339\u914d\u7684\u6a21\u5f0f\u4e32\u5b50\u4e32 \u4e2d\uff0c\u627e\u51fa\u6700\u957f\u7684\u76f8\u540c\u7684 \u524d\u7f00 \u548c \u540e\u7f00 \uff0c\u7136\u540e\u79fb\u52a8\u4f7f\u5b83\u4eec\u91cd\u53e0\u3002 \u8fd9\u4e00\u6bb5\u53ea\u662f\u4e3a\u4e86\u8bc1\u660e\u6211\u4eec\u4e3a\u4ec0\u4e48\u53ef\u4ee5\u76f4\u63a5\u5c06 j \u79fb\u52a8\u5230 k \u800c\u65e0\u987b\u518d\u6bd4\u8f83\u524d\u9762\u7684 k \u4e2a\u5b57\u7b26\u3002 \u6c42\u89e3next\u6570\u7ec4 # \u597d\uff0c\u63a5\u4e0b\u6765\u5c31\u662f\u91cd\u70b9\u4e86\uff0c\u600e\u4e48\u6c42\u8fd9\u4e2a\uff08\u8fd9\u4e9b\uff09 k \u5462\uff1f\u56e0\u4e3a\u5728 P \u7684\u6bcf\u4e00\u4e2a\u4f4d\u7f6e\u90fd\u53ef\u80fd\u53d1\u751f\u4e0d\u5339\u914d\uff0c\u4e5f\u5c31\u662f\u8bf4\u6211\u4eec\u8981\u8ba1\u7b97\u6bcf\u4e00\u4e2a\u4f4d\u7f6e j \u5bf9\u5e94\u7684 k \uff0c\u6240\u4ee5\u7528\u4e00\u4e2a\u6570\u7ec4 next \u6765\u4fdd\u5b58\uff0c next[j] = k \uff0c\u8868\u793a\u5f53 T[i] != P[j] \u65f6\uff0c j \u6307\u9488 \u7684\u4e0b\u4e00\u4e2a\u4f4d\u7f6e\u3002 \u5f88\u591a\u6559\u6750\u6216\u535a\u6587\u5728\u8fd9\u4e2a\u5730\u65b9\u90fd\u662f\u8bb2\u5f97\u6bd4\u8f83\u542b\u7cca\u6216\u662f\u6839\u672c\u5c31\u4e00\u7b14\u5e26\u8fc7\uff0c\u751a\u81f3\u5c31\u662f\u8d34\u4e00\u6bb5\u4ee3\u7801\u4e0a\u6765\uff0c\u4e3a\u4ec0\u4e48\u662f\u8fd9\u6837\u6c42\uff1f\u600e\u4e48\u53ef\u4ee5\u8fd9\u6837\u6c42\uff1f\u6839\u672c\u5c31\u6ca1\u6709\u8bf4\u6e05\u695a\u3002\u800c\u8fd9\u91cc\u6070\u6070\u662f\u6574\u4e2a\u7b97\u6cd5\u6700\u5173\u952e\u7684\u5730\u65b9\u3002 public static int[] getNext(String ps) { char[] p = ps.toCharArray(); int[] next = new int[p.length]; next[0] = -1; int j = 0; int k = -1; while (j < p.length - 1) { if (k == -1 || p[j] == p[k]) { next[++j] = ++k; } else { k = next[k]; } } return next; } \u8fd9\u4e2a\u7248\u672c\u7684\u6c42 next \u6570\u7ec4\u7684\u7b97\u6cd5\u5e94\u8be5\u662f\u6d41\u4f20\u6700\u5e7f\u6cdb\u7684\uff0c\u4ee3\u7801\u662f\u5f88\u7b80\u6d01\u3002\u53ef\u662f\u771f\u7684\u5f88\u8ba9\u4eba\u6478\u4e0d\u5230\u5934\u8111\uff0c\u5b83\u8fd9\u6837\u8ba1\u7b97\u7684\u4f9d\u636e\u5230\u5e95\u662f\u4ec0\u4e48\uff1f \u597d\uff0c\u5148\u628a\u8fd9\u4e2a\u653e\u4e00\u8fb9\uff0c\u6211\u4eec\u81ea\u5df1\u6765\u63a8\u5bfc\u601d\u8def\uff0c\u73b0\u5728\u8981\u59cb\u7ec8\u8bb0\u4f4f\u4e00\u70b9\uff0c next[j] \u7684\u503c\uff08\u4e5f\u5c31\u662f k \uff09\u8868\u793a\uff0c\u5f53 P[j] != T[i] \u65f6\uff0c j \u6307\u9488\u7684\u4e0b\u4e00\u6b65\u79fb\u52a8\u4f4d\u7f6e\u3002 \u5148\u6765\u770b\u7b2c\u4e00\u4e2a\uff1a\u5f53 j \u4e3a0\u65f6\uff0c\u5982\u679c\u8fd9\u65f6\u5019\u4e0d\u5339\u914d\uff0c\u600e\u4e48\u529e\uff1f \u50cf\u4e0a\u56fe\u8fd9\u79cd\u60c5\u51b5\uff0c j \u5df2\u7ecf\u5728\u6700\u5de6\u8fb9\u4e86\uff0c\u4e0d\u53ef\u80fd\u518d\u79fb\u52a8\u4e86\uff0c\u8fd9\u65f6\u5019\u8981\u5e94\u8be5\u662f i \u6307\u9488\u540e\u79fb\u3002\u6240\u4ee5\u5728\u4ee3\u7801\u4e2d\u624d\u4f1a\u6709 next[0] = -1; \u8fd9\u4e2a\u521d\u59cb\u5316\u3002 NOTE: \u770b\u4e86\u4e0b\u9762\u7684\u5b8c\u6574\u7684\u4ee3\u7801\u5c31\u77e5\u9053\u4e3a\u4ec0\u4e48\u4f7f\u7528 -1 \u6765\u4f5c\u4e3a\u521d\u59cb\u503c\uff0c\u56e0\u4e3a i++ \u548c j++ \u662f\u5728\u76f8\u540c\u7684\u5206\u652f\u4e2d\uff0c j++ \u540e j \u4e3a0\uff0c\u8fd9\u5c31\u4fdd\u8bc1\u4e86\u4eceP\u7684\u7b2c\u4e00\u4e2a\u5143\u7d20\u5f00\u59cb\u5339\u914d\u3002 \u5982\u679c\u662f\u5f53 j \u4e3a1\u7684\u65f6\u5019\u5462\uff1f \u663e\u7136\uff0c j \u6307\u9488\u4e00\u5b9a\u662f\u540e\u79fb\u52300\u4f4d\u7f6e\u7684\u3002\u56e0\u4e3a\u5b83\u524d\u9762\u4e5f\u5c31\u53ea\u6709\u8fd9\u4e00\u4e2a\u4f4d\u7f6e\u4e86\u3002 \u4e0b\u9762\u8fd9\u4e2a\u662f\u6700\u91cd\u8981\u7684\uff0c\u8bf7\u770b\u5982\u4e0b\u56fe\uff1a \u8bf7\u4ed4\u7ec6\u5bf9\u6bd4\u8fd9\u4e24\u4e2a\u56fe\u3002 \u6211\u4eec\u53d1\u73b0\u4e00\u4e2a\u89c4\u5f8b\uff1a \u5f53 P[k] == P[j] \u65f6 # \u5f53 P[k] == P[j] \u65f6\uff0c\u6709 next[j+1] == next[j] + 1 \u5176\u5b9e\u8fd9\u4e2a\u662f\u53ef\u4ee5\u8bc1\u660e\u7684\uff1a \u56e0\u4e3a\u5728 P[j] \u4e4b\u524d\u5df2\u7ecf\u6709 P[0 ~ k-1] == p[j-k ~ j-1] \u3002\uff08 next[j] == k \uff09 \u8fd9\u65f6\u5019\u73b0\u6709 P[k] == P[j] \uff0c\u6211\u4eec\u662f\u4e0d\u662f\u53ef\u4ee5\u5f97\u5230 P[0 ~ k-1] + P[k] == p[j-k ~ j-1] + P[j] \u3002 \u5373\uff1a P[0 ~ k] == P[j-k ~ j] \uff0c\u5373 next[j+1] == k + 1 == next[j] + 1 \u3002 \u8fd9\u91cc\u7684\u516c\u5f0f\u4e0d\u662f\u5f88\u597d\u61c2\uff0c\u8fd8\u662f\u770b\u56fe\u4f1a\u5bb9\u6613\u7406\u89e3\u4e9b\u3002 \u5f53 P[k] != P[j] \u65f6, # \u5f53 P[k] != P[j] \u65f6\uff0c\u5982\u4e0b\u56fe\u6240\u793a\uff1a \u50cf\u8fd9\u79cd\u60c5\u51b5\uff0c\u5982\u679c\u4f60\u4ece\u4ee3\u7801\u4e0a\u770b\u5e94\u8be5\u662f\u8fd9\u4e00\u53e5\uff1a k = next[k]; \u4e3a\u4ec0\u4e48\u662f\u8fd9\u6837\u5b50\uff1f\u4f60\u770b\u4e0b\u9762\u5e94\u8be5\u5c31\u660e\u767d\u4e86\u3002 \u73b0\u5728\u4f60\u5e94\u8be5\u77e5\u9053\u4e3a\u4ec0\u4e48\u8981 k = next[k] \u4e86\u5427\uff01\u50cf\u4e0a\u8fb9\u7684\u4f8b\u5b50\uff0c\u6211\u4eec\u5df2\u7ecf\u4e0d\u53ef\u80fd\u627e\u5230 [ A\uff0cB\uff0cA\uff0cB ] \u8fd9\u4e2a\u6700\u957f\u7684\u540e\u7f00\u4e32\u4e86\uff0c\u4f46\u6211\u4eec\u8fd8\u662f\u53ef\u80fd\u627e\u5230 [ A\uff0cB ] \u3001 [ B ] \u8fd9\u6837\u7684\u524d\u7f00\u4e32\u7684\u3002\u6240\u4ee5\u8fd9\u4e2a\u8fc7\u7a0b\u50cf\u4e0d\u50cf\u5728\u5b9a\u4f4d [ A\uff0cB\uff0cA\uff0cC ] \u8fd9\u4e2a\u4e32\uff0c\u5f53 C \u548c\u4e3b\u4e32\u4e0d\u4e00\u6837\u4e86\uff08\u4e5f\u5c31\u662f k \u4f4d\u7f6e\u4e0d\u4e00\u6837\u4e86\uff09\uff0c\u90a3\u5f53\u7136\u662f\u628a\u6307\u9488\u79fb\u52a8\u5230 next[k] \u5566\u3002 NOTE: \u8fd9\u7bc7\u6587\u7ae0\u8fd9\u91cc\u7684\u5206\u6790\u8fd8\u662f\u6bd4\u8f83\u96be\u4ee5\u7406\u89e3\u7684\uff0c\u4e0b\u4e00\u7bc7\u5728\u5206\u6790\u66f4\u52a0\u900f\u5f7b\u3002 NOTE: \u6784\u5efa next \u6570\u7ec4\u7684\u7b97\u6cd5\u662f\u4f7f\u7528\u7684\u6570\u5b66\u5f52\u7eb3\u6cd5\u6765\u6c42\u89e3next\u6570\u7ec4\u7684\u6bcf\u4e2a\u503c\uff0c\u5373\u6839\u636e next \u6570\u7ec4\u4e2d\u524d j \u4e2a\u5143\u7d20\u7684\u503c\u6765\u6c42\u89e3 next[j+1] \u7684\u503c\u3002 \u6709\u4e86 next \u6570\u7ec4\u4e4b\u540e\u5c31\u4e00\u5207\u597d\u529e\u4e86\uff0c\u6211\u4eec\u53ef\u4ee5\u52a8\u624b\u5199KMP\u7b97\u6cd5\u4e86\uff1a public static int KMP(String ts, String ps) { char[] t = ts.toCharArray(); char[] p = ps.toCharArray(); int i = 0; // \u4e3b\u4e32\u7684\u4f4d\u7f6e int j = 0; // \u6a21\u5f0f\u4e32\u7684\u4f4d\u7f6e int[] next = getNext(ps); while (i < t.length && j < p.length) { if (j == -1 || t[i] == p[j]) { // \u5f53j\u4e3a-1\u65f6\uff0c\u8981\u79fb\u52a8\u7684\u662fi\uff0c\u5f53\u7136j\u4e5f\u8981\u5f520 i++; j++; } else { // i\u4e0d\u9700\u8981\u56de\u6eaf\u4e86 // i = i - j + 1; j = next[j]; // j\u56de\u5230\u6307\u5b9a\u4f4d\u7f6e } } if (j == p.length) { return i - j; } else { return -1; } } \u548c\u66b4\u529b\u7834\u89e3\u76f8\u6bd4\uff0c\u5c31\u6539\u52a8\u4e864\u4e2a\u5730\u65b9\u3002\u5176\u4e2d\u6700\u4e3b\u8981\u7684\u4e00\u70b9\u5c31\u662f\uff0c i \u4e0d\u9700\u8981\u56de\u6eaf\u4e86\u3002 \u6700\u540e\uff0c\u6765\u770b\u4e00\u4e0b\u4e0a\u8fb9\u7684\u7b97\u6cd5\u5b58\u5728\u7684\u7f3a\u9677\u3002\u6765\u770b\u7b2c\u4e00\u4e2a\u4f8b\u5b50\uff1a \u663e\u7136\uff0c\u5f53\u6211\u4eec\u4e0a\u8fb9\u7684\u7b97\u6cd5\u5f97\u5230\u7684 next \u6570\u7ec4\u5e94\u8be5\u662f [ -1\uff0c0\uff0c0\uff0c1 ] \u6240\u4ee5\u4e0b\u4e00\u6b65\u6211\u4eec\u5e94\u8be5\u662f\u628a j \u79fb\u52a8\u5230\u7b2c1\u4e2a\u5143\u7d20\u54af\uff1a \u4e0d\u96be\u53d1\u73b0\uff0c \u8fd9\u4e00\u6b65\u662f\u5b8c\u5168\u6ca1\u6709\u610f\u4e49\u7684\u3002\u56e0\u4e3a\u540e\u9762\u7684 B \u5df2\u7ecf\u4e0d\u5339\u914d\u4e86\uff0c\u90a3\u524d\u9762\u7684 B \u4e5f\u4e00\u5b9a\u662f\u4e0d\u5339\u914d\u7684 \uff0c\u540c\u6837\u7684\u60c5\u51b5\u5176\u5b9e\u8fd8\u53d1\u751f\u5728\u7b2c2\u4e2a\u5143\u7d20 A \u4e0a\u3002 \u663e\u7136\uff0c \u53d1\u751f\u95ee\u9898\u7684\u539f\u56e0\u5728\u4e8e P[j] == P[next[j]] \u3002 \u6240\u4ee5\u6211\u4eec\u4e5f\u53ea\u9700\u8981\u6dfb\u52a0\u4e00\u4e2a\u5224\u65ad\u6761\u4ef6\u5373\u53ef\uff1a public static int[] getNext(String ps) { char[] p = ps.toCharArray(); int[] next = new int[p.length]; next[0] = -1; int j = 0; int k = -1; while (j < p.length - 1) { if (k == -1 || p[j] == p[k]) { if (p[++j] == p[++k]) { // \u5f53\u4e24\u4e2a\u5b57\u7b26\u76f8\u7b49\u65f6\u8981\u8df3\u8fc7 next[j] = next[k]; } else { next[j] = k; } } else { k = next[k]; } } return next; } \u8be5\u7b97\u6cd5\u7684\u5b9e\u73b0\u662f\u975e\u5e38\u7c7b\u4f3c\u4e8e\u52a8\u6001\u89c4\u5212\u7b97\u6cd5\u7684 next[j] \u7684\u503c k \u5c31\u662f j \u4f4d\u4e4b\u524d\u7684\u5b50\u4e32\u4e2d\uff0c\u524d\u7f00\u96c6\u548c\u540e\u7f00\u96c6\u4e2d\u7684\u6700\u5927\u91cd\u590d\u5b50\u4e32\u7684\u957f\u5ea6\u3002 abacabac next[0]=-1;j=0;k=-1 \u6761\u4ef61 \u6761\u4ef62 \u5206\u652f2 \u5206\u652f1 k==-1 next[1]=0;j=1;k=0 p[1]!=p[0]; k=next[0]=-1 k==-1 next[2]=0;j=2;k=0 p[2]==p[0]; next[3]=1;j=3;k=1 p[3]!=p[1]; k=next[1]=0 p[3]!=p[0]; k=next[0]=-1 k==-1; next[4]=0;j=4;k=0 p[4]==p[0] next[5]=1;j=5;k=1 p[5]==p[1] next[6]=2;j=6;k=2 p[6]==p[2] next[7]=3;j=7;k=3 p[7]==p[3] next[8]=4;j=8;k=4 \u8981\u60f3\u5f97\u5230 p[j+1] \uff0c\u53ea\u9700\u8981\u6bd4\u8f83 p[j] \u548c p[k] \u5373\u53ef\uff1b 3. Computing the KMP failure function (f(k)) # definition of f(k) # f(k) = MaxOverlap ( \"p0 p1 ... pk\" ) where: \"p0 p1 ... pk\" = the prefix of length k+1 of pattern P Graphically: Naive way to find f(k) : # Given P = \"p0 p1 ... pm-1\" Given k = 1, 2, ..., m-1 (k = 0 ==> f(0) = 0) 1. Extract the sub-pattern: \"p0 p1 ... pk\" 2. Find the first (= largest) overlap: Try: (p0) p1 p2 ... pk-1 p0 p1 ... pk-1 pk If (no match) Try: (p0) p1 p2 ... pk-1 p0 p1 ... pk-1 pk And so on... The first overlap is the longest ! NOTE: \u4e0a\u8ff0\u7b97\u6cd5\u662f\u4e00\u4e2a\u5faa\u73af\u7b97\u6cd5\uff0c\u5373 for k in range(1, m) \uff0c\u4e0b\u9762\u662f\u4e0a\u8ff0\u7b97\u6cd5\u7684python\u5b9e\u73b0\uff1a python def build_failure_table(p): \"\"\" \u6784\u5efa\u5b57\u7b26\u4e32p\u7684\u6700\u957f\u516c\u5171\u524d\u7f00\u540e\u7f00\u6570\u7ec4 :param p: :return: \"\"\" failure_table = list() len_of_p = len(p) for len_of_sub_str in range(1, len_of_p + 1): max_len_of_overlap = int(len_of_sub_str / 2) # \u6700\u5927\u91cd\u53e0\u524d\u7f00\u540e\u7f00\u7684\u957f\u5ea6 print(\"\u5b50\u4e32\u957f\u5ea6:{},\u6700\u5927\u91cd\u53e0\u524d\u7f00\u540e\u7f00\u957f\u5ea6:{}\".format(len_of_sub_str, max_len_of_overlap)) if max_len_of_overlap == 0: # \u957f\u5ea6\u4e3a1\u7684\u4e32\uff0c\u662f\u6ca1\u6709\u91cd\u53e0\u524d\u7f00\u540e\u7f00\u7684 failure_table.append(0) else: found = False # \u662f\u5426\u627e\u5230\u91cd\u53e0\u524d\u7f00\u540e\u7f00 for len_of_overlap in range(max_len_of_overlap, 0, -1): print(\"\u91cd\u53e0\u524d\u7f00\u540e\u7f00\u957f\u5ea6:{}\".format(len_of_overlap)) # len_of_overlap \u91cd\u53e0\u524d\u7f00\u540e\u7f00\u7684\u957f\u5ea6 for prefix_index in range(len_of_overlap): suffix_index = prefix_index + (len_of_sub_str - len_of_overlap) print(\"\u524d\u7f00\u8d77\u59cb\u4f4d\u7f6e:{},\u540e\u7f00\u8d77\u59cb\u4f4d\u7f6e:{}\".format(prefix_index, suffix_index)) if p[prefix_index] == p[suffix_index]: if suffix_index == len_of_sub_str - 1: # \u627e\u5230\u4e86\u91cd\u53e0\u90e8\u5206 failure_table.append(len_of_overlap) found = True break else: break if found: break if not found: failure_table.append(0) return failure_table Relating f(k) to f(k\u22121) # The values f(k) are computed easily using existing prefix overlap information : f(0) = 0 ( f(0) is always 0) f(1) is computing using (already computed) value f(0) f(2) is computing using (already computed) value f(0) , f(1) f(3) is computing using (already computed) value f(0) , f(1) , f(2) And so on According to the definition of f(k) : NOTE: \u4e0a\u9762\u8fd9\u79cd\u8868\u793a\u95ee\u9898\u7684\u65b9\u5f0f\u662f\u6bd4\u8f83\u5bb9\u6613\u7406\u89e3\u7684\uff0c\u5373\u5728\u539f\u95ee\u9898\u7684\u57fa\u7840\u4e0a\u6dfb\u52a0\u4e00\u4e2a\u65b0\u5143\u7d20\u4ece\u800c\u6784\u6210\u4e86\u4e00\u4e2a\u89c4\u6a21\u66f4\u5927\u7684\u95ee\u9898\u3002 Suppose that we know that: f(k\u22121) = x In other words: the longest overlapping suffix and prefix in \" p0 p1 ... pk-1 \" has x characters: f(k-1) = x characters <-----------------------> p1 p2 p3 ... pk-x-2 pk-x-3 pk-x-4 .... pk-1 ^ ^ ^ ^ | | | equal | v v v v p0 p1 p2 .... px-1 px ... pk-1 question: Can we use the fact that f(k\u22121) = x to compute f(k) ? answer: Yes, because f(k) is computed using a similar prefix as f(k\u22121): prefix used to compute f(k-1) +--------------------------------+ | | p0 p1 p2 .... px-1 ... pk-1 pk | | +------------------------------------+ prefix used to compute f(k) We will next learn how to exploit the similarity to compute f(k) Fact between f(k) and f(k\u22121) # Fact: f(k) \u2264 f(k\u22121) + 1 Computation trick 1 # Let use denote: f(k\u22121) = x (Note: f(k\u22121) is equal to some value. The above assumption simply gave a more convenient notation for this value). If px == pk , then: f(k) = x+1 (i.e., the maximum overlap of the prefix p0 p1 p2 .... pk-1 pk has x+1 characters Proof: These x+1 characters match IF pk == px! <----------------------------> p1 p2 p3 ... pk-x-2 pk-x-3 pk-x-4 .... pk-1 pk ^ ^ ^ ^ ^ | | | equal | |equal v v v v v p0 p1 p2 .... px-1 px ... pk-1 pk | | +--------------------------+ These characters matches because f(k-1) = x Prelude to computation trick 2 # Consider the prefix ababyabab where f(8) = 4: 012345678 prefix = ababyabab f(8) = 4 because: ababyabab ababyabab <--> 4 characters overlap We want to compute f(9) using f(8) , but now the next character does not match(that is the next char is not equal to y): 0123456789 prefix = ababyababa ababyababa ababyababa Conclusion: *** We CANNOT use f(8) to compute f(9) *** question: What should we try next to find the maximum overlap for the prefix \"ababyababa\" answer: To find the maximum overlap, we must slide the prefix down and look for matching letters !!! NOTE: \u601d\u8def\u662f\u4f7f\u7528\u5df2\u7ecf\u5339\u914d\u7684\u5b57\u7b26\u4e32\u6765\u5c3d\u53ef\u80fd\u51cf\u5c11\u5339\u914d\u6b21\u6570\u5e76\u4e14\u5bfb\u627e\u7b2c\u4e00\u4e2a\u6700\u53ef\u80fd\u7684\u4f4d\u7f6e\u3002 Now, let us use only the matching prefix information: ababyababa ababyababa Look only at these characters: ?????abab? abab?????? We can know for sure that the overlap cannot be found starting at these positions: ?????abab? abab?????? NOTE: \u56e0\u4e3a\u6211\u4eec\u77e5\u9053\u4e32 abab \u7684\u6700\u957f\u516c\u5171\u524d\u7f00\u540e\u7f00\u7684\u957f\u5ea6\u662f2\uff0c\u5373 f(3) \uff0c\u6240\u4ee5\u5b83\u7684\u524d\u4e24\u4e2a\u5143\u7d20\u53ef\u4ee5\u5339\u914d\u4e0a\u7684\uff0c\u6240\u4ee5\u7b2c\u4e00\u4e2a\u53ef\u80fd\u4f4d\u7f6e\u662f\u5982\u4e0b\u56fe\u6240\u793a\u7684\uff0c\u8fd9\u5c31\u662f\u5bf9\u5df2\u7ecf\u5339\u914d\u4fe1\u606f\u7684\u5145\u5206\u8fd0\u7528\u3002\u81f3\u4e8e\u7b2c\u4e09\u4e2a\u5143\u7d20\u662f\u5426\u80fd\u591f\u5339\u914d\u4e0a\uff0c\u5c31\u8981\u6bd4\u8f83\u7684\u7ed3\u679c\u4e86\u3002 The first possible way that overlap can be found is starting here: ?????abab? abab?????? In other words: we can compute f(9) using f(3) : 0123 prefix = abab abab abab f(3) = 2 Notice that: 3 = 4\u22121 and f(8) = 4 Worked out further: 0123456789 prefix = ababyababa ababyababa ababyababa ^ | compare the character at position 2 (f(3) = 2) Note: The prefix abab is hightlighted in yellow Because the characters are equal, we have found the maximum overlap: f(9) = f(3) + 1 = 2 + 1 = 3 !!! NOTE: \u8fd9\u91cc\u53ef\u4ee5\u5047\u8bbe\uff0c\u5982\u679c p[3] \u548c p[9] \u5e76\u4e0d\u76f8\u7b49\uff0c\u5219\u4e0a\u8ff0\u6d41\u7a0b\u9700\u8981\u7ee7\u7eed\u4e0b\u53bb\uff0c\u81f3\u4e8e\u7ec8\u6b62\u6761\u4ef6\uff0c\u663e\u7136\u662f\u76f4\u81f3\u6bd4\u8f83\u5230\u7b2c\u4e00\u4e2a\u5143\u7d20\u90fd\u4e0d\u76f8\u7b49\u3002 Computation trick #2 # Let: f(k\u22121) = x (Note: f(k\u22121) is equal to some value. The above assumption simply gave a more convenient notation for this value). If px \u2260 pk , then: The next prefix that can be used to compute f(k) is: p0 p1 .... px-1 In pseudo code: i = k-1; // Try to use f(k-1) to compute f(k) x = f(i); // x = character position to match against pk if ( P[k] == P[x] ) then f(k) = f(x\u22121) + 1 else Use: p0 p1 .... px-1 to compute f(k) What that means in terms of program statements: i = x-1; // Try to use f(x-1) to compute f(k) x = f(i); // x = character position to match against pk Note: We must repeat trick #2 as long as i \u2265 0, In other words: use a while loop instead of an if statement ! Algorithm to compute KMP failure function # Java code: public static int[] KMP_failure_function(String P) { int k, i, x, m; int f[] = new int[P.length()]; m = P.length(); f[0] = 0; // f(0) is always 0 for ( k = 1; k < m; k++ ) { // Compute f[k] i = k-1; // First try to use f(k-1) to compute f(k) x = f[i]; while ( P.charAt(x) != P.charAt(k) ) { i = x-1; // Try the next candidate f(.) to compute f(k) if ( i < 0 ) // Make sure x is valid break; // STOP the search !!! x = f[i]; } if ( i < 0 ) f[k] = 0; // No overlap at all: max overlap = 0 characters else f[k] = f[i] + 1; // We can compute f(k) using f(i) } return(f); } \u5b8c\u6574\u6d4b\u8bd5\u7a0b\u5e8f /* ---------------------------------- My own KMP Failure function alg S.Y. Cheung - 3/3/2013 ---------------------------------- */ import java.util.*; public class ComputeF { public static int[] KMP_failure_function(String P) { int k, i, x, m; int f[] = new int[P.length()]; String s; m = P.length(); f[0] = 0; for ( k = 1; k < m; k++ ) { // Compute f[k] s = P.substring(0,k+1); System.out.println(\"-----------------------------------------------\"); System.out.println(\"Prefix = \" + s + \" --- Computing f(\"+k+\"):\"); i = k-1; // First try to use f(k-1) to compute f(k) x = f[i]; System.out.println(\"===================================\"); System.out.println(\"Try using: f(\" + i + \") = \" + x ); printState(s, s, k, x); while ( P.charAt(x) != P.charAt(k) ) { i = f[i]-1; // Try the next candidate f(.) to compute f(k) if ( i < 0 ) // Search ended in failure.... break; x = f[i]; System.out.println(\"===================================\"); System.out.println(\"Try using: f(\" + i + \") = \" + x ); printState(s, s, k, x); } if ( i < 0 ) { System.out.println(\"No overlap possible... --> f[\"+k+\"] = 0\"); f[k] = 0; // No overlap possible } else { f[k] = f[i] + 1; // Compute f(k) using f(i) System.out.println(\"Overlap found ... --> f[\"+k+\"] = \"+f[k]); } } return(f); } public static void main(String[] args) { String P; Scanner in; int[] f; in = new Scanner( System.in ); System.out.print(\"P = \"); P = in.nextLine(); System.out.println(); f = KMP_failure_function(P); for (int i = 0; i < P.length(); i++ ) { System.out.println(\"f(\"+i+\") = \" + f[i]); } System.out.println(); } /* ===================================================== Variables and Methods to make the algorithm visual ===================================================== */ public static String T_ruler, P_ruler; public static String ruler(int n) { String out = \"\"; char x = '0'; for ( int i = 0; i < n; i++ ) { out = out + x; x++; if ( x > '9' ) x = '0'; } return out; } public static void printState(String T, String P, int i, int j) { T_ruler = ruler( T.length() ); P_ruler = ruler( P.length() ); System.out.println(\"=====================================\"); System.out.println(\"Matching: i = \" + i + \", j = \" + j); System.out.println(\" \" + T_ruler ); System.out.println(\" \" + T); System.out.print(\" \"); for ( int k = 0; k < i-j; k++) System.out.print(\" \"); System.out.println(P); System.out.print(\" \"); for ( int k = 0; k < i-j; k++) System.out.print(\" \"); System.out.println( P_ruler ); System.out.print(\" \"); for ( int k = 0; k < i; k++) System.out.print(\" \"); System.out.println(\"^\"); System.out.print(\" \"); for ( int k = 0; k < i; k++) System.out.print(\" \"); System.out.println(\"|\"); System.out.println(); } } KMP\u5b9e\u73b0\u5206\u6790 # \u901a\u8fc7\u4e0a\u8ff0\u4e09\u7bc7\u6587\u7ae0\uff0c\u80fd\u591f\u77e5\u9053KMP\u7b97\u6cd5\u7684\u539f\u7406\uff0c\u73b0\u5728\u9700\u8981\u8003\u8651\u7684\u662f\u5982\u4f55\u6765\u8fdb\u884c\u5b9e\u73b0\u3002 \u8ba1\u7b97KMP failure function\u7684\u9012\u5f52\u516c\u5f0f # \u5f53 pattern[j] \u4e0e pattern[f[j-1]] \u4e0d\u76f8\u7b49\u7684\u65f6\u5019\uff0c\u8fd9\u4e2a\u9012\u5f52\u516c\u5f0f\u4e2d\u6d89\u53ca\u5230\u4e86\u4e0d\u65ad\u5730\u5faa\u73af\u9012\u5f52\uff0c\u4f7f\u7528\u6570\u5b66\u516c\u5f0f\u4e0d\u65b9\u4fbf\u63cf\u8ff0\uff0c\u4e0b\u9762\u7684python\u7a0b\u5e8f\u662f\u975e\u5e38\u7b80\u6d01\u6613\u61c2\u7684\uff0c\u5e76\u4e14\u662f\u975e\u5e38\u63a5\u8fd1\u6570\u5b66\u516c\u5f0f\u7684\uff0c\u6240\u4ee5\u8fd9\u91cc\u5c31\u7701\u7565\u6389\u9012\u5f52\u516c\u5f0f\u3002 \u8ba1\u7b97KMP failure function\u7684python\u5b9e\u73b0 # failure function f(j) \u8868\u793a\u7684\u662f\u4ece pattern[0] \u5230 pattern[j] \u7684\u5e8f\u5217\uff08\u663e\u7136\u8fd9\u4e2a\u5e8f\u5217\u7684\u957f\u5ea6\u662f j+1 \uff09\u7684\u6700\u957f\u516c\u5171\u524d\u7f00\u540e\u7f00\u7684 \u957f\u5ea6 \uff0c\u5373 f(j) \u6240\u8868\u793a\u7684\u662f\u957f\u5ea6\u4e3a j+1 \u7684\u5e8f\u5217\u7684\u6700\u957f\u516c\u5171\u524d\u7f00\u540e\u7f00\u7684\u957f\u5ea6\u3002\u663e\u7136 f[0]==0 \uff0c\u56e0\u4e3a\u957f\u5ea6\u4e3a1\u7684\u5e8f\u5217\u7684\u6700\u957f\u524d\u7f00\u540e\u7f00\u7684\u957f\u5ea6\u4e3a0\u3002\u6240\u4ee5\uff0c\u5f53\u5df2\u77e5\u5e8f\u5217\u7684\u957f\u5ea6\u4e3a i \uff0c\u6765\u67e5\u8be2\u5176\u6700\u957f\u516c\u5171\u524d\u7f00\u540e\u7f00\u7684\u65f6\u5019\uff0c\u4f7f\u7528\u7684\u662f f(i-1) \u3002\u56e0\u4e3a i \u8868\u793a\u7684\u662f\u957f\u5ea6\uff0c\u6240\u4ee5 pattern[i] \u5f15\u7528\u7684\u662f\u6570\u7ec4\u7684\u7b2c i+1 \u4e2a\u5143\u7d20\u3002 def get_failure_array(pattern): failure = [0] # \u521d\u59cb\u6761\u4ef6 i = 0 # f(j-1)\u7684\u503c\uff0c\u662f\u5df2\u77e5\u7684\uff0c\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u5b83\u7684\u542b\u4e49\u662f\u957f\u5ea6 j = 1 # f(j)\u662f\u672a\u77e5\u7684\uff0cj\u8868\u793a\u7684\u662findex while j < len(pattern): if pattern[i] == pattern[j]: i += 1 elif i > 0: i = failure[i - 1] continue j += 1 failure.append(i) return failure \u8ba1\u7b97KMP failure function \u548c dynamic programming # KMP\u7684failure function\u7684\u6c42\u89e3\u8fc7\u7a0b\u5728\u8ba1\u7b97 f(k+1) \u7684\u65f6\u6240\u4f9d\u8d56\u7684 f(0),f(1)...,f(k) \u90fd\u662f\u901a\u8fc7\u67e5failure table\u800c\u83b7\u5f97\u7684\uff0c\u800c\u4e0d\u662f\u91cd\u65b0\u8ba1\u7b97\uff0c\u8fd9\u5176\u5b9e\u5c31\u662f\u52a8\u6001\u89c4\u5212\u7b97\u6cd5\u7684\u601d\u60f3\u3002\u5728\u4e0a\u8ff0\u4ee3\u7801\u4e2d\uff0c i \u5c31\u8868\u793a\u8ba1\u7b97 f(k+1) \u6240\u4f9d\u8d56\u7684\u6570\u636e\uff0c\u5b83\u7684\u5b9e\u73b0\u65b9\u5f0f\u662f\u975e\u5e38\u7c7b\u4f3c\u4e8e\u8fed\u4ee3\u7248\u7684\u6590\u6ce2\u90a3\u5951\u6570\u5217\u3002 KMP\u7684\u5b9e\u73b0 # def kmp_search(pattern, text): \"\"\" :param pattern: :param text: :return: \"\"\" # 1) Construct the failure array failure = get_failure_array(pattern) # 2) Step through text searching for pattern i, j = 0, 0 # index into text, pattern while i < len(text): if pattern[j] == text[i]: if j == (len(pattern) - 1): return True j += 1 elif j > 0: # if this is a prefix in our pattern # just go back far enough to continue j = failure[j - 1] continue i += 1 return False \u601d\u8003\uff1a\u4e3a\u4ec0\u4e48 j = failure[j - 1] \uff1f\u5176\u5b9e\u7ed3\u5408\u524d\u9762\u7684\u4f8b\u5b50\u5c31\u53ef\u4ee5\u77e5\u9053\u4e86\uff0c\u8fd9\u91cc\u4e0d\u518d\u8d58\u8ff0\u3002 KMP in leetcode # http://www.voidcn.com/article/p-uuefgkai-bnw.html https://leetcode-cn.com/problems/implement-strstr/comments/ https://leetcode.com/problems/shortest-palindrome/discuss/60113/clean-kmp-solution-with-super-detailed-explanation","title":"Knuth\u2013Morris\u2013Pratt algorithm"},{"location":"Application/wikipedia-String/string-searching/Knuth\u2013Morris\u2013Pratt-algorithm/Knuth\u2013Morris\u2013Pratt-algorithm/#introduction","text":"It takes me some effort to master KMP algorithm. Here are three articles that helped me solve the mystery as I learned.","title":"Introduction"},{"location":"Application/wikipedia-String/string-searching/Knuth\u2013Morris\u2013Pratt-algorithm/Knuth\u2013Morris\u2013Pratt-algorithm/#1_knuthmorrispratt_algorithm","text":"In computer science , the Knuth\u2013Morris\u2013Pratt string-searching algorithm (or KMP algorithm ) searches for occurrences of a \"word\" W within a main \"text string\" S by employing the observation that when a mismatch occurs, the word itself embodies sufficient information to determine where the next match could begin, thus bypassing re-examination of previously matched characters. The algorithm was conceived by James H. Morris and independently discovered by Donald Knuth \"a few weeks later\" from automata theory.[ 1] [ 2] Morris and Vaughan Pratt published a technical report in 1970.[ 3] The three also published the algorithm jointly in 1977.[ 1] Independently, in 1969, Matiyasevich [ 4] [ 5] discovered a similar algorithm, coded by a two-dimensional Turing machine, while studying a string-pattern-matching recognition problem over a binary alphabet. This was the first linear-time algorithm for string matching.","title":"1. Knuth\u2013Morris\u2013Pratt algorithm"},{"location":"Application/wikipedia-String/string-searching/Knuth\u2013Morris\u2013Pratt-algorithm/Knuth\u2013Morris\u2013Pratt-algorithm/#2_kmp","text":"KMP\u7b97\u6cd5\u8981\u89e3\u51b3\u7684\u95ee\u9898\u5c31\u662f\u5728\u5b57\u7b26\u4e32\uff08\u4e5f\u53eb\u4e3b\u4e32\uff09\u4e2d\u7684\u6a21\u5f0f\uff08pattern\uff09\u5b9a\u4f4d\u95ee\u9898\u3002\u8bf4\u7b80\u5355\u70b9\u5c31\u662f\u6211\u4eec\u5e73\u65f6\u5e38\u8bf4\u7684\u5173\u952e\u5b57\u641c\u7d22\u3002\u6a21\u5f0f\u4e32\u5c31\u662f\u5173\u952e\u5b57\uff08\u63a5\u4e0b\u6765\u79f0\u5b83\u4e3a P \uff09\uff0c\u5982\u679c\u5b83\u5728\u4e00\u4e2a\u4e3b\u4e32\uff08\u63a5\u4e0b\u6765\u79f0\u4e3a T \uff09\u4e2d\u51fa\u73b0\uff0c\u5c31\u8fd4\u56de\u5b83\u7684\u5177\u4f53\u4f4d\u7f6e\uff0c\u5426\u5219\u8fd4\u56de -1 \uff08\u5e38\u7528\u624b\u6bb5\uff09\u3002 \u9996\u5148\uff0c\u5bf9\u4e8e\u8fd9\u4e2a\u95ee\u9898\u6709\u4e00\u4e2a\u5f88\u5355\u7eaf\u7684\u60f3\u6cd5\uff1a\u4ece\u5de6\u5230\u53f3\u4e00\u4e2a\u4e2a\u5339\u914d\uff0c\u5982\u679c\u8fd9\u4e2a\u8fc7\u7a0b\u4e2d\u6709\u67d0\u4e2a\u5b57\u7b26\u4e0d\u5339\u914d\uff0c\u5c31\u8df3\u56de\u53bb\uff0c\u5c06\u6a21\u5f0f\u4e32\u5411\u53f3\u79fb\u52a8\u4e00\u4f4d\u3002\u8fd9\u6709\u4ec0\u4e48\u96be\u7684\uff1f \u6211\u4eec\u53ef\u4ee5\u8fd9\u6837\u521d\u59cb\u5316\uff1a \u4e4b\u540e\u6211\u4eec\u53ea\u9700\u8981\u6bd4\u8f83 i \u6307\u9488 \u6307\u5411\u7684\u5b57\u7b26\u548c j \u6307\u9488 \u6307\u5411\u7684\u5b57\u7b26\u662f\u5426\u4e00\u81f4\u3002\u5982\u679c\u4e00\u81f4\u5c31\u90fd\u5411\u540e\u79fb\u52a8\uff0c\u5982\u679c\u4e0d\u4e00\u81f4\uff0c\u5982\u4e0b\u56fe\uff1a A \u548c E \u4e0d\u76f8\u7b49\uff0c\u90a3\u5c31\u628a i \u6307\u9488 \u79fb\u56de\u7b2c1\u4f4d\uff08\u5047\u8bbe\u4e0b\u6807\u4ece0\u5f00\u59cb\uff09\uff0c j \u79fb\u52a8\u5230\u6a21\u5f0f\u4e32\u7684\u7b2c0\u4f4d\uff0c\u7136\u540e\u53c8\u91cd\u65b0\u5f00\u59cb\u8fd9\u4e2a\u6b65\u9aa4\uff1a \u57fa\u4e8e\u8fd9\u4e2a\u60f3\u6cd5\u6211\u4eec\u53ef\u4ee5\u5f97\u5230\u4ee5\u4e0b\u7684\u7a0b\u5e8f\uff1a /** * \u66b4\u529b\u7834\u89e3\u6cd5 * @param ts \u4e3b\u4e32 * @param ps \u6a21\u5f0f\u4e32 * @return \u5982\u679c\u627e\u5230\uff0c\u8fd4\u56de\u5728\u4e3b\u4e32\u4e2d\u7b2c\u4e00\u4e2a\u5b57\u7b26\u51fa\u73b0\u7684\u4e0b\u6807\uff0c\u5426\u5219\u4e3a-1 */ public static int bf(String ts, String ps) { char[] t = ts.toCharArray(); char[] p = ps.toCharArray(); int i = 0; // \u4e3b\u4e32\u7684\u4f4d\u7f6e int j = 0; // \u6a21\u5f0f\u4e32\u7684\u4f4d\u7f6e while (i < t.length && j < p.length) { if (t[i] == p[j]) { // \u5f53\u4e24\u4e2a\u5b57\u7b26\u76f8\u540c\uff0c\u5c31\u6bd4\u8f83\u4e0b\u4e00\u4e2a i++; j++; } else { i = i - j + 1; // \u4e00\u65e6\u4e0d\u5339\u914d\uff0ci\u540e\u9000 j = 0; // j\u5f520 } } if (j == p.length) { return i - j; } else { return -1; } } \u4e0a\u9762\u7684\u7a0b\u5e8f\u662f\u6ca1\u6709\u95ee\u9898\u7684\uff0c\u4f46\u4e0d\u591f\u597d\uff01 NOTE: geeksforgeeks\u7684\u6587\u7ae0 Naive algorithm for Pattern Searching \u4e2d\u7ed9\u51fa\u7684\u4ee3\u7801\u662f\u6bd4\u4e0a\u8ff0\u4ee3\u7801\u66f4\u52a0\u5bb9\u6613\u7406\u89e3\u7684\u3002 \u5982\u679c\u662f\u4eba\u4e3a\u6765\u5bfb\u627e\u7684\u8bdd\uff0c\u80af\u5b9a\u4e0d\u4f1a\u518d\u628a i \u79fb\u52a8\u56de\u7b2c1\u4f4d\uff0c \u56e0\u4e3a\u4e3b\u4e32\u5339\u914d\u5931\u8d25\u7684\u4f4d\u7f6e\u524d\u9762\u9664\u4e86\u7b2c\u4e00\u4e2a A \u4e4b\u5916\u518d\u4e5f\u6ca1\u6709 A \u4e86\uff0c\u6211\u4eec\u4e3a\u4ec0\u4e48\u80fd\u77e5\u9053\u4e3b\u4e32\u524d\u9762\u53ea\u6709\u4e00\u4e2a A \uff1f \u56e0\u4e3a\u6211\u4eec\u5df2\u7ecf\u77e5\u9053\u524d\u9762\u4e09\u4e2a\u5b57\u7b26\u90fd\u662f\u5339\u914d\u7684\uff01\uff08\u8fd9\u5f88\u91cd\u8981\uff09 \u3002\u79fb\u52a8\u8fc7\u53bb\u80af\u5b9a\u4e5f\u662f\u4e0d\u5339\u914d\u7684\uff01\u6709\u4e00\u4e2a\u60f3\u6cd5\uff0c i \u53ef\u4ee5\u4e0d\u52a8\uff0c\u6211\u4eec\u53ea\u9700\u8981\u79fb\u52a8 j \u5373\u53ef\uff0c\u5982\u4e0b\u56fe\uff1a \u4e0a\u9762\u7684\u8fd9\u79cd\u60c5\u51b5\u8fd8\u662f\u6bd4\u8f83\u7406\u60f3\u7684\u60c5\u51b5\uff0c\u6211\u4eec\u6700\u591a\u4e5f\u5c31\u591a\u6bd4\u8f83\u4e86\u4e24\u6b21\u3002\u4f46\u5047\u5982\u662f\u5728\u4e3b\u4e32 SSSSSSSSSSSSSA \u4e2d\u67e5\u627e SSSSB \uff0c\u6bd4\u8f83\u5230\u6700\u540e\u4e00\u4e2a\u624d\u77e5\u9053\u4e0d\u5339\u914d\uff0c\u7136\u540e i \u56de\u6eaf \uff0c\u8fd9\u4e2a\u7684\u6548\u7387\u662f\u663e\u7136\u662f\u6700\u4f4e\u7684\u3002 NOTE: \u5173\u4e8e\u56de\u6eaf\uff0c\u53c2\u89c1 Backtracking \u5927\u725b\u4eec\u662f\u65e0\u6cd5\u5fcd\u53d7\u201c\u66b4\u529b\u7834\u89e3\u201d\u8fd9\u79cd\u4f4e\u6548\u7684\u624b\u6bb5\u7684\uff0c\u4e8e\u662f\u4ed6\u4eec\u4e09\u4e2a\u7814\u7a76\u51fa\u4e86KMP\u7b97\u6cd5\u3002\u5176\u601d\u60f3\u5c31\u5982\u540c\u6211\u4eec\u4e0a\u8fb9\u6240\u770b\u5230\u7684\u4e00\u6837\uff1a\u201c \u5229\u7528\u5df2\u7ecf\u90e8\u5206\u5339\u914d\u8fd9\u4e2a\u6709\u6548\u4fe1\u606f\uff0c\u4fdd\u6301 i \u6307\u9488\u4e0d\u56de\u6eaf\uff0c\u901a\u8fc7\u4fee\u6539 j \u6307\u9488\uff0c\u8ba9\u6a21\u5f0f\u4e32\u5c3d\u91cf\u5730\u79fb\u52a8\u5230\u6709\u6548\u7684\u4f4d\u7f6e \u3002\u201d NOTE: \u63d0\u9192\u4f60\u6ce8\u610f \u5c3d\u91cf\u5730 \u8fd9\u4e2a\u4fee\u9970\u8bed\uff0c\u7b49\u4f60\u5b8c\u5168\u7406\u89e3\u4e86KMP\u7b97\u6cd5\uff0c\u4f60\u5c31\u5e61\u7136\u9192\u609f\u8fd9\u4e2a\u4fee\u9970\u8bed\u662f\u975e\u5e38\u5999\u7684\u3002\u5176\u5b9e\u5728\u8fd9\u91cc\uff0c\u6211\u662f\u53ef\u4ee5\u5411\u4f60\u63d0\u524d\u900f\u9732\u7684\uff0c\u65e2\u7136\u8bf4\u662f\u5c3d\u91cf\uff0c\u90a3\u4e48\u4e5f\u5c31\u662f\u8bf4\u79fb\u52a8\u5230\u7684\u4f4d\u7f6e\u4e0d\u4e00\u5b9a\u662f\u6700\u6700\u6709\u6548\u7684\u4f4d\u7f6e\uff0c\u800c\u662f\u4e00\u4e2a\u76f8\u5bf9\u6709\u6548\u7684\u4f4d\u7f6e\uff0c\u53ef\u80fd\u9700\u8981\u7ecf\u8fc7\u591a\u6b21\u79fb\u52a8\u624d\u80fd\u591f\u5230\u8fbe\u6b63\u786e\u7684\u4f4d\u7f6e\uff0c\u6bd5\u7adf\u8ba1\u7b97\u673a\u4e0d\u662f\u50cf\u6211\u4eec\u4eba\u8fd9\u6837\u7684\u667a\u80fd\u3002 \u6240\u4ee5\uff0c\u6574\u4e2aKMP\u7684\u91cd\u70b9\u5c31\u5728\u4e8e \u5f53\u67d0\u4e00\u4e2a\u5b57\u7b26\u4e0e\u4e3b\u4e32\u4e0d\u5339\u914d\u65f6\uff0c\u6211\u4eec\u5e94\u8be5\u77e5\u9053 j \u6307\u9488\u8981\u79fb\u52a8\u5230\u54ea \uff1f \u63a5\u4e0b\u6765\u6211\u4eec\u81ea\u5df1\u6765\u53d1\u73b0 j \u7684\u79fb\u52a8\u89c4\u5f8b\uff1a \u5982\u56fe\uff1a C \u548c D \u4e0d\u5339\u914d\u4e86\uff0c\u6211\u4eec\u8981\u628a j \u79fb\u52a8\u5230\u54ea\uff1f\u663e\u7136\u662f\u7b2c1\u4f4d\u3002\u4e3a\u4ec0\u4e48\uff1f\u56e0\u4e3a\u524d\u9762\u6709\u4e00\u4e2a A \u76f8\u540c\u554a\uff1a \u5982\u4e0b\u56fe\u4e5f\u662f\u4e00\u6837\u7684\u60c5\u51b5\uff1a \u53ef\u4ee5\u628a j \u6307\u9488\u79fb\u52a8\u5230\u7b2c2\u4f4d\uff0c\u56e0\u4e3a\u524d\u9762\u6709\u4e24\u4e2a\u5b57\u6bcd\u662f\u4e00\u6837\u7684\uff1a \u81f3\u6b64\u6211\u4eec\u53ef\u4ee5\u5927\u6982\u770b\u51fa\u4e00\u70b9\u7aef\u502a\uff0c\u5f53\u5339\u914d\u5931\u8d25\u65f6\uff0c j \u8981\u79fb\u52a8\u7684\u4e0b\u4e00\u4e2a\u4f4d\u7f6e k \u3002\u5b58\u5728\u7740\u8fd9\u6837\u7684\u6027\u8d28\uff1a \u6700\u524d\u9762\u7684 k \u5b57\u7b26\u548c j \u4e4b\u524d\u7684\u6700\u540e k \u4e2a\u5b57\u7b26\u662f\u4e00\u6837\u7684 \u3002 \u5982\u679c\u7528\u6570\u5b66\u516c\u5f0f\u6765\u8868\u793a\u662f\u8fd9\u6837\u7684 P[0 ~ k-1] == P[j-k ~ j-1] \u8fd9\u4e2a\u76f8\u5f53\u91cd\u8981\uff0c\u5982\u679c\u89c9\u5f97\u4e0d\u597d\u8bb0\u7684\u8bdd\uff0c\u53ef\u4ee5\u901a\u8fc7\u4e0b\u56fe\u6765\u7406\u89e3\uff1a \u5f04\u660e\u767d\u4e86\u8fd9\u4e2a\u5c31\u5e94\u8be5\u53ef\u80fd\u660e\u767d\u4e3a\u4ec0\u4e48\u53ef\u4ee5\u76f4\u63a5\u5c06 j \u79fb\u52a8\u5230 k \u4f4d\u7f6e\u4e86\u3002 \u56e0\u4e3a: \u5f53 T[i] != P[j] \u65f6 \u6709 T[i-j ~ i-1] == P[0 ~ j-1] \u7531 P[0 ~ k-1] == P[j-k ~ j-1] \u5fc5\u7136\uff1a T[i-k ~ i-1] == P[0 ~ k-1] NOTE: \u4e0a\u8ff0\u516c\u5f0f\u5176\u5b9e\u5c31\u662fa==b, b==c,\u5219a==c \u516c\u5f0f\u5f88\u65e0\u804a\uff0c\u80fd\u770b\u660e\u767d\u5c31\u884c\u4e86\uff0c\u4e0d\u9700\u8981\u8bb0\u4f4f\u3002 NOTE: \u4f5c\u8005\u8fd9\u91cc\u7684\u603b\u7ed3\u4e0d\u591f\u76f4\u63a5\uff0c\u4e0b\u9762\u662f\u6458\u81ea\u767e\u5ea6\u767e\u79d1 kmp\u7b97\u6cd5 \u4e2d\u5bf9\u8fd9\u4e2a\u7ed3\u8bba\u7684\u603b\u7ed3\uff0c\u5b83\u975e\u5e38\u76f4\u63a5\uff1a \u7528\u66b4\u529b\u7b97\u6cd5\u5339\u914d\u5b57\u7b26\u4e32\u8fc7\u7a0b\u4e2d\uff0c\u6211\u4eec\u4f1a\u628a T[0] \u8ddf W[0] \u5339\u914d\uff0c\u5982\u679c\u76f8\u540c\u5219\u5339\u914d\u4e0b\u4e00\u4e2a\u5b57\u7b26\uff0c\u76f4\u5230\u51fa\u73b0\u4e0d\u76f8\u540c\u7684\u60c5\u51b5\uff0c\u6b64\u65f6\u6211\u4eec\u4f1a\u4e22\u5f03\u524d\u9762\u7684\u5339\u914d\u4fe1\u606f\uff0c\u7136\u540e\u628a T[1] \u8ddf W[0] \u5339\u914d\uff0c\u5faa\u73af\u8fdb\u884c\uff0c\u76f4\u5230\u4e3b\u4e32\u7ed3\u675f\uff0c\u6216\u8005\u51fa\u73b0\u5339\u914d\u6210\u529f\u7684\u60c5\u51b5\u3002\u8fd9\u79cd\u4e22\u5f03\u524d\u9762\u7684\u5339\u914d\u4fe1\u606f\u7684\u65b9\u6cd5\uff0c\u6781\u5927\u5730\u964d\u4f4e\u4e86\u5339\u914d\u6548\u7387\u3002 \u800c\u5728KMP\u7b97\u6cd5\u4e2d\uff0c\u5bf9\u4e8e\u6bcf\u4e00\u4e2a\u6a21\u5f0f\u4e32\u6211\u4eec\u4f1a\u4e8b\u5148\u8ba1\u7b97\u51fa\u6a21\u5f0f\u4e32\u7684\u5185\u90e8\u5339\u914d\u4fe1\u606f\uff0c\u5728\u5339\u914d\u5931\u8d25\u65f6\u6700\u5927\u7684\u79fb\u52a8\u6a21\u5f0f\u4e32\uff0c\u4ee5\u51cf\u5c11\u5339\u914d\u6b21\u6570\u3002 \u6bd4\u5982\uff0c\u5728\u7b80\u5355\u7684\u4e00\u6b21\u5339\u914d\u5931\u8d25\u540e\uff0c\u6211\u4eec\u4f1a\u60f3\u5c06\u6a21\u5f0f\u4e32\u5c3d\u91cf\u7684\u53f3\u79fb\u548c\u4e3b\u4e32\u8fdb\u884c\u5339\u914d\u3002\u53f3\u79fb\u7684\u8ddd\u79bb\u5728KMP\u7b97\u6cd5\u4e2d\u662f\u5982\u6b64\u8ba1\u7b97\u7684\uff1a\u5728 \u5df2\u7ecf\u5339\u914d\u7684\u6a21\u5f0f\u4e32\u5b50\u4e32 \u4e2d\uff0c\u627e\u51fa\u6700\u957f\u7684\u76f8\u540c\u7684 \u524d\u7f00 \u548c \u540e\u7f00 \uff0c\u7136\u540e\u79fb\u52a8\u4f7f\u5b83\u4eec\u91cd\u53e0\u3002 \u8fd9\u4e00\u6bb5\u53ea\u662f\u4e3a\u4e86\u8bc1\u660e\u6211\u4eec\u4e3a\u4ec0\u4e48\u53ef\u4ee5\u76f4\u63a5\u5c06 j \u79fb\u52a8\u5230 k \u800c\u65e0\u987b\u518d\u6bd4\u8f83\u524d\u9762\u7684 k \u4e2a\u5b57\u7b26\u3002","title":"2. \u8be6\u89e3KMP\u7b97\u6cd5"},{"location":"Application/wikipedia-String/string-searching/Knuth\u2013Morris\u2013Pratt-algorithm/Knuth\u2013Morris\u2013Pratt-algorithm/#next","text":"\u597d\uff0c\u63a5\u4e0b\u6765\u5c31\u662f\u91cd\u70b9\u4e86\uff0c\u600e\u4e48\u6c42\u8fd9\u4e2a\uff08\u8fd9\u4e9b\uff09 k \u5462\uff1f\u56e0\u4e3a\u5728 P \u7684\u6bcf\u4e00\u4e2a\u4f4d\u7f6e\u90fd\u53ef\u80fd\u53d1\u751f\u4e0d\u5339\u914d\uff0c\u4e5f\u5c31\u662f\u8bf4\u6211\u4eec\u8981\u8ba1\u7b97\u6bcf\u4e00\u4e2a\u4f4d\u7f6e j \u5bf9\u5e94\u7684 k \uff0c\u6240\u4ee5\u7528\u4e00\u4e2a\u6570\u7ec4 next \u6765\u4fdd\u5b58\uff0c next[j] = k \uff0c\u8868\u793a\u5f53 T[i] != P[j] \u65f6\uff0c j \u6307\u9488 \u7684\u4e0b\u4e00\u4e2a\u4f4d\u7f6e\u3002 \u5f88\u591a\u6559\u6750\u6216\u535a\u6587\u5728\u8fd9\u4e2a\u5730\u65b9\u90fd\u662f\u8bb2\u5f97\u6bd4\u8f83\u542b\u7cca\u6216\u662f\u6839\u672c\u5c31\u4e00\u7b14\u5e26\u8fc7\uff0c\u751a\u81f3\u5c31\u662f\u8d34\u4e00\u6bb5\u4ee3\u7801\u4e0a\u6765\uff0c\u4e3a\u4ec0\u4e48\u662f\u8fd9\u6837\u6c42\uff1f\u600e\u4e48\u53ef\u4ee5\u8fd9\u6837\u6c42\uff1f\u6839\u672c\u5c31\u6ca1\u6709\u8bf4\u6e05\u695a\u3002\u800c\u8fd9\u91cc\u6070\u6070\u662f\u6574\u4e2a\u7b97\u6cd5\u6700\u5173\u952e\u7684\u5730\u65b9\u3002 public static int[] getNext(String ps) { char[] p = ps.toCharArray(); int[] next = new int[p.length]; next[0] = -1; int j = 0; int k = -1; while (j < p.length - 1) { if (k == -1 || p[j] == p[k]) { next[++j] = ++k; } else { k = next[k]; } } return next; } \u8fd9\u4e2a\u7248\u672c\u7684\u6c42 next \u6570\u7ec4\u7684\u7b97\u6cd5\u5e94\u8be5\u662f\u6d41\u4f20\u6700\u5e7f\u6cdb\u7684\uff0c\u4ee3\u7801\u662f\u5f88\u7b80\u6d01\u3002\u53ef\u662f\u771f\u7684\u5f88\u8ba9\u4eba\u6478\u4e0d\u5230\u5934\u8111\uff0c\u5b83\u8fd9\u6837\u8ba1\u7b97\u7684\u4f9d\u636e\u5230\u5e95\u662f\u4ec0\u4e48\uff1f \u597d\uff0c\u5148\u628a\u8fd9\u4e2a\u653e\u4e00\u8fb9\uff0c\u6211\u4eec\u81ea\u5df1\u6765\u63a8\u5bfc\u601d\u8def\uff0c\u73b0\u5728\u8981\u59cb\u7ec8\u8bb0\u4f4f\u4e00\u70b9\uff0c next[j] \u7684\u503c\uff08\u4e5f\u5c31\u662f k \uff09\u8868\u793a\uff0c\u5f53 P[j] != T[i] \u65f6\uff0c j \u6307\u9488\u7684\u4e0b\u4e00\u6b65\u79fb\u52a8\u4f4d\u7f6e\u3002 \u5148\u6765\u770b\u7b2c\u4e00\u4e2a\uff1a\u5f53 j \u4e3a0\u65f6\uff0c\u5982\u679c\u8fd9\u65f6\u5019\u4e0d\u5339\u914d\uff0c\u600e\u4e48\u529e\uff1f \u50cf\u4e0a\u56fe\u8fd9\u79cd\u60c5\u51b5\uff0c j \u5df2\u7ecf\u5728\u6700\u5de6\u8fb9\u4e86\uff0c\u4e0d\u53ef\u80fd\u518d\u79fb\u52a8\u4e86\uff0c\u8fd9\u65f6\u5019\u8981\u5e94\u8be5\u662f i \u6307\u9488\u540e\u79fb\u3002\u6240\u4ee5\u5728\u4ee3\u7801\u4e2d\u624d\u4f1a\u6709 next[0] = -1; \u8fd9\u4e2a\u521d\u59cb\u5316\u3002 NOTE: \u770b\u4e86\u4e0b\u9762\u7684\u5b8c\u6574\u7684\u4ee3\u7801\u5c31\u77e5\u9053\u4e3a\u4ec0\u4e48\u4f7f\u7528 -1 \u6765\u4f5c\u4e3a\u521d\u59cb\u503c\uff0c\u56e0\u4e3a i++ \u548c j++ \u662f\u5728\u76f8\u540c\u7684\u5206\u652f\u4e2d\uff0c j++ \u540e j \u4e3a0\uff0c\u8fd9\u5c31\u4fdd\u8bc1\u4e86\u4eceP\u7684\u7b2c\u4e00\u4e2a\u5143\u7d20\u5f00\u59cb\u5339\u914d\u3002 \u5982\u679c\u662f\u5f53 j \u4e3a1\u7684\u65f6\u5019\u5462\uff1f \u663e\u7136\uff0c j \u6307\u9488\u4e00\u5b9a\u662f\u540e\u79fb\u52300\u4f4d\u7f6e\u7684\u3002\u56e0\u4e3a\u5b83\u524d\u9762\u4e5f\u5c31\u53ea\u6709\u8fd9\u4e00\u4e2a\u4f4d\u7f6e\u4e86\u3002 \u4e0b\u9762\u8fd9\u4e2a\u662f\u6700\u91cd\u8981\u7684\uff0c\u8bf7\u770b\u5982\u4e0b\u56fe\uff1a \u8bf7\u4ed4\u7ec6\u5bf9\u6bd4\u8fd9\u4e24\u4e2a\u56fe\u3002 \u6211\u4eec\u53d1\u73b0\u4e00\u4e2a\u89c4\u5f8b\uff1a","title":"\u6c42\u89e3next\u6570\u7ec4"},{"location":"Application/wikipedia-String/string-searching/Knuth\u2013Morris\u2013Pratt-algorithm/Knuth\u2013Morris\u2013Pratt-algorithm/#pk_pj","text":"\u5f53 P[k] == P[j] \u65f6\uff0c\u6709 next[j+1] == next[j] + 1 \u5176\u5b9e\u8fd9\u4e2a\u662f\u53ef\u4ee5\u8bc1\u660e\u7684\uff1a \u56e0\u4e3a\u5728 P[j] \u4e4b\u524d\u5df2\u7ecf\u6709 P[0 ~ k-1] == p[j-k ~ j-1] \u3002\uff08 next[j] == k \uff09 \u8fd9\u65f6\u5019\u73b0\u6709 P[k] == P[j] \uff0c\u6211\u4eec\u662f\u4e0d\u662f\u53ef\u4ee5\u5f97\u5230 P[0 ~ k-1] + P[k] == p[j-k ~ j-1] + P[j] \u3002 \u5373\uff1a P[0 ~ k] == P[j-k ~ j] \uff0c\u5373 next[j+1] == k + 1 == next[j] + 1 \u3002 \u8fd9\u91cc\u7684\u516c\u5f0f\u4e0d\u662f\u5f88\u597d\u61c2\uff0c\u8fd8\u662f\u770b\u56fe\u4f1a\u5bb9\u6613\u7406\u89e3\u4e9b\u3002","title":"\u5f53P[k] == P[j]\u65f6"},{"location":"Application/wikipedia-String/string-searching/Knuth\u2013Morris\u2013Pratt-algorithm/Knuth\u2013Morris\u2013Pratt-algorithm/#pk_pj_1","text":"\u5f53 P[k] != P[j] \u65f6\uff0c\u5982\u4e0b\u56fe\u6240\u793a\uff1a \u50cf\u8fd9\u79cd\u60c5\u51b5\uff0c\u5982\u679c\u4f60\u4ece\u4ee3\u7801\u4e0a\u770b\u5e94\u8be5\u662f\u8fd9\u4e00\u53e5\uff1a k = next[k]; \u4e3a\u4ec0\u4e48\u662f\u8fd9\u6837\u5b50\uff1f\u4f60\u770b\u4e0b\u9762\u5e94\u8be5\u5c31\u660e\u767d\u4e86\u3002 \u73b0\u5728\u4f60\u5e94\u8be5\u77e5\u9053\u4e3a\u4ec0\u4e48\u8981 k = next[k] \u4e86\u5427\uff01\u50cf\u4e0a\u8fb9\u7684\u4f8b\u5b50\uff0c\u6211\u4eec\u5df2\u7ecf\u4e0d\u53ef\u80fd\u627e\u5230 [ A\uff0cB\uff0cA\uff0cB ] \u8fd9\u4e2a\u6700\u957f\u7684\u540e\u7f00\u4e32\u4e86\uff0c\u4f46\u6211\u4eec\u8fd8\u662f\u53ef\u80fd\u627e\u5230 [ A\uff0cB ] \u3001 [ B ] \u8fd9\u6837\u7684\u524d\u7f00\u4e32\u7684\u3002\u6240\u4ee5\u8fd9\u4e2a\u8fc7\u7a0b\u50cf\u4e0d\u50cf\u5728\u5b9a\u4f4d [ A\uff0cB\uff0cA\uff0cC ] \u8fd9\u4e2a\u4e32\uff0c\u5f53 C \u548c\u4e3b\u4e32\u4e0d\u4e00\u6837\u4e86\uff08\u4e5f\u5c31\u662f k \u4f4d\u7f6e\u4e0d\u4e00\u6837\u4e86\uff09\uff0c\u90a3\u5f53\u7136\u662f\u628a\u6307\u9488\u79fb\u52a8\u5230 next[k] \u5566\u3002 NOTE: \u8fd9\u7bc7\u6587\u7ae0\u8fd9\u91cc\u7684\u5206\u6790\u8fd8\u662f\u6bd4\u8f83\u96be\u4ee5\u7406\u89e3\u7684\uff0c\u4e0b\u4e00\u7bc7\u5728\u5206\u6790\u66f4\u52a0\u900f\u5f7b\u3002 NOTE: \u6784\u5efa next \u6570\u7ec4\u7684\u7b97\u6cd5\u662f\u4f7f\u7528\u7684\u6570\u5b66\u5f52\u7eb3\u6cd5\u6765\u6c42\u89e3next\u6570\u7ec4\u7684\u6bcf\u4e2a\u503c\uff0c\u5373\u6839\u636e next \u6570\u7ec4\u4e2d\u524d j \u4e2a\u5143\u7d20\u7684\u503c\u6765\u6c42\u89e3 next[j+1] \u7684\u503c\u3002 \u6709\u4e86 next \u6570\u7ec4\u4e4b\u540e\u5c31\u4e00\u5207\u597d\u529e\u4e86\uff0c\u6211\u4eec\u53ef\u4ee5\u52a8\u624b\u5199KMP\u7b97\u6cd5\u4e86\uff1a public static int KMP(String ts, String ps) { char[] t = ts.toCharArray(); char[] p = ps.toCharArray(); int i = 0; // \u4e3b\u4e32\u7684\u4f4d\u7f6e int j = 0; // \u6a21\u5f0f\u4e32\u7684\u4f4d\u7f6e int[] next = getNext(ps); while (i < t.length && j < p.length) { if (j == -1 || t[i] == p[j]) { // \u5f53j\u4e3a-1\u65f6\uff0c\u8981\u79fb\u52a8\u7684\u662fi\uff0c\u5f53\u7136j\u4e5f\u8981\u5f520 i++; j++; } else { // i\u4e0d\u9700\u8981\u56de\u6eaf\u4e86 // i = i - j + 1; j = next[j]; // j\u56de\u5230\u6307\u5b9a\u4f4d\u7f6e } } if (j == p.length) { return i - j; } else { return -1; } } \u548c\u66b4\u529b\u7834\u89e3\u76f8\u6bd4\uff0c\u5c31\u6539\u52a8\u4e864\u4e2a\u5730\u65b9\u3002\u5176\u4e2d\u6700\u4e3b\u8981\u7684\u4e00\u70b9\u5c31\u662f\uff0c i \u4e0d\u9700\u8981\u56de\u6eaf\u4e86\u3002 \u6700\u540e\uff0c\u6765\u770b\u4e00\u4e0b\u4e0a\u8fb9\u7684\u7b97\u6cd5\u5b58\u5728\u7684\u7f3a\u9677\u3002\u6765\u770b\u7b2c\u4e00\u4e2a\u4f8b\u5b50\uff1a \u663e\u7136\uff0c\u5f53\u6211\u4eec\u4e0a\u8fb9\u7684\u7b97\u6cd5\u5f97\u5230\u7684 next \u6570\u7ec4\u5e94\u8be5\u662f [ -1\uff0c0\uff0c0\uff0c1 ] \u6240\u4ee5\u4e0b\u4e00\u6b65\u6211\u4eec\u5e94\u8be5\u662f\u628a j \u79fb\u52a8\u5230\u7b2c1\u4e2a\u5143\u7d20\u54af\uff1a \u4e0d\u96be\u53d1\u73b0\uff0c \u8fd9\u4e00\u6b65\u662f\u5b8c\u5168\u6ca1\u6709\u610f\u4e49\u7684\u3002\u56e0\u4e3a\u540e\u9762\u7684 B \u5df2\u7ecf\u4e0d\u5339\u914d\u4e86\uff0c\u90a3\u524d\u9762\u7684 B \u4e5f\u4e00\u5b9a\u662f\u4e0d\u5339\u914d\u7684 \uff0c\u540c\u6837\u7684\u60c5\u51b5\u5176\u5b9e\u8fd8\u53d1\u751f\u5728\u7b2c2\u4e2a\u5143\u7d20 A \u4e0a\u3002 \u663e\u7136\uff0c \u53d1\u751f\u95ee\u9898\u7684\u539f\u56e0\u5728\u4e8e P[j] == P[next[j]] \u3002 \u6240\u4ee5\u6211\u4eec\u4e5f\u53ea\u9700\u8981\u6dfb\u52a0\u4e00\u4e2a\u5224\u65ad\u6761\u4ef6\u5373\u53ef\uff1a public static int[] getNext(String ps) { char[] p = ps.toCharArray(); int[] next = new int[p.length]; next[0] = -1; int j = 0; int k = -1; while (j < p.length - 1) { if (k == -1 || p[j] == p[k]) { if (p[++j] == p[++k]) { // \u5f53\u4e24\u4e2a\u5b57\u7b26\u76f8\u7b49\u65f6\u8981\u8df3\u8fc7 next[j] = next[k]; } else { next[j] = k; } } else { k = next[k]; } } return next; } \u8be5\u7b97\u6cd5\u7684\u5b9e\u73b0\u662f\u975e\u5e38\u7c7b\u4f3c\u4e8e\u52a8\u6001\u89c4\u5212\u7b97\u6cd5\u7684 next[j] \u7684\u503c k \u5c31\u662f j \u4f4d\u4e4b\u524d\u7684\u5b50\u4e32\u4e2d\uff0c\u524d\u7f00\u96c6\u548c\u540e\u7f00\u96c6\u4e2d\u7684\u6700\u5927\u91cd\u590d\u5b50\u4e32\u7684\u957f\u5ea6\u3002 abacabac next[0]=-1;j=0;k=-1 \u6761\u4ef61 \u6761\u4ef62 \u5206\u652f2 \u5206\u652f1 k==-1 next[1]=0;j=1;k=0 p[1]!=p[0]; k=next[0]=-1 k==-1 next[2]=0;j=2;k=0 p[2]==p[0]; next[3]=1;j=3;k=1 p[3]!=p[1]; k=next[1]=0 p[3]!=p[0]; k=next[0]=-1 k==-1; next[4]=0;j=4;k=0 p[4]==p[0] next[5]=1;j=5;k=1 p[5]==p[1] next[6]=2;j=6;k=2 p[6]==p[2] next[7]=3;j=7;k=3 p[7]==p[3] next[8]=4;j=8;k=4 \u8981\u60f3\u5f97\u5230 p[j+1] \uff0c\u53ea\u9700\u8981\u6bd4\u8f83 p[j] \u548c p[k] \u5373\u53ef\uff1b","title":"\u5f53P[k] != P[j]\u65f6,"},{"location":"Application/wikipedia-String/string-searching/Knuth\u2013Morris\u2013Pratt-algorithm/Knuth\u2013Morris\u2013Pratt-algorithm/#3_computing_the_kmp_failure_function_fk","text":"","title":"3. Computing the KMP failure function (f(k))"},{"location":"Application/wikipedia-String/string-searching/Knuth\u2013Morris\u2013Pratt-algorithm/Knuth\u2013Morris\u2013Pratt-algorithm/#definition_of_fk","text":"f(k) = MaxOverlap ( \"p0 p1 ... pk\" ) where: \"p0 p1 ... pk\" = the prefix of length k+1 of pattern P Graphically:","title":"definition of f(k)"},{"location":"Application/wikipedia-String/string-searching/Knuth\u2013Morris\u2013Pratt-algorithm/Knuth\u2013Morris\u2013Pratt-algorithm/#naive_way_to_find_fk","text":"Given P = \"p0 p1 ... pm-1\" Given k = 1, 2, ..., m-1 (k = 0 ==> f(0) = 0) 1. Extract the sub-pattern: \"p0 p1 ... pk\" 2. Find the first (= largest) overlap: Try: (p0) p1 p2 ... pk-1 p0 p1 ... pk-1 pk If (no match) Try: (p0) p1 p2 ... pk-1 p0 p1 ... pk-1 pk And so on... The first overlap is the longest ! NOTE: \u4e0a\u8ff0\u7b97\u6cd5\u662f\u4e00\u4e2a\u5faa\u73af\u7b97\u6cd5\uff0c\u5373 for k in range(1, m) \uff0c\u4e0b\u9762\u662f\u4e0a\u8ff0\u7b97\u6cd5\u7684python\u5b9e\u73b0\uff1a python def build_failure_table(p): \"\"\" \u6784\u5efa\u5b57\u7b26\u4e32p\u7684\u6700\u957f\u516c\u5171\u524d\u7f00\u540e\u7f00\u6570\u7ec4 :param p: :return: \"\"\" failure_table = list() len_of_p = len(p) for len_of_sub_str in range(1, len_of_p + 1): max_len_of_overlap = int(len_of_sub_str / 2) # \u6700\u5927\u91cd\u53e0\u524d\u7f00\u540e\u7f00\u7684\u957f\u5ea6 print(\"\u5b50\u4e32\u957f\u5ea6:{},\u6700\u5927\u91cd\u53e0\u524d\u7f00\u540e\u7f00\u957f\u5ea6:{}\".format(len_of_sub_str, max_len_of_overlap)) if max_len_of_overlap == 0: # \u957f\u5ea6\u4e3a1\u7684\u4e32\uff0c\u662f\u6ca1\u6709\u91cd\u53e0\u524d\u7f00\u540e\u7f00\u7684 failure_table.append(0) else: found = False # \u662f\u5426\u627e\u5230\u91cd\u53e0\u524d\u7f00\u540e\u7f00 for len_of_overlap in range(max_len_of_overlap, 0, -1): print(\"\u91cd\u53e0\u524d\u7f00\u540e\u7f00\u957f\u5ea6:{}\".format(len_of_overlap)) # len_of_overlap \u91cd\u53e0\u524d\u7f00\u540e\u7f00\u7684\u957f\u5ea6 for prefix_index in range(len_of_overlap): suffix_index = prefix_index + (len_of_sub_str - len_of_overlap) print(\"\u524d\u7f00\u8d77\u59cb\u4f4d\u7f6e:{},\u540e\u7f00\u8d77\u59cb\u4f4d\u7f6e:{}\".format(prefix_index, suffix_index)) if p[prefix_index] == p[suffix_index]: if suffix_index == len_of_sub_str - 1: # \u627e\u5230\u4e86\u91cd\u53e0\u90e8\u5206 failure_table.append(len_of_overlap) found = True break else: break if found: break if not found: failure_table.append(0) return failure_table","title":"Naive way to find f(k):"},{"location":"Application/wikipedia-String/string-searching/Knuth\u2013Morris\u2013Pratt-algorithm/Knuth\u2013Morris\u2013Pratt-algorithm/#relating_fk_to_fk1","text":"The values f(k) are computed easily using existing prefix overlap information : f(0) = 0 ( f(0) is always 0) f(1) is computing using (already computed) value f(0) f(2) is computing using (already computed) value f(0) , f(1) f(3) is computing using (already computed) value f(0) , f(1) , f(2) And so on According to the definition of f(k) : NOTE: \u4e0a\u9762\u8fd9\u79cd\u8868\u793a\u95ee\u9898\u7684\u65b9\u5f0f\u662f\u6bd4\u8f83\u5bb9\u6613\u7406\u89e3\u7684\uff0c\u5373\u5728\u539f\u95ee\u9898\u7684\u57fa\u7840\u4e0a\u6dfb\u52a0\u4e00\u4e2a\u65b0\u5143\u7d20\u4ece\u800c\u6784\u6210\u4e86\u4e00\u4e2a\u89c4\u6a21\u66f4\u5927\u7684\u95ee\u9898\u3002 Suppose that we know that: f(k\u22121) = x In other words: the longest overlapping suffix and prefix in \" p0 p1 ... pk-1 \" has x characters: f(k-1) = x characters <-----------------------> p1 p2 p3 ... pk-x-2 pk-x-3 pk-x-4 .... pk-1 ^ ^ ^ ^ | | | equal | v v v v p0 p1 p2 .... px-1 px ... pk-1 question: Can we use the fact that f(k\u22121) = x to compute f(k) ? answer: Yes, because f(k) is computed using a similar prefix as f(k\u22121): prefix used to compute f(k-1) +--------------------------------+ | | p0 p1 p2 .... px-1 ... pk-1 pk | | +------------------------------------+ prefix used to compute f(k) We will next learn how to exploit the similarity to compute f(k)","title":"Relating f(k) to f(k\u22121)"},{"location":"Application/wikipedia-String/string-searching/Knuth\u2013Morris\u2013Pratt-algorithm/Knuth\u2013Morris\u2013Pratt-algorithm/#fact_between_fk_and_fk1","text":"Fact: f(k) \u2264 f(k\u22121) + 1","title":"Fact between f(k) and f(k\u22121)"},{"location":"Application/wikipedia-String/string-searching/Knuth\u2013Morris\u2013Pratt-algorithm/Knuth\u2013Morris\u2013Pratt-algorithm/#computation_trick_1","text":"Let use denote: f(k\u22121) = x (Note: f(k\u22121) is equal to some value. The above assumption simply gave a more convenient notation for this value). If px == pk , then: f(k) = x+1 (i.e., the maximum overlap of the prefix p0 p1 p2 .... pk-1 pk has x+1 characters Proof: These x+1 characters match IF pk == px! <----------------------------> p1 p2 p3 ... pk-x-2 pk-x-3 pk-x-4 .... pk-1 pk ^ ^ ^ ^ ^ | | | equal | |equal v v v v v p0 p1 p2 .... px-1 px ... pk-1 pk | | +--------------------------+ These characters matches because f(k-1) = x","title":"Computation trick 1"},{"location":"Application/wikipedia-String/string-searching/Knuth\u2013Morris\u2013Pratt-algorithm/Knuth\u2013Morris\u2013Pratt-algorithm/#prelude_to_computation_trick_2","text":"Consider the prefix ababyabab where f(8) = 4: 012345678 prefix = ababyabab f(8) = 4 because: ababyabab ababyabab <--> 4 characters overlap We want to compute f(9) using f(8) , but now the next character does not match(that is the next char is not equal to y): 0123456789 prefix = ababyababa ababyababa ababyababa Conclusion: *** We CANNOT use f(8) to compute f(9) *** question: What should we try next to find the maximum overlap for the prefix \"ababyababa\" answer: To find the maximum overlap, we must slide the prefix down and look for matching letters !!! NOTE: \u601d\u8def\u662f\u4f7f\u7528\u5df2\u7ecf\u5339\u914d\u7684\u5b57\u7b26\u4e32\u6765\u5c3d\u53ef\u80fd\u51cf\u5c11\u5339\u914d\u6b21\u6570\u5e76\u4e14\u5bfb\u627e\u7b2c\u4e00\u4e2a\u6700\u53ef\u80fd\u7684\u4f4d\u7f6e\u3002 Now, let us use only the matching prefix information: ababyababa ababyababa Look only at these characters: ?????abab? abab?????? We can know for sure that the overlap cannot be found starting at these positions: ?????abab? abab?????? NOTE: \u56e0\u4e3a\u6211\u4eec\u77e5\u9053\u4e32 abab \u7684\u6700\u957f\u516c\u5171\u524d\u7f00\u540e\u7f00\u7684\u957f\u5ea6\u662f2\uff0c\u5373 f(3) \uff0c\u6240\u4ee5\u5b83\u7684\u524d\u4e24\u4e2a\u5143\u7d20\u53ef\u4ee5\u5339\u914d\u4e0a\u7684\uff0c\u6240\u4ee5\u7b2c\u4e00\u4e2a\u53ef\u80fd\u4f4d\u7f6e\u662f\u5982\u4e0b\u56fe\u6240\u793a\u7684\uff0c\u8fd9\u5c31\u662f\u5bf9\u5df2\u7ecf\u5339\u914d\u4fe1\u606f\u7684\u5145\u5206\u8fd0\u7528\u3002\u81f3\u4e8e\u7b2c\u4e09\u4e2a\u5143\u7d20\u662f\u5426\u80fd\u591f\u5339\u914d\u4e0a\uff0c\u5c31\u8981\u6bd4\u8f83\u7684\u7ed3\u679c\u4e86\u3002 The first possible way that overlap can be found is starting here: ?????abab? abab?????? In other words: we can compute f(9) using f(3) : 0123 prefix = abab abab abab f(3) = 2 Notice that: 3 = 4\u22121 and f(8) = 4 Worked out further: 0123456789 prefix = ababyababa ababyababa ababyababa ^ | compare the character at position 2 (f(3) = 2) Note: The prefix abab is hightlighted in yellow Because the characters are equal, we have found the maximum overlap: f(9) = f(3) + 1 = 2 + 1 = 3 !!! NOTE: \u8fd9\u91cc\u53ef\u4ee5\u5047\u8bbe\uff0c\u5982\u679c p[3] \u548c p[9] \u5e76\u4e0d\u76f8\u7b49\uff0c\u5219\u4e0a\u8ff0\u6d41\u7a0b\u9700\u8981\u7ee7\u7eed\u4e0b\u53bb\uff0c\u81f3\u4e8e\u7ec8\u6b62\u6761\u4ef6\uff0c\u663e\u7136\u662f\u76f4\u81f3\u6bd4\u8f83\u5230\u7b2c\u4e00\u4e2a\u5143\u7d20\u90fd\u4e0d\u76f8\u7b49\u3002","title":"Prelude to computation trick 2"},{"location":"Application/wikipedia-String/string-searching/Knuth\u2013Morris\u2013Pratt-algorithm/Knuth\u2013Morris\u2013Pratt-algorithm/#computation_trick_2","text":"Let: f(k\u22121) = x (Note: f(k\u22121) is equal to some value. The above assumption simply gave a more convenient notation for this value). If px \u2260 pk , then: The next prefix that can be used to compute f(k) is: p0 p1 .... px-1 In pseudo code: i = k-1; // Try to use f(k-1) to compute f(k) x = f(i); // x = character position to match against pk if ( P[k] == P[x] ) then f(k) = f(x\u22121) + 1 else Use: p0 p1 .... px-1 to compute f(k) What that means in terms of program statements: i = x-1; // Try to use f(x-1) to compute f(k) x = f(i); // x = character position to match against pk Note: We must repeat trick #2 as long as i \u2265 0, In other words: use a while loop instead of an if statement !","title":"Computation trick #2"},{"location":"Application/wikipedia-String/string-searching/Knuth\u2013Morris\u2013Pratt-algorithm/Knuth\u2013Morris\u2013Pratt-algorithm/#algorithm_to_compute_kmp_failure_function","text":"Java code: public static int[] KMP_failure_function(String P) { int k, i, x, m; int f[] = new int[P.length()]; m = P.length(); f[0] = 0; // f(0) is always 0 for ( k = 1; k < m; k++ ) { // Compute f[k] i = k-1; // First try to use f(k-1) to compute f(k) x = f[i]; while ( P.charAt(x) != P.charAt(k) ) { i = x-1; // Try the next candidate f(.) to compute f(k) if ( i < 0 ) // Make sure x is valid break; // STOP the search !!! x = f[i]; } if ( i < 0 ) f[k] = 0; // No overlap at all: max overlap = 0 characters else f[k] = f[i] + 1; // We can compute f(k) using f(i) } return(f); } \u5b8c\u6574\u6d4b\u8bd5\u7a0b\u5e8f /* ---------------------------------- My own KMP Failure function alg S.Y. Cheung - 3/3/2013 ---------------------------------- */ import java.util.*; public class ComputeF { public static int[] KMP_failure_function(String P) { int k, i, x, m; int f[] = new int[P.length()]; String s; m = P.length(); f[0] = 0; for ( k = 1; k < m; k++ ) { // Compute f[k] s = P.substring(0,k+1); System.out.println(\"-----------------------------------------------\"); System.out.println(\"Prefix = \" + s + \" --- Computing f(\"+k+\"):\"); i = k-1; // First try to use f(k-1) to compute f(k) x = f[i]; System.out.println(\"===================================\"); System.out.println(\"Try using: f(\" + i + \") = \" + x ); printState(s, s, k, x); while ( P.charAt(x) != P.charAt(k) ) { i = f[i]-1; // Try the next candidate f(.) to compute f(k) if ( i < 0 ) // Search ended in failure.... break; x = f[i]; System.out.println(\"===================================\"); System.out.println(\"Try using: f(\" + i + \") = \" + x ); printState(s, s, k, x); } if ( i < 0 ) { System.out.println(\"No overlap possible... --> f[\"+k+\"] = 0\"); f[k] = 0; // No overlap possible } else { f[k] = f[i] + 1; // Compute f(k) using f(i) System.out.println(\"Overlap found ... --> f[\"+k+\"] = \"+f[k]); } } return(f); } public static void main(String[] args) { String P; Scanner in; int[] f; in = new Scanner( System.in ); System.out.print(\"P = \"); P = in.nextLine(); System.out.println(); f = KMP_failure_function(P); for (int i = 0; i < P.length(); i++ ) { System.out.println(\"f(\"+i+\") = \" + f[i]); } System.out.println(); } /* ===================================================== Variables and Methods to make the algorithm visual ===================================================== */ public static String T_ruler, P_ruler; public static String ruler(int n) { String out = \"\"; char x = '0'; for ( int i = 0; i < n; i++ ) { out = out + x; x++; if ( x > '9' ) x = '0'; } return out; } public static void printState(String T, String P, int i, int j) { T_ruler = ruler( T.length() ); P_ruler = ruler( P.length() ); System.out.println(\"=====================================\"); System.out.println(\"Matching: i = \" + i + \", j = \" + j); System.out.println(\" \" + T_ruler ); System.out.println(\" \" + T); System.out.print(\" \"); for ( int k = 0; k < i-j; k++) System.out.print(\" \"); System.out.println(P); System.out.print(\" \"); for ( int k = 0; k < i-j; k++) System.out.print(\" \"); System.out.println( P_ruler ); System.out.print(\" \"); for ( int k = 0; k < i; k++) System.out.print(\" \"); System.out.println(\"^\"); System.out.print(\" \"); for ( int k = 0; k < i; k++) System.out.print(\" \"); System.out.println(\"|\"); System.out.println(); } }","title":"Algorithm to compute KMP failure function"},{"location":"Application/wikipedia-String/string-searching/Knuth\u2013Morris\u2013Pratt-algorithm/Knuth\u2013Morris\u2013Pratt-algorithm/#kmp","text":"\u901a\u8fc7\u4e0a\u8ff0\u4e09\u7bc7\u6587\u7ae0\uff0c\u80fd\u591f\u77e5\u9053KMP\u7b97\u6cd5\u7684\u539f\u7406\uff0c\u73b0\u5728\u9700\u8981\u8003\u8651\u7684\u662f\u5982\u4f55\u6765\u8fdb\u884c\u5b9e\u73b0\u3002","title":"KMP\u5b9e\u73b0\u5206\u6790"},{"location":"Application/wikipedia-String/string-searching/Knuth\u2013Morris\u2013Pratt-algorithm/Knuth\u2013Morris\u2013Pratt-algorithm/#kmp_failure_function","text":"\u5f53 pattern[j] \u4e0e pattern[f[j-1]] \u4e0d\u76f8\u7b49\u7684\u65f6\u5019\uff0c\u8fd9\u4e2a\u9012\u5f52\u516c\u5f0f\u4e2d\u6d89\u53ca\u5230\u4e86\u4e0d\u65ad\u5730\u5faa\u73af\u9012\u5f52\uff0c\u4f7f\u7528\u6570\u5b66\u516c\u5f0f\u4e0d\u65b9\u4fbf\u63cf\u8ff0\uff0c\u4e0b\u9762\u7684python\u7a0b\u5e8f\u662f\u975e\u5e38\u7b80\u6d01\u6613\u61c2\u7684\uff0c\u5e76\u4e14\u662f\u975e\u5e38\u63a5\u8fd1\u6570\u5b66\u516c\u5f0f\u7684\uff0c\u6240\u4ee5\u8fd9\u91cc\u5c31\u7701\u7565\u6389\u9012\u5f52\u516c\u5f0f\u3002","title":"\u8ba1\u7b97KMP failure function\u7684\u9012\u5f52\u516c\u5f0f"},{"location":"Application/wikipedia-String/string-searching/Knuth\u2013Morris\u2013Pratt-algorithm/Knuth\u2013Morris\u2013Pratt-algorithm/#kmp_failure_functionpython","text":"failure function f(j) \u8868\u793a\u7684\u662f\u4ece pattern[0] \u5230 pattern[j] \u7684\u5e8f\u5217\uff08\u663e\u7136\u8fd9\u4e2a\u5e8f\u5217\u7684\u957f\u5ea6\u662f j+1 \uff09\u7684\u6700\u957f\u516c\u5171\u524d\u7f00\u540e\u7f00\u7684 \u957f\u5ea6 \uff0c\u5373 f(j) \u6240\u8868\u793a\u7684\u662f\u957f\u5ea6\u4e3a j+1 \u7684\u5e8f\u5217\u7684\u6700\u957f\u516c\u5171\u524d\u7f00\u540e\u7f00\u7684\u957f\u5ea6\u3002\u663e\u7136 f[0]==0 \uff0c\u56e0\u4e3a\u957f\u5ea6\u4e3a1\u7684\u5e8f\u5217\u7684\u6700\u957f\u524d\u7f00\u540e\u7f00\u7684\u957f\u5ea6\u4e3a0\u3002\u6240\u4ee5\uff0c\u5f53\u5df2\u77e5\u5e8f\u5217\u7684\u957f\u5ea6\u4e3a i \uff0c\u6765\u67e5\u8be2\u5176\u6700\u957f\u516c\u5171\u524d\u7f00\u540e\u7f00\u7684\u65f6\u5019\uff0c\u4f7f\u7528\u7684\u662f f(i-1) \u3002\u56e0\u4e3a i \u8868\u793a\u7684\u662f\u957f\u5ea6\uff0c\u6240\u4ee5 pattern[i] \u5f15\u7528\u7684\u662f\u6570\u7ec4\u7684\u7b2c i+1 \u4e2a\u5143\u7d20\u3002 def get_failure_array(pattern): failure = [0] # \u521d\u59cb\u6761\u4ef6 i = 0 # f(j-1)\u7684\u503c\uff0c\u662f\u5df2\u77e5\u7684\uff0c\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u5b83\u7684\u542b\u4e49\u662f\u957f\u5ea6 j = 1 # f(j)\u662f\u672a\u77e5\u7684\uff0cj\u8868\u793a\u7684\u662findex while j < len(pattern): if pattern[i] == pattern[j]: i += 1 elif i > 0: i = failure[i - 1] continue j += 1 failure.append(i) return failure","title":"\u8ba1\u7b97KMP failure function\u7684python\u5b9e\u73b0"},{"location":"Application/wikipedia-String/string-searching/Knuth\u2013Morris\u2013Pratt-algorithm/Knuth\u2013Morris\u2013Pratt-algorithm/#kmp_failure_function_dynamic_programming","text":"KMP\u7684failure function\u7684\u6c42\u89e3\u8fc7\u7a0b\u5728\u8ba1\u7b97 f(k+1) \u7684\u65f6\u6240\u4f9d\u8d56\u7684 f(0),f(1)...,f(k) \u90fd\u662f\u901a\u8fc7\u67e5failure table\u800c\u83b7\u5f97\u7684\uff0c\u800c\u4e0d\u662f\u91cd\u65b0\u8ba1\u7b97\uff0c\u8fd9\u5176\u5b9e\u5c31\u662f\u52a8\u6001\u89c4\u5212\u7b97\u6cd5\u7684\u601d\u60f3\u3002\u5728\u4e0a\u8ff0\u4ee3\u7801\u4e2d\uff0c i \u5c31\u8868\u793a\u8ba1\u7b97 f(k+1) \u6240\u4f9d\u8d56\u7684\u6570\u636e\uff0c\u5b83\u7684\u5b9e\u73b0\u65b9\u5f0f\u662f\u975e\u5e38\u7c7b\u4f3c\u4e8e\u8fed\u4ee3\u7248\u7684\u6590\u6ce2\u90a3\u5951\u6570\u5217\u3002","title":"\u8ba1\u7b97KMP failure function \u548c dynamic programming"},{"location":"Application/wikipedia-String/string-searching/Knuth\u2013Morris\u2013Pratt-algorithm/Knuth\u2013Morris\u2013Pratt-algorithm/#kmp_1","text":"def kmp_search(pattern, text): \"\"\" :param pattern: :param text: :return: \"\"\" # 1) Construct the failure array failure = get_failure_array(pattern) # 2) Step through text searching for pattern i, j = 0, 0 # index into text, pattern while i < len(text): if pattern[j] == text[i]: if j == (len(pattern) - 1): return True j += 1 elif j > 0: # if this is a prefix in our pattern # just go back far enough to continue j = failure[j - 1] continue i += 1 return False \u601d\u8003\uff1a\u4e3a\u4ec0\u4e48 j = failure[j - 1] \uff1f\u5176\u5b9e\u7ed3\u5408\u524d\u9762\u7684\u4f8b\u5b50\u5c31\u53ef\u4ee5\u77e5\u9053\u4e86\uff0c\u8fd9\u91cc\u4e0d\u518d\u8d58\u8ff0\u3002","title":"KMP\u7684\u5b9e\u73b0"},{"location":"Application/wikipedia-String/string-searching/Knuth\u2013Morris\u2013Pratt-algorithm/Knuth\u2013Morris\u2013Pratt-algorithm/#kmp_in_leetcode","text":"http://www.voidcn.com/article/p-uuefgkai-bnw.html https://leetcode-cn.com/problems/implement-strstr/comments/ https://leetcode.com/problems/shortest-palindrome/discuss/60113/clean-kmp-solution-with-super-detailed-explanation","title":"KMP in leetcode"},{"location":"Application/wikipedia-String/string-searching/Knuth\u2013Morris\u2013Pratt-algorithm/geeksforgeeks-KMP-Algorithm-for-Pattern-Searching/","text":"KMP Algorithm for Pattern Searching KMP (Knuth Morris Pratt) Pattern Searching Preprocessing Overview: Searching Algorithm: KMP Algorithm for Pattern Searching # KMP (Knuth Morris Pratt) Pattern Searching # The Naive pattern searching algorithm doesn\u2019t work well in cases where we see many matching characters followed by a mismatching character. Following are some examples. txt[] = \"AAAAAAAAAAAAAAAAAB\" pat[] = \"AAAAB\" txt[] = \"ABABABCABABABCABABABC\" pat[] = \"ABABAC\" (not a worst case, but a bad case for Naive) The KMP matching algorithm uses degenerating property (pattern having same sub-patterns appearing more than once in the pattern) of the pattern and improves the worst case complexity to O(n) . The basic idea behind KMP\u2019s algorithm is: whenever we detect a mismatch (after some matches), we already know some of the characters in the text of the next window. We take advantage of this information to avoid matching the characters that we know will anyway match. Let us consider below example to understand this. Matching Overview txt = \"AAAAABAAABA\" pat = \"AAAA\" We compare first window of txt with pat txt = \"AAAAABAAABA\" pat = \"AAAA\" [Initial position] We find a match. This is same as Naive String Matching. In the next step, we compare next window of txt with pat. txt = \"AAAAABAAABA\" pat = \"AAAA\" [Pattern shifted one position] This is where KMP does optimization over Naive. In this second window, we only compare fourth A of pattern with fourth character of current window of text to decide whether current window matches or not. Since we know first three characters will anyway match, we skipped matching first three characters. Need of Preprocessing? An important question arises from the above explanation, how to know how many characters to be skipped. To know this, we pre-process pattern and prepare an integer array lps[] that tells us the count of characters to be skipped. Preprocessing Overview: # KMP algorithm preprocesses pat[] and constructs an auxiliary lps[] of size m (same as size of pattern) which is used to skip characters while matching. name lps indicates longest proper prefix which is also suffix. . A proper prefix is prefix with whole string not allowed. For example, prefixes of \u201cABC\u201d are \u201c\u201d, \u201cA\u201d, \u201cAB\u201d and \u201cABC\u201d. Proper prefixes are \u201c\u201d, \u201cA\u201d and \u201cAB\u201d. Suffixes of the string are \u201c\u201d, \u201cC\u201d, \u201cBC\u201d and \u201cABC\u201d. We search for lps in sub-patterns. More clearly we focus on sub-strings of patterns that are either prefix and suffix. For each sub-pattern pat[0..i] where i = 0 to m-1 , lps[i] stores length of the maximum matching proper prefix which is also a suffix of the sub-pattern pat[0..i] . lps[i] = the longest proper prefix of pat[0..i] which is also a suffix of pat[0..i]. Note : lps[i] could also be defined as longest prefix which is also proper suffix. We need to use properly at one place to make sure that the whole substring is not considered. Searching Algorithm: # Unlike Naive algorithm , where we slide the pattern by one and compare all characters at each shift, we use a value from lps[] to decide the next characters to be matched. The idea is to not match a character that we know will anyway match. How to use lps[] to decide next positions (or to know a number of characters to be skipped)? We start comparison of pat[j] with j = 0 with characters of current window of text. We keep matching characters txt[i] and pat[j] and keep incrementing i and j while pat[j] and txt[i] keep matching . When we see a mismatch We know that characters pat[0..j-1] match with txt[i-j\u2026i-1] (Note that j starts with 0 and increment it only when there is a match). We also know (from above definition) that lps[j-1] is count of characters of pat[0\u2026j-1] that are both proper prefix and suffix. From above two points, we can conclude that we do not need to match these lps[j-1] characters with txt[i-j\u2026i-1] because we know that these characters will anyway match. Let us consider above example to understand this.","title":"geeksforgeeks KMP Algorithm for Pattern Searching"},{"location":"Application/wikipedia-String/string-searching/Knuth\u2013Morris\u2013Pratt-algorithm/geeksforgeeks-KMP-Algorithm-for-Pattern-Searching/#kmp_algorithm_for_pattern_searching","text":"","title":"KMP Algorithm for Pattern Searching"},{"location":"Application/wikipedia-String/string-searching/Knuth\u2013Morris\u2013Pratt-algorithm/geeksforgeeks-KMP-Algorithm-for-Pattern-Searching/#kmp_knuth_morris_pratt_pattern_searching","text":"The Naive pattern searching algorithm doesn\u2019t work well in cases where we see many matching characters followed by a mismatching character. Following are some examples. txt[] = \"AAAAAAAAAAAAAAAAAB\" pat[] = \"AAAAB\" txt[] = \"ABABABCABABABCABABABC\" pat[] = \"ABABAC\" (not a worst case, but a bad case for Naive) The KMP matching algorithm uses degenerating property (pattern having same sub-patterns appearing more than once in the pattern) of the pattern and improves the worst case complexity to O(n) . The basic idea behind KMP\u2019s algorithm is: whenever we detect a mismatch (after some matches), we already know some of the characters in the text of the next window. We take advantage of this information to avoid matching the characters that we know will anyway match. Let us consider below example to understand this. Matching Overview txt = \"AAAAABAAABA\" pat = \"AAAA\" We compare first window of txt with pat txt = \"AAAAABAAABA\" pat = \"AAAA\" [Initial position] We find a match. This is same as Naive String Matching. In the next step, we compare next window of txt with pat. txt = \"AAAAABAAABA\" pat = \"AAAA\" [Pattern shifted one position] This is where KMP does optimization over Naive. In this second window, we only compare fourth A of pattern with fourth character of current window of text to decide whether current window matches or not. Since we know first three characters will anyway match, we skipped matching first three characters. Need of Preprocessing? An important question arises from the above explanation, how to know how many characters to be skipped. To know this, we pre-process pattern and prepare an integer array lps[] that tells us the count of characters to be skipped.","title":"KMP (Knuth Morris Pratt) Pattern Searching"},{"location":"Application/wikipedia-String/string-searching/Knuth\u2013Morris\u2013Pratt-algorithm/geeksforgeeks-KMP-Algorithm-for-Pattern-Searching/#preprocessing_overview","text":"KMP algorithm preprocesses pat[] and constructs an auxiliary lps[] of size m (same as size of pattern) which is used to skip characters while matching. name lps indicates longest proper prefix which is also suffix. . A proper prefix is prefix with whole string not allowed. For example, prefixes of \u201cABC\u201d are \u201c\u201d, \u201cA\u201d, \u201cAB\u201d and \u201cABC\u201d. Proper prefixes are \u201c\u201d, \u201cA\u201d and \u201cAB\u201d. Suffixes of the string are \u201c\u201d, \u201cC\u201d, \u201cBC\u201d and \u201cABC\u201d. We search for lps in sub-patterns. More clearly we focus on sub-strings of patterns that are either prefix and suffix. For each sub-pattern pat[0..i] where i = 0 to m-1 , lps[i] stores length of the maximum matching proper prefix which is also a suffix of the sub-pattern pat[0..i] . lps[i] = the longest proper prefix of pat[0..i] which is also a suffix of pat[0..i]. Note : lps[i] could also be defined as longest prefix which is also proper suffix. We need to use properly at one place to make sure that the whole substring is not considered.","title":"Preprocessing Overview:"},{"location":"Application/wikipedia-String/string-searching/Knuth\u2013Morris\u2013Pratt-algorithm/geeksforgeeks-KMP-Algorithm-for-Pattern-Searching/#searching_algorithm","text":"Unlike Naive algorithm , where we slide the pattern by one and compare all characters at each shift, we use a value from lps[] to decide the next characters to be matched. The idea is to not match a character that we know will anyway match. How to use lps[] to decide next positions (or to know a number of characters to be skipped)? We start comparison of pat[j] with j = 0 with characters of current window of text. We keep matching characters txt[i] and pat[j] and keep incrementing i and j while pat[j] and txt[i] keep matching . When we see a mismatch We know that characters pat[0..j-1] match with txt[i-j\u2026i-1] (Note that j starts with 0 and increment it only when there is a match). We also know (from above definition) that lps[j-1] is count of characters of pat[0\u2026j-1] that are both proper prefix and suffix. From above two points, we can conclude that we do not need to match these lps[j-1] characters with txt[i-j\u2026i-1] because we know that these characters will anyway match. Let us consider above example to understand this.","title":"Searching Algorithm:"},{"location":"Application/wikipedia-String/string-searching/Rabin\u2013Karp-algorithm/Rabin\u2013Karp-algorithm/","text":"Rabin\u2013Karp algorithm # NOTE: \u8fd9\u4e2a\u7b97\u6cd5\u5c55\u793a\u4e86\u8fdb\u884c\u5b57\u7b26\u4e32\u6bd4\u8f83\u7684\u53e6\u5916\u4e00\u79cd\u601d\u8def\uff1a hashing In computer science , the Rabin\u2013Karp algorithm or Karp\u2013Rabin algorithm is a string-searching algorithm created by Richard M. Karp and Michael O. Rabin ( 1987 ) that uses hashing to find an exact match of a pattern string in a text. It uses a rolling hash to quickly filter out positions of the text that cannot match the pattern, and then checks for a match at the remaining positions. Generalizations of the same idea can be used to find more than one match of a single pattern, or to find matches for more than one pattern. A practical application of the algorithm is detecting plagiarism . Given source material, the algorithm can rapidly search through a paper for instances of sentences from the source material, ignoring details such as case and punctuation. Because of the abundance of the sought strings, single-string searching algorithms are impractical. NOTE: Detecting plagiarism \u7684\u4e00\u4e2a\u7279\u70b9\u5c31\u662f\u5b83\u5e76\u4e0d\u8981\u6c42\u4e00\u5b57\u4e0d\u5dee\u7684\u7cbe\u51c6\u5339\u914d\u3002 Overview # Several string-matching algorithms, including the Knuth\u2013Morris\u2013Pratt algorithm and the Boyer\u2013Moore string-search algorithm , reduce the worst-case time for string matching by extracting more information from each mismatch, allowing them to skip over positions of the text that are guaranteed not to match the pattern. The Rabin\u2013Karp algorithm instead achieves its speedup by using a hash function to quickly perform an approximate check for each position, and then only performing an exact comparison at the positions that pass this approximate check. A hash function is a function which converts every string into a numeric value, called its hash value ; for example, we might have hash(\"hello\")=5 . If two strings are equal, their hash values are also equal. For a well-designed hash function, the opposite is true, in an approximate sense: strings that are unequal are very unlikely to have equal hash values. The Rabin\u2013Karp algorithm proceeds by computing, at each position of the text, the hash value of a string starting at that position with the same length as the pattern. If this hash value equals the hash value of the pattern, it performs a full comparison at that position. The algorithm # The algorithm is as shown: 1function RabinKarp(string s[1..n], string pattern[1..m]) 2 hpattern := hash(pattern[1..m]); 3 for i from 1 to n-m+1 4 hs := hash(s[i..i+m-1]) 5 if hs = hpattern 6 if s[i..i+m-1] = pattern[1..m] 7 return i 8 return not found Lines 2, 4, and 6 each require O ( m ) time. However, line 2 is only executed once, and line 6 is only executed if the hash values match, which is unlikely to happen more than a few times. Line 5 is executed O( n ) times, but each comparison only requires constant time, so its impact is O( n ). The issue is line 4. Naively computing the hash value for the substring s[i+1..i+m] requires O ( m ) time because each character is examined. Since the hash computation is done on each loop, the algorithm with a na\u00efve hash computation requires O (mn) time, the same complexity as a straightforward string matching algorithms. For speed, the hash must be computed in constant time. The trick is the variable hs already contains the previous hash value of s[i..i+m-1] . If that value can be used to compute the next hash value in constant time, then computing successive hash values will be fast. The trick can be exploited using a rolling hash . A rolling hash is a hash function specially designed to enable this operation. A trivial (but not very good) rolling hash function just adds the values of each character in the substring. This rolling hash formula can compute the next hash value from the previous value in constant time: s[i+1..i+m] = s[i..i+m-1] - s[i] + s[i+m] This simple function works, but will result in statement 5 being executed more often than other more sophisticated rolling hash functions such as those discussed in the next section. NOTE: \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u7684\u610f\u601d\u662f\u4e0a\u9762\u5c55\u793a\u7684rolling hash function\u662f\u4e0d\u591f\u597d\u7684\uff0c\u5728\u4e0b\u4e00\u8282\u4e2d\u4f1a\u4ecb\u7ecd\u66f4\u597d\u7684\u540c\u65f6\u4e5f\u66f4\u52a0\u590d\u6742\u7684hash function Good performance requires a good hashing function for the encountered data. If the hashing is poor (such as producing the same hash value for every input), then line 6 would be executed O ( n ) times (i.e. on every iteration of the loop). Because character-by-character comparison of strings with length m takes O (m) time, the whole algorithm then takes a worst-case O ( mn ) time. Hash function used #","title":"[Rabin\u2013Karp algorithm](https://en.wikipedia.org/wiki/Rabin%E2%80%93Karp_algorithm)"},{"location":"Application/wikipedia-String/string-searching/Rabin\u2013Karp-algorithm/Rabin\u2013Karp-algorithm/#rabinkarp_algorithm","text":"NOTE: \u8fd9\u4e2a\u7b97\u6cd5\u5c55\u793a\u4e86\u8fdb\u884c\u5b57\u7b26\u4e32\u6bd4\u8f83\u7684\u53e6\u5916\u4e00\u79cd\u601d\u8def\uff1a hashing In computer science , the Rabin\u2013Karp algorithm or Karp\u2013Rabin algorithm is a string-searching algorithm created by Richard M. Karp and Michael O. Rabin ( 1987 ) that uses hashing to find an exact match of a pattern string in a text. It uses a rolling hash to quickly filter out positions of the text that cannot match the pattern, and then checks for a match at the remaining positions. Generalizations of the same idea can be used to find more than one match of a single pattern, or to find matches for more than one pattern. A practical application of the algorithm is detecting plagiarism . Given source material, the algorithm can rapidly search through a paper for instances of sentences from the source material, ignoring details such as case and punctuation. Because of the abundance of the sought strings, single-string searching algorithms are impractical. NOTE: Detecting plagiarism \u7684\u4e00\u4e2a\u7279\u70b9\u5c31\u662f\u5b83\u5e76\u4e0d\u8981\u6c42\u4e00\u5b57\u4e0d\u5dee\u7684\u7cbe\u51c6\u5339\u914d\u3002","title":"Rabin\u2013Karp algorithm"},{"location":"Application/wikipedia-String/string-searching/Rabin\u2013Karp-algorithm/Rabin\u2013Karp-algorithm/#overview","text":"Several string-matching algorithms, including the Knuth\u2013Morris\u2013Pratt algorithm and the Boyer\u2013Moore string-search algorithm , reduce the worst-case time for string matching by extracting more information from each mismatch, allowing them to skip over positions of the text that are guaranteed not to match the pattern. The Rabin\u2013Karp algorithm instead achieves its speedup by using a hash function to quickly perform an approximate check for each position, and then only performing an exact comparison at the positions that pass this approximate check. A hash function is a function which converts every string into a numeric value, called its hash value ; for example, we might have hash(\"hello\")=5 . If two strings are equal, their hash values are also equal. For a well-designed hash function, the opposite is true, in an approximate sense: strings that are unequal are very unlikely to have equal hash values. The Rabin\u2013Karp algorithm proceeds by computing, at each position of the text, the hash value of a string starting at that position with the same length as the pattern. If this hash value equals the hash value of the pattern, it performs a full comparison at that position.","title":"Overview"},{"location":"Application/wikipedia-String/string-searching/Rabin\u2013Karp-algorithm/Rabin\u2013Karp-algorithm/#the_algorithm","text":"The algorithm is as shown: 1function RabinKarp(string s[1..n], string pattern[1..m]) 2 hpattern := hash(pattern[1..m]); 3 for i from 1 to n-m+1 4 hs := hash(s[i..i+m-1]) 5 if hs = hpattern 6 if s[i..i+m-1] = pattern[1..m] 7 return i 8 return not found Lines 2, 4, and 6 each require O ( m ) time. However, line 2 is only executed once, and line 6 is only executed if the hash values match, which is unlikely to happen more than a few times. Line 5 is executed O( n ) times, but each comparison only requires constant time, so its impact is O( n ). The issue is line 4. Naively computing the hash value for the substring s[i+1..i+m] requires O ( m ) time because each character is examined. Since the hash computation is done on each loop, the algorithm with a na\u00efve hash computation requires O (mn) time, the same complexity as a straightforward string matching algorithms. For speed, the hash must be computed in constant time. The trick is the variable hs already contains the previous hash value of s[i..i+m-1] . If that value can be used to compute the next hash value in constant time, then computing successive hash values will be fast. The trick can be exploited using a rolling hash . A rolling hash is a hash function specially designed to enable this operation. A trivial (but not very good) rolling hash function just adds the values of each character in the substring. This rolling hash formula can compute the next hash value from the previous value in constant time: s[i+1..i+m] = s[i..i+m-1] - s[i] + s[i+m] This simple function works, but will result in statement 5 being executed more often than other more sophisticated rolling hash functions such as those discussed in the next section. NOTE: \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u7684\u610f\u601d\u662f\u4e0a\u9762\u5c55\u793a\u7684rolling hash function\u662f\u4e0d\u591f\u597d\u7684\uff0c\u5728\u4e0b\u4e00\u8282\u4e2d\u4f1a\u4ecb\u7ecd\u66f4\u597d\u7684\u540c\u65f6\u4e5f\u66f4\u52a0\u590d\u6742\u7684hash function Good performance requires a good hashing function for the encountered data. If the hashing is poor (such as producing the same hash value for every input), then line 6 would be executed O ( n ) times (i.e. on every iteration of the loop). Because character-by-character comparison of strings with length m takes O (m) time, the whole algorithm then takes a worst-case O ( mn ) time.","title":"The algorithm"},{"location":"Application/wikipedia-String/string-searching/Rabin\u2013Karp-algorithm/Rabin\u2013Karp-algorithm/#hash_function_used","text":"","title":"Hash function used"},{"location":"Application/wikipedia-parsing-algorithms/wikipedia-Dyck-language/","text":"Dyck language # Properties # The Dyck language is closed under the operation of concatenation . By treating $ \\Sigma ^{ } $ as an algebraic monoid under concatenation we see that the monoid structure transfers onto the quotient $ \\Sigma ^{ }/R $, resulting in the syntactic monoid of the Dyck language . The class $ \\operatorname {Cl} (\\epsilon ) $ will be denoted $ 1 $. The syntactic monoid of the Dyck language is not commutative : if $ u=\\operatorname {Cl} ([) $ and $ v=\\operatorname {Cl} (]) $ then $ uv=\\operatorname {Cl} ([])=1\\neq \\operatorname {Cl} (][)=vu $. With the notation above, $ uv=1 $ but neither $ u $ nor $ v $ are invertible in $ \\Sigma ^{*}/R $. The syntactic monoid of the Dyck language is isomorphic to the bicyclic semigroup by virtue of the properties of $ \\operatorname {Cl} ([) $ and $ \\operatorname {Cl} (]) $ described above. By the Chomsky\u2013Sch\u00fctzenberger representation theorem , any context-free language is a homomorphic image of the intersection of some regular language with a Dyck language on one or more kinds of bracket pairs. NOTE: \u63d0\u8fc7Dyck language\uff0c\u5f15\u5165hierarchy\u3002 The Dyck language with two distinct types of brackets can be recognized in the complexity class $ TC^{0} $ .[ 2] The number of distinct Dyck words with exactly n pairs of parentheses is the n -th Catalan number .","title":"[Dyck language](https://en.wikipedia.org/wiki/Dyck_language)"},{"location":"Application/wikipedia-parsing-algorithms/wikipedia-Dyck-language/#dyck_language","text":"","title":"Dyck language"},{"location":"Application/wikipedia-parsing-algorithms/wikipedia-Dyck-language/#properties","text":"The Dyck language is closed under the operation of concatenation . By treating $ \\Sigma ^{ } $ as an algebraic monoid under concatenation we see that the monoid structure transfers onto the quotient $ \\Sigma ^{ }/R $, resulting in the syntactic monoid of the Dyck language . The class $ \\operatorname {Cl} (\\epsilon ) $ will be denoted $ 1 $. The syntactic monoid of the Dyck language is not commutative : if $ u=\\operatorname {Cl} ([) $ and $ v=\\operatorname {Cl} (]) $ then $ uv=\\operatorname {Cl} ([])=1\\neq \\operatorname {Cl} (][)=vu $. With the notation above, $ uv=1 $ but neither $ u $ nor $ v $ are invertible in $ \\Sigma ^{*}/R $. The syntactic monoid of the Dyck language is isomorphic to the bicyclic semigroup by virtue of the properties of $ \\operatorname {Cl} ([) $ and $ \\operatorname {Cl} (]) $ described above. By the Chomsky\u2013Sch\u00fctzenberger representation theorem , any context-free language is a homomorphic image of the intersection of some regular language with a Dyck language on one or more kinds of bracket pairs. NOTE: \u63d0\u8fc7Dyck language\uff0c\u5f15\u5165hierarchy\u3002 The Dyck language with two distinct types of brackets can be recognized in the complexity class $ TC^{0} $ .[ 2] The number of distinct Dyck words with exactly n pairs of parentheses is the n -th Catalan number .","title":"Properties"},{"location":"Recursion/","text":"\u5173\u4e8e\u672c\u7ae0 # \u60f3\u8981\u5b66\u4e60\u7b97\u6cd5\uff1f\u5148\u4ece\u9012\u5f52\u5f00\u59cb\u5427\uff01","title":"Introduction"},{"location":"Recursion/#_1","text":"\u60f3\u8981\u5b66\u4e60\u7b97\u6cd5\uff1f\u5148\u4ece\u9012\u5f52\u5f00\u59cb\u5427\uff01","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"Recursion/Corecursion/","text":"Corecursion Examples Factorial Fibonacci sequence Tree traversal Corecursion # \u5171\u9012\u5f52 In computer science , corecursion is a type of operation that is dual to recursion . Whereas recursion works analytically , starting on data further from a base case and breaking it down into smaller data and repeating until one reaches a base case, corecursion works synthetically , starting from a base case and building it up, iteratively producing data further removed from a base case. Put simply, corecursive algorithms use the data that they themselves produce, bit by bit, as they become available, and needed, to produce further bits of data. A similar but distinct concept is generative recursion which may lack a definite\uff08\u786e\u5207\u7684\uff09 \"direction\" inherent in corecursion and recursion. SUMMARY : recursion \u548c corecursion \u7684\u8ba1\u7b97\u65b9\u5411\u662f\u76f8\u53cd\uff1a\u5bf9\u4e8e\u4e00\u4e2a recurrence relations \uff0c\u5982 n! := n \u00d7 (n - 1)! .\uff0crecursion\u662f\u4ece\u5de6\u81f3\u53f3\uff0c\u4f46\u662fcorecursion\u662f\u4ece\u53f3\u81f3\u5de6\uff0c\u4f46\u662f\u80fd\u591f\u6b8a\u9014\u540c\u5f52 recursion works analytically VS corecursion works synthetically recursion top-down VS corecursion bottom-up recursion reduce VS corecursion produce Where recursion allows programs to operate on arbitrarily complex data, so long as they can be reduced to simple data (base cases), corecursion allows programs to produce arbitrarily complex and potentially infinite data structures, such as streams , so long as it can be produced from simple data ( base cases ) in a sequence of finite steps. Where recursion may not terminate, never reaching a base state , corecursion starts from a base state , and thus produces subsequent steps deterministically, though it may proceed indefinitely (and thus not terminate under strict evaluation), or it may consume more than it produces and thus become non- productive . Many functions that are traditionally analyzed as recursive can alternatively, and arguably more naturally, be interpreted as corecursive functions that are terminated at a given stage, for example recurrence relations such as the factorial\uff08\u9636\u4e58\uff09. Corecursion can produce both finite and infinite data structures as results, and may employ self-referential data structures. Corecursion is often used in conjunction with lazy evaluation , to produce only a finite subset of a potentially infinite structure (rather than trying to produce an entire infinite structure at once). Corecursion is a particularly important concept in functional programming , where corecursion and codata allow total languages to work with infinite data structures. SUMMARY : python\u7684generator\u5c31\u662fCorecursion\u7684\u6700\u597d\u7684\u4f8b\u5b50\u3002 Examples # Corecursion can be understood by contrast with recursion, which is more familiar. While corecursion is primarily of interest in functional programming, it can be illustrated using imperative programming, which is done below using the generator facility in Python. In these examples local variables are used, and assigned values imperatively (destructively), though these are not necessary in corecursion in pure functional programming. In pure functional programming, rather than assigning to local variables , these computed values form an invariable sequence, and prior values are accessed by self-reference (later values in the sequence reference earlier values in the sequence to be computed). The assignments simply express this in the imperative paradigm and explicitly specify where the computations happen, which serves to clarify the exposition. Factorial # A classic example of recursion is computing the factorial \uff08\u9636\u4e58\uff09, which is defined recursively by 0! := 1 and n! := n \u00d7 (n - 1)! . To recursively compute its result on a given input, a recursive function calls (a copy of) itself with a different (\"smaller\" in some way) input and uses the result of this call to construct its result. The recursive call does the same, unless the base case has been reached. Thus a call stack develops in the process. For example, to compute fac(3) , this recursively calls in turn fac(2) , fac(1) , fac(0) (\"winding up\" the stack), at which point recursion terminates with fac(0) = 1 , and then the stack unwinds in reverse order and the results are calculated on the way back along the call stack to the initial call frame fac(3) that uses the result of fac(2) = 2 to calculate the final result as 3 \u00d7 2 = 3 \u00d7 fac(2) =: fac(3) and finally return fac(3) = 6 . In this example a function returns a single value. This stack unwinding can be explicated, defining the factorial corecursively , as an iterator , where one starts with the case of $ 1=:0! $, then from this starting value constructs factorial values for increasing numbers 1, 2, 3... as in the above recursive definition with \"time arrow\" reversed, as it were, by reading it backwards as $ n!\\times (n+1)=:(n+1)! $. The corecursive algorithm thus defined produces a stream of all factorials. This may be concretely implemented as a generator . Symbolically, noting that computing next factorial value requires keeping track of both n and f (a previous factorial value), this can be represented as: $ n,f=(0,1):(n+1,f\\times (n+1)) $ In Python, a recursive factorial function can be defined as:[ a] def factorial(n): if n == 0: return 1 else: return n * factorial(n - 1) This could then be called for example as factorial(5) to compute 5! . A corresponding corecursive generator can be defined as: def factorials(): n, f = 0, 1 while True: yield f n, f = n + 1, f * (n + 1) This generates an infinite stream of factorials in order; a finite portion of it can be produced by: def n_factorials(k): n, f = 0, 1 while n <= k: yield f n, f = n + 1, f * (n + 1) This could then be called to produce the factorials up to 5! via: for f in n_factorials(5): print(f) If we're only interested in a certain factorial, just the last value can be taken, or we can fuse the production and the access into one function, def nth_factorial(k): n, f = 0, 1 while n < k: n, f = n + 1, f * (n + 1) yield f As can be readily seen here, this is practically equivalent (just by substituting return for the only yield there) to the accumulator argument technique for tail recursion , unwound into an explicit loop. Thus it can be said that the concept of corecursion is an explication of the embodiment of iterative computation processes by recursive definitions, where applicable. \u56e0\u6b64\uff0c\u53ef\u4ee5\u8fd9\u6837\u8bf4\uff0c\u534f\u9012\u5f52\u7684\u6982\u5ff5\u662f\u901a\u8fc7\u9012\u5f52\u5b9a\u4e49\u6765\u89e3\u91ca\u8fed\u4ee3\u8ba1\u7b97\u8fc7\u7a0b\u7684\u4f53\u73b0\u3002 Fibonacci sequence # In the same way, the Fibonacci sequence can be represented as: $ a,b=(0,1):(b,a+b) $ Note that because the Fibonacci sequence is a recurrence relation of order 2, the corecursive relation must track two successive terms, with the $ (b,-) $ corresponding to shift forward by one step, and the $ (-,a+b) $ corresponding to computing the next term. This can then be implemented as follows (using parallel assignment ): def fibonacci_sequence(): a, b = 0, 1 while True: yield a a, b = b, a + b In Haskell, map fst ( (\\(a,b) -> (b,a+b)) `iterate` (0,1) ) Tree traversal # Tree traversal via a depth-first approach is a classic example of recursion . Dually, breadth-first traversal can very naturally be implemented via corecursion . Without using recursion or corecursion specifically, one may traverse a tree by starting at the root node , placing its child nodes in a data structure , then iterating by removing node after node from the data structure while placing each removed node's children back into that data structure.[ b] If the data structure is a stack (LIFO), this yields depth-first traversal , and if the data structure is a queue (FIFO), this yields breadth-first traversal . SUMMARY : \u5728\u4e0d\u4f7f\u7528recursion\u6216\u8005corecursion\u7684\u65f6\u5019\uff0c\u6211\u4eec\u5982\u679c\u60f3\u8981\u904d\u5386\u4e00\u68f5\u6811\uff0c\u5219\u9700\u8981\u501f\u52a9\u4e8e\u4e00\u4e2adata structure\u6765\u5b9e\u73b0\uff1b Using recursion , a (post-order)[ c] depth-first traversal can be implemented by starting at the root node and recursively traversing each child subtree in turn (the subtree based at each child node) \u2013 the second child subtree does not start processing until the first child subtree is finished. Once a leaf node is reached or the children of a branch node have been exhausted, the node itself is visited (e.g., the value of the node itself is outputted). In this case, the call stack (of the recursive functions ) acts as the stack that is iterated over. Using corecursion , a breadth-first traversal can be implemented by starting at the root node, outputting its value,[ d] then breadth-first traversing the subtrees \u2013 i.e., passing on the whole list of subtrees to the next step (not a single subtree, as in the recursive approach) \u2013 at the next step outputting the value of all of their root nodes, then passing on their child subtrees, etc.[ e] In this case the generator function , indeed the output sequence itself, acts as the queue . As in the factorial example (above), where the auxiliary information of the index (which step one was at, n ) was pushed forward, in addition to the actual output of n !, in this case the auxiliary information of the remaining subtrees is pushed forward, in addition to the actual output. Symbolically: $ v,t=([],[FullTree]):(RootValues(t),ChildTrees(t)) $","title":"Corecursion"},{"location":"Recursion/Corecursion/#corecursion","text":"\u5171\u9012\u5f52 In computer science , corecursion is a type of operation that is dual to recursion . Whereas recursion works analytically , starting on data further from a base case and breaking it down into smaller data and repeating until one reaches a base case, corecursion works synthetically , starting from a base case and building it up, iteratively producing data further removed from a base case. Put simply, corecursive algorithms use the data that they themselves produce, bit by bit, as they become available, and needed, to produce further bits of data. A similar but distinct concept is generative recursion which may lack a definite\uff08\u786e\u5207\u7684\uff09 \"direction\" inherent in corecursion and recursion. SUMMARY : recursion \u548c corecursion \u7684\u8ba1\u7b97\u65b9\u5411\u662f\u76f8\u53cd\uff1a\u5bf9\u4e8e\u4e00\u4e2a recurrence relations \uff0c\u5982 n! := n \u00d7 (n - 1)! .\uff0crecursion\u662f\u4ece\u5de6\u81f3\u53f3\uff0c\u4f46\u662fcorecursion\u662f\u4ece\u53f3\u81f3\u5de6\uff0c\u4f46\u662f\u80fd\u591f\u6b8a\u9014\u540c\u5f52 recursion works analytically VS corecursion works synthetically recursion top-down VS corecursion bottom-up recursion reduce VS corecursion produce Where recursion allows programs to operate on arbitrarily complex data, so long as they can be reduced to simple data (base cases), corecursion allows programs to produce arbitrarily complex and potentially infinite data structures, such as streams , so long as it can be produced from simple data ( base cases ) in a sequence of finite steps. Where recursion may not terminate, never reaching a base state , corecursion starts from a base state , and thus produces subsequent steps deterministically, though it may proceed indefinitely (and thus not terminate under strict evaluation), or it may consume more than it produces and thus become non- productive . Many functions that are traditionally analyzed as recursive can alternatively, and arguably more naturally, be interpreted as corecursive functions that are terminated at a given stage, for example recurrence relations such as the factorial\uff08\u9636\u4e58\uff09. Corecursion can produce both finite and infinite data structures as results, and may employ self-referential data structures. Corecursion is often used in conjunction with lazy evaluation , to produce only a finite subset of a potentially infinite structure (rather than trying to produce an entire infinite structure at once). Corecursion is a particularly important concept in functional programming , where corecursion and codata allow total languages to work with infinite data structures. SUMMARY : python\u7684generator\u5c31\u662fCorecursion\u7684\u6700\u597d\u7684\u4f8b\u5b50\u3002","title":"Corecursion"},{"location":"Recursion/Corecursion/#examples","text":"Corecursion can be understood by contrast with recursion, which is more familiar. While corecursion is primarily of interest in functional programming, it can be illustrated using imperative programming, which is done below using the generator facility in Python. In these examples local variables are used, and assigned values imperatively (destructively), though these are not necessary in corecursion in pure functional programming. In pure functional programming, rather than assigning to local variables , these computed values form an invariable sequence, and prior values are accessed by self-reference (later values in the sequence reference earlier values in the sequence to be computed). The assignments simply express this in the imperative paradigm and explicitly specify where the computations happen, which serves to clarify the exposition.","title":"Examples"},{"location":"Recursion/Corecursion/#factorial","text":"A classic example of recursion is computing the factorial \uff08\u9636\u4e58\uff09, which is defined recursively by 0! := 1 and n! := n \u00d7 (n - 1)! . To recursively compute its result on a given input, a recursive function calls (a copy of) itself with a different (\"smaller\" in some way) input and uses the result of this call to construct its result. The recursive call does the same, unless the base case has been reached. Thus a call stack develops in the process. For example, to compute fac(3) , this recursively calls in turn fac(2) , fac(1) , fac(0) (\"winding up\" the stack), at which point recursion terminates with fac(0) = 1 , and then the stack unwinds in reverse order and the results are calculated on the way back along the call stack to the initial call frame fac(3) that uses the result of fac(2) = 2 to calculate the final result as 3 \u00d7 2 = 3 \u00d7 fac(2) =: fac(3) and finally return fac(3) = 6 . In this example a function returns a single value. This stack unwinding can be explicated, defining the factorial corecursively , as an iterator , where one starts with the case of $ 1=:0! $, then from this starting value constructs factorial values for increasing numbers 1, 2, 3... as in the above recursive definition with \"time arrow\" reversed, as it were, by reading it backwards as $ n!\\times (n+1)=:(n+1)! $. The corecursive algorithm thus defined produces a stream of all factorials. This may be concretely implemented as a generator . Symbolically, noting that computing next factorial value requires keeping track of both n and f (a previous factorial value), this can be represented as: $ n,f=(0,1):(n+1,f\\times (n+1)) $ In Python, a recursive factorial function can be defined as:[ a] def factorial(n): if n == 0: return 1 else: return n * factorial(n - 1) This could then be called for example as factorial(5) to compute 5! . A corresponding corecursive generator can be defined as: def factorials(): n, f = 0, 1 while True: yield f n, f = n + 1, f * (n + 1) This generates an infinite stream of factorials in order; a finite portion of it can be produced by: def n_factorials(k): n, f = 0, 1 while n <= k: yield f n, f = n + 1, f * (n + 1) This could then be called to produce the factorials up to 5! via: for f in n_factorials(5): print(f) If we're only interested in a certain factorial, just the last value can be taken, or we can fuse the production and the access into one function, def nth_factorial(k): n, f = 0, 1 while n < k: n, f = n + 1, f * (n + 1) yield f As can be readily seen here, this is practically equivalent (just by substituting return for the only yield there) to the accumulator argument technique for tail recursion , unwound into an explicit loop. Thus it can be said that the concept of corecursion is an explication of the embodiment of iterative computation processes by recursive definitions, where applicable. \u56e0\u6b64\uff0c\u53ef\u4ee5\u8fd9\u6837\u8bf4\uff0c\u534f\u9012\u5f52\u7684\u6982\u5ff5\u662f\u901a\u8fc7\u9012\u5f52\u5b9a\u4e49\u6765\u89e3\u91ca\u8fed\u4ee3\u8ba1\u7b97\u8fc7\u7a0b\u7684\u4f53\u73b0\u3002","title":"Factorial"},{"location":"Recursion/Corecursion/#fibonacci_sequence","text":"In the same way, the Fibonacci sequence can be represented as: $ a,b=(0,1):(b,a+b) $ Note that because the Fibonacci sequence is a recurrence relation of order 2, the corecursive relation must track two successive terms, with the $ (b,-) $ corresponding to shift forward by one step, and the $ (-,a+b) $ corresponding to computing the next term. This can then be implemented as follows (using parallel assignment ): def fibonacci_sequence(): a, b = 0, 1 while True: yield a a, b = b, a + b In Haskell, map fst ( (\\(a,b) -> (b,a+b)) `iterate` (0,1) )","title":"Fibonacci sequence"},{"location":"Recursion/Corecursion/#tree_traversal","text":"Tree traversal via a depth-first approach is a classic example of recursion . Dually, breadth-first traversal can very naturally be implemented via corecursion . Without using recursion or corecursion specifically, one may traverse a tree by starting at the root node , placing its child nodes in a data structure , then iterating by removing node after node from the data structure while placing each removed node's children back into that data structure.[ b] If the data structure is a stack (LIFO), this yields depth-first traversal , and if the data structure is a queue (FIFO), this yields breadth-first traversal . SUMMARY : \u5728\u4e0d\u4f7f\u7528recursion\u6216\u8005corecursion\u7684\u65f6\u5019\uff0c\u6211\u4eec\u5982\u679c\u60f3\u8981\u904d\u5386\u4e00\u68f5\u6811\uff0c\u5219\u9700\u8981\u501f\u52a9\u4e8e\u4e00\u4e2adata structure\u6765\u5b9e\u73b0\uff1b Using recursion , a (post-order)[ c] depth-first traversal can be implemented by starting at the root node and recursively traversing each child subtree in turn (the subtree based at each child node) \u2013 the second child subtree does not start processing until the first child subtree is finished. Once a leaf node is reached or the children of a branch node have been exhausted, the node itself is visited (e.g., the value of the node itself is outputted). In this case, the call stack (of the recursive functions ) acts as the stack that is iterated over. Using corecursion , a breadth-first traversal can be implemented by starting at the root node, outputting its value,[ d] then breadth-first traversing the subtrees \u2013 i.e., passing on the whole list of subtrees to the next step (not a single subtree, as in the recursive approach) \u2013 at the next step outputting the value of all of their root nodes, then passing on their child subtrees, etc.[ e] In this case the generator function , indeed the output sequence itself, acts as the queue . As in the factorial example (above), where the auxiliary information of the index (which step one was at, n ) was pushed forward, in addition to the actual output of n !, in this case the auxiliary information of the remaining subtrees is pushed forward, in addition to the actual output. Symbolically: $ v,t=([],[FullTree]):(RootValues(t),ChildTrees(t)) $","title":"Tree traversal"},{"location":"Recursion/Recurrence-relation/","text":"Recurrence relation Definition Examples Factorial Recurrence relation # SUMMARY : \u9012\u5f52\u5173\u7cfb In mathematics , a recurrence relation is an equation that recursively defines a sequence or multidimensional array of values, once one or more initial terms are given; each further term of the sequence or array is defined as a function of the preceding terms. The term difference equation sometimes (and for the purposes of this article) refers to a specific type of recurrence relation . However, \"difference equation\" is frequently used to refer to any recurrence relation. Definition # A recurrence relation is an equation that expresses each element of a sequence as a function of the preceding ones . More precisely, in the case where only the immediately preceding element is involved, a recurrence relation has the form $ u_{n}=\\varphi (n,u_{n-1})\\quad {\\text{for}}\\quad n>0, $ where $ \\varphi :\\mathbb {N} \\times X\\to X $ is a function, where X is a set to which the elements of a sequence must belong. For any $ u_{0}\\in X $, this defines a unique sequence with $ u_{0} $as its first element, called the initial value .[ 1] It is easy to modify the definition for getting sequences starting from the term of index 1 or higher. This defines recurrence relation of first order . A recurrence relation of order k has the form $ u_{n}=\\varphi (n,u_{n-1},u_{n-2},\\ldots ,u_{n-k})\\quad {\\text{for}}\\quad n\\geq k, $ where $ \\varphi :\\mathbb {N} \\times X^{k}\\to X $ is a function that involves k consecutive elements of the sequence. In this case, k initial values are needed for defining a sequence. Examples # Factorial # The factorial is defined by the recurrence relation $ n!=n(n-1)!\\quad {\\text{for}}\\quad n>0, $ and the initial condition $ 0!=1. $","title":"Recurrence-relation"},{"location":"Recursion/Recurrence-relation/#recurrence_relation","text":"SUMMARY : \u9012\u5f52\u5173\u7cfb In mathematics , a recurrence relation is an equation that recursively defines a sequence or multidimensional array of values, once one or more initial terms are given; each further term of the sequence or array is defined as a function of the preceding terms. The term difference equation sometimes (and for the purposes of this article) refers to a specific type of recurrence relation . However, \"difference equation\" is frequently used to refer to any recurrence relation.","title":"Recurrence relation"},{"location":"Recursion/Recurrence-relation/#definition","text":"A recurrence relation is an equation that expresses each element of a sequence as a function of the preceding ones . More precisely, in the case where only the immediately preceding element is involved, a recurrence relation has the form $ u_{n}=\\varphi (n,u_{n-1})\\quad {\\text{for}}\\quad n>0, $ where $ \\varphi :\\mathbb {N} \\times X\\to X $ is a function, where X is a set to which the elements of a sequence must belong. For any $ u_{0}\\in X $, this defines a unique sequence with $ u_{0} $as its first element, called the initial value .[ 1] It is easy to modify the definition for getting sequences starting from the term of index 1 or higher. This defines recurrence relation of first order . A recurrence relation of order k has the form $ u_{n}=\\varphi (n,u_{n-1},u_{n-2},\\ldots ,u_{n-k})\\quad {\\text{for}}\\quad n\\geq k, $ where $ \\varphi :\\mathbb {N} \\times X^{k}\\to X $ is a function that involves k consecutive elements of the sequence. In this case, k initial values are needed for defining a sequence.","title":"Definition"},{"location":"Recursion/Recurrence-relation/#examples","text":"","title":"Examples"},{"location":"Recursion/Recurrence-relation/#factorial","text":"The factorial is defined by the recurrence relation $ n!=n(n-1)!\\quad {\\text{for}}\\quad n>0, $ and the initial condition $ 0!=1. $","title":"Factorial"},{"location":"Recursion/Recursion(computer-science)/","text":"Recursion (computer science) Recursive functions and algorithms Recursive data types Inductively defined data Coinductively defined data and corecursion Types of recursion Single recursion and multiple recursion Indirect recursion Anonymous recursion Structural versus generative recursion Recursive programs Recursive procedures Factorial Greatest common divisor Towers of Hanoi Binary search Recursive data structures (structural recursion) Linked lists Binary trees Implementation issues Recursion versus iteration Multiply recursive problems Refactoring recursion Tail-recursive functions Time-efficiency of recursive algorithms Shortcut rule (master theorem) Recursion (computer science) # Recursion in computer science is a method of solving a problem where the solution depends on solutions to smaller instances of the same problem (as opposed to iteration ).[ 1] The approach can be applied to many types of problems, and recursion is one of the central ideas of computer science.[ 2] SUMMARY : \u5728 Recursion \u4e2d\u662f\u4ece\u5b9a\u4e49\u7684\u89d2\u5ea6\u6765\u63cf\u8ff0\u9012\u5f52\uff0c\u5728\u672c\u7bc7\u4e2d\uff0c\u662f\u4ece\u89e3\u51b3\u95ee\u9898\u7684\u89d2\u5ea6\u6765\u63cf\u8ff0\u9012\u5f52\uff1b The power of recursion evidently lies in the possibility of defining an infinite\uff08\u65e0\u9650\u7684\uff09 set of objects by a finite\uff08\u6709\u9650\u7684\uff09 statement. In the same manner, an infinite number of computations can be described by a finite recursive program, even if this program contains no explicit repetitions. \u2014\u2009 Niklaus Wirth , Algorithms + Data Structures = Programs , 1976[ 3] SUMMARY : \u4ecefinite\uff08\u6709\u9650\u7684\uff09 statement\u5230 infinite\uff08\u65e0\u9650\u7684\uff09\uff0c\u8fd9\u6b63\u662frecursion\u7684\u5f3a\u5927\u6240\u5728\uff1b Most computer programming languages support recursion by allowing a function to call itself from within its own code. Some functional programming languages do not define any looping constructs but rely solely on recursion to repeatedly call code. Computability theory proves that these recursive-only languages are Turing complete ; they are as computationally powerful as Turing complete imperative languages, meaning they can solve the same kinds of problems as imperative languages even without iterative control structures such as while and for . \u5927\u591a\u6570\u8ba1\u7b97\u673a\u7f16\u7a0b\u8bed\u8a00\u901a\u8fc7\u5141\u8bb8\u51fd\u6570\u5728\u81ea\u5df1\u7684\u4ee3\u7801\u4e2d\u8c03\u7528\u81ea\u8eab\u6765\u652f\u6301\u9012\u5f52\u3002 \u4e00\u4e9b\u51fd\u6570\u5f0f\u7f16\u7a0b\u8bed\u8a00\u4e0d\u5b9a\u4e49\u4efb\u4f55\u5faa\u73af\u7ed3\u6784\uff0c\u800c\u4ec5\u4ec5\u4f9d\u8d56\u9012\u5f52\u6765\u91cd\u590d\u8c03\u7528\u4ee3\u7801\u3002 \u53ef\u8ba1\u7b97\u6027\u7406\u8bba\u8bc1\u660e\u4e86\u8fd9\u4e9b\u9012\u5f52\u8bed\u8a00\u662f\u56fe\u7075\u5b8c\u5907\u7684; \u5b83\u4eec\u4e0e\u56fe\u7075\u5b8c\u5168\u547d\u4ee4\u5f0f\u8bed\u8a00\u4e00\u6837\u5177\u6709\u5f3a\u5927\u7684\u8ba1\u7b97\u80fd\u529b\uff0c\u8fd9\u610f\u5473\u7740\u5373\u4f7f\u6ca1\u6709\u8bf8\u5982while\u548cfor\u4e4b\u7c7b\u7684\u8fed\u4ee3\u63a7\u5236\u7ed3\u6784\uff0c\u5b83\u4eec\u4e5f\u53ef\u4ee5\u50cf\u547d\u4ee4\u5f0f\u8bed\u8a00\u4e00\u6837\u89e3\u51b3\u76f8\u540c\u7c7b\u578b\u7684\u95ee\u9898\u3002 Recursive functions and algorithms # A common computer programming tactic is to divide a problem into sub-problems of the same type as the original, solve those sub-problems, and combine the results. This is often referred to as the divide-and-conquer method ; when combined with a lookup table that stores the results of solving sub-problems (to avoid solving them repeatedly and incurring extra computation time), it can be referred to as dynamic programming or memoization . SUMMARY : dynamic programming VS Recursion (computer science) A recursive function definition has one or more base cases , meaning input(s) for which the function produces a result trivially (without recurring), and one or more recursive cases , meaning input(s) for which the program recurs\uff08\u9012\u5f52\uff0c\u91cd\u73b0\uff0c\u91cd\u590d\uff09 (calls itself). For example, the factorial function can be defined recursively by the equations 0! = 1 and, for all n > 0, n ! = n ( n \u2212 1)!. Neither equation by itself constitutes a complete definition; the first is the base case, and the second is the recursive case. Because the base case breaks the chain of recursion, it is sometimes also called the \"terminating case\". SUMMARY : \u4e00\u4e2arecursive function\u7684\u6784\u6210 The job of the recursive cases can be seen as breaking down complex inputs into simpler ones. In a properly designed recursive function, with each recursive call, the input problem must be simplified in such a way that eventually the base case must be reached. (Functions that are not intended to terminate under normal circumstances\u2014for example, some system and server processes \u2014are an exception to this.) Neglecting to write a base case, or testing for it incorrectly, can cause an infinite loop . For some functions (such as one that computes the series for e = 1/0! + 1/1! + 1/2! + 1/3! + ...) there is not an obvious base case implied by the input data; for these one may add a parameter (such as the number of terms to be added, in our series example) to provide a 'stopping criterion' that establishes the base case . Such an example is more naturally treated by co-recursion , where successive terms in the output are the partial sums; this can be converted to a recursion by using the indexing parameter to say \"compute the n th term ( n th partial sum)\". SUMMARY : \u5728\u9012\u5f52\u51fd\u6570\u4e2d\u6dfb\u52a0\u4e00\u4e2a\u5165\u53c2\uff0c\u8fd9\u4e2a\u5165\u53c2\u5c31\u8868\u793a\u505c\u6b62\u6761\u4ef6\uff1b Recursive data types # Many computer programs must process or generate an arbitrarily large quantity of data . Recursion is one technique for representing data whose exact size the programmer does not know: the programmer can specify this data with a self-referential definition. There are two types of self-referential definitions : inductive and coinductive definitions. Further information: Algebraic data type Inductively defined data # Main article: Recursive data type Similarly recursive definitions are often used to model the structure of expressions and statements in programming languages. Language designers often express grammars in a syntax such as Backus\u2013Naur form ; here is such a grammar, for a simple language of arithmetic expressions with multiplication and addition: <expr> ::= <number> | (<expr> * <expr>) | (<expr> + <expr>) This says that an expression is either a number, a product of two expressions, or a sum of two expressions. By recursively referring to expressions in the second and third lines, the grammar permits arbitrarily complex arithmetic expressions such as (5 * ((3 * 6) + 8)) , with more than one product or sum operation in a single expression. Coinductively defined data and corecursion # Main articles: Coinduction and Corecursion Types of recursion # Single recursion and multiple recursion # Recursion that only contains a single self-reference is known as single recursion , while recursion that contains multiple self-references is known as multiple recursion . Standard examples of single recursion include list traversal, such as in a linear search, or computing the factorial function, while standard examples of multiple recursion include tree traversal , such as in a depth-first search . Single recursion is often much more efficient than multiple recursion , and can generally be replaced by an iterative computation , running in linear time and requiring constant space. Multiple recursion , by contrast, may require exponential time and space, and is more fundamentally recursive, not being able to be replaced by iteration without an explicit stack . SUMMARY : \u901a\u8fc7Fibonacci\u548ctree traversal \u7684\u4f8b\u5b50\u5c31\u53ef\u4ee5\u9a8c\u8bc1\u4e0a\u9762\u8fd9\u6bb5\u8bdd SUMMARY : Multiple recursion\u7684\u590d\u6742\u6027 Multiple recursion can sometimes be converted to single recursion (and, if desired, thence to iteration). For example, while computing the Fibonacci sequence naively is multiple iteration, as each value requires two previous values , it can be computed by single recursion by passing two successive values as parameters. This is more naturally framed as corecursion , building up from the initial values , tracking at each step two successive values \u2013 see corecursion: examples . A more sophisticated example is using a threaded binary tree , which allows iterative tree traversal, rather than multiple recursion. SUMMARY : Fibonacci \u51fd\u6570\u7684\u8868\u8fbe\u5f0f\u4e2d\u5305\u542b\u4e86\u4e24\u4e2a\u9012\u5f52\u8c03\u7528\uff0c\u4f46\u662f\u6b63\u5982\u4e0a\u9762\u6240\u8bf4\u7684\uff1a it can be computed by single recursion by passing two successive values as parameters\uff1b\u8fd9\u6837\u5c31\u53ef\u4ee5\u5c06\u5b83\u8f6c\u6362\u4e3a\u4e00\u4e2atail recursion\u4e86\uff0c\u8fd9\u6837\u5c31\u53ef\u4ee5\u6d88\u9664\u6389tail recursion\uff0c\u4f7f\u7528iterative\u65b9\u6cd5\u6765\u5b9e\u73b0\u4e86\uff1b Indirect recursion # Main article: Mutual recursion Most basic examples of recursion , and most of the examples presented here, demonstrate direct recursion , in which a function calls itself. Indirect recursion occurs when a function is called not by itself but by another function that it called (either directly or indirectly). For example, if f calls f, that is direct recursion, but if f calls g which calls f, then that is indirect recursion of f. Chains of three or more functions are possible; for example, function 1 calls function 2, function 2 calls function 3, and function 3 calls function 1 again. Indirect recursion is also called mutual recursion , which is a more symmetric term, though this is simply a difference of emphasis, not a different notion. That is, if f calls g and then g calls f, which in turn calls g again, from the point of view of f alone, f is indirectly recursing, while from the point of view of g alone, it is indirectly recursing, while from the point of view of both, f and g are mutually recursing on each other. Similarly a set of three or more functions that call each other can be called a set of mutually recursive functions. Anonymous recursion # Main article: Anonymous recursion Recursion is usually done by explicitly calling a function by name. However, recursion can also be done via implicitly calling a function based on the current context, which is particularly useful for anonymous functions , and is known as anonymous recursion . Structural versus generative recursion # See also: Structural recursion Some authors classify recursion as either \" structural \" or \" generative \". The distinction is related to where a recursive procedure gets the data that it works on, and how it processes that data: [Functions that consume structured data ] typically decompose their arguments into their immediate structural components and then process those components. If one of the immediate components belongs to the same class of data as the input, the function is recursive. For that reason, we refer to these functions as (STRUCTURALLY) RECURSIVE FUNCTIONS. \u2014\u2009Felleisen, Findler, Flatt, and Krishnaurthi, How to Design Programs , 2001[ 4] Thus, the defining characteristic of a structurally recursive function is that the argument to each recursive call is the content of a field of the original input. Structural recursion includes nearly all tree traversals , including XML processing , binary tree creation and search , etc. By considering the algebraic structure of the natural numbers (that is, a natural number is either zero or the successor of a natural number), functions such as factorial may also be regarded as structural recursion . Generative recursion is the alternative: Many well-known recursive algorithms generate an entirely new piece of data from the given data and recur on it. HtDP ( How to Design Programs ) refers to this kind as generative recursion . Examples of generative recursion include: gcd , quicksort , binary search , mergesort , Newton's method , fractals , and adaptive integration . \u2014\u2009Matthias Felleisen, Advanced Functional Programming , 2002[ 5] This distinction is important in proving termination of a function. All structurally recursive functions on finite ( inductively defined ) data structures can easily be shown to terminate, via structural induction : intuitively, each recursive call receives a smaller piece of input data, until a base case is reached. Generatively recursive functions , in contrast, do not necessarily feed smaller input to their recursive calls, so proof of their termination is not necessarily as simple, and avoiding infinite loops requires greater care. These generatively recursive functions can often be interpreted as corecursive functions \u2013 each step generates the new data, such as successive approximation in Newton's method \u2013 and terminating this corecursion requires that the data eventually satisfy some condition, which is not necessarily guaranteed. In terms of loop variants , structural recursion is when there is an obvious loop variant, namely size or complexity, which starts off finite and decreases at each recursive step. By contrast, generative recursion is when there is not such an obvious loop variant , and termination depends on a function, such as \"error of approximation\" that does not necessarily decrease to zero, and thus termination is not guaranteed without further analysis. Recursive programs # Recursive procedures # Factorial # Greatest common divisor # The Euclidean algorithm , which computes the greatest common divisor of two integers, can be written recursively. Function definition : $ \\gcd(x,y)={\\begin{cases}x&{\\mbox{if }}y=0\\\\gcd(y,\\operatorname {remainder} (x,y))&{\\mbox{if }}y>0\\\\end{cases}} $ Pseudocode (recursive): function gcd is: input: integer x, integer y such that x > 0 and y >= 0 1. if y is 0, return x 2. otherwise, return [ gcd( y, (remainder of x/y) ) ] end gcd Recurrence relation for greatest common divisor, where $ x\\%y $ expresses the remainder of $ x/y $: $ \\gcd(x,y)=\\gcd(y,x\\%y) $ if $ y\\neq 0 $ $ \\gcd(x,0)=x $ Computing the recurrence relation for x = 27 and y = 9: gcd(27, 9) = gcd(9, 27% 9) = gcd(9, 0) = 9 The recursive program above is tail-recursive ; it is equivalent to an iterative algorithm , and the computation shown above shows the steps of evaluation that would be performed by a language that eliminates tail calls . Below is a version of the same algorithm using explicit iteration, suitable for a language that does not eliminate tail calls. By maintaining its state entirely in the variables x and y and using a looping construct, the program avoids making recursive calls and growing the call stack. Pseudocode (iterative): function gcd is: input: integer x, integer y such that x >= y and y >= 0 1. create new variable called remainder 2. begin loop 1. if y is zero, exit loop 2. set remainder to the remainder of x/y 3. set x to y 4. set y to remainder 5. repeat loop 3. return x end gcd The iterative algorithm requires a temporary variable, and even given knowledge of the Euclidean algorithm it is more difficult to understand the process by simple inspection, although the two algorithms are very similar in their steps. Towers of Hanoi # Main article: Towers of Hanoi The Towers of Hanoi is a mathematical puzzle whose solution illustrates recursion.[ 6] [ 7] There are three pegs which can hold stacks of disks of different diameters. A larger disk may never be stacked on top of a smaller. Starting with n disks on one peg, they must be moved to another peg one at a time. What is the smallest number of steps to move the stack? Function definition : $ \\operatorname {hanoi} (n)={\\begin{cases}1&{\\mbox{if }}n=1\\2\\cdot \\operatorname {hanoi} (n-1)+1&{\\mbox{if }}n>1\\\\end{cases}} $ Recurrence relation for hanoi : $ h_{n}=2h_{n-1}+1 $ $ h_{1}=1 $ Computing the recurrence relation for n = 4: hanoi(4) = 2*hanoi(3) + 1 = 2*(2*hanoi(2) + 1) + 1 = 2*(2*(2*hanoi(1) + 1) + 1) + 1 = 2*(2*(2*1 + 1) + 1) + 1 = 2*(2*(3) + 1) + 1 = 2*(7) + 1 = 15 Example implementations: Binary search # The binary search algorithm is a method of searching a sorted array for a single element by cutting the array in half with each recursive pass. The trick is to pick a midpoint near the center of the array, compare the data at that point with the data being searched and then responding to one of three possible conditions: the data is found at the midpoint, the data at the midpoint is greater than the data being searched for, or the data at the midpoint is less than the data being searched for. Recursion is used in this algorithm because with each pass a new array is created by cutting the old one in half. The binary search procedure is then called recursively, this time on the new (and smaller) array. Typically the array's size is adjusted by manipulating a beginning and ending index. The algorithm exhibits a logarithmic order of growth because it essentially divides the problem domain in half with each pass. Example implementation of binary search in C: /* Call binary_search with proper initial conditions. INPUT: data is an array of integers SORTED in ASCENDING order, toFind is the integer to search for, count is the total number of elements in the array OUTPUT: result of binary_search */ int search(int *data, int toFind, int count) { // Start = 0 (beginning index) // End = count - 1 (top index) return binary_search(data, toFind, 0, count-1); } /* Binary Search Algorithm. INPUT: data is a array of integers SORTED in ASCENDING order, toFind is the integer to search for, start is the minimum array index, end is the maximum array index OUTPUT: position of the integer toFind within array data, -1 if not found */ int binary_search(int *data, int toFind, int start, int end) { //Get the midpoint. int mid = start + (end - start)/2; //Integer division //Stop condition. if (start > end) return -1; else if (data[mid] == toFind) //Found? return mid; else if (data[mid] > toFind) //Data is greater than toFind, search lower half return binary_search(data, toFind, start, mid-1); else //Data is less than toFind, search upper half return binary_search(data, toFind, mid+1, end); } Recursive data structures (structural recursion) # Main article: Recursive data type An important application of recursion in computer science is in defining dynamic data structures such as lists and trees . Recursive data structures can dynamically grow to a theoretically infinite size in response to runtime requirements; in contrast, the size of a static array must be set at compile time. \" Recursive algorithms are particularly appropriate when the underlying problem or the data to be treated are defined in recursive terms.\"[ 9] The examples in this section illustrate what is known as \" structural recursion \". This term refers to the fact that the recursive procedures are acting on data that is defined recursively. As long as a programmer derives the template from a data definition, functions employ structural recursion. That is, the recursions in a function's body consume some immediate piece of a given compound value.[ 5] Linked lists # Main article: Linked list Below is a C definition of a linked list node structure. Notice especially how the node is defined in terms of itself. The \"next\" element of struct node is a pointer to another struct node , effectively creating a list type. struct node { int data; // some integer data struct node *next; // pointer to another struct node }; Because the struct node data structure is defined recursively , procedures that operate on it can be implemented naturally as recursive procedures . The list_print procedure defined below walks down the list until the list is empty (i.e., the list pointer has a value of NULL). For each node it prints the data element (an integer). In the C implementation, the list remains unchanged by the list_print procedure. void list_print(struct node *list) { if (list != NULL) // base case { printf (\"%d \", list->data); // print integer data followed by a space list_print (list->next); // recursive call on the next node } } Binary trees # Main article: Binary tree Below is a simple definition for a binary tree node. Like the node for linked lists, it is defined in terms of itself, recursively. There are two self-referential pointers: left (pointing to the left sub-tree) and right (pointing to the right sub-tree). struct node { int data; // some integer data struct node *left; // pointer to the left subtree struct node *right; // point to the right subtree }; Operations on the tree can be implemented using recursion. Note that because there are two self-referencing pointers (left and right), tree operations may require two recursive calls: // Test if tree_node contains i; return 1 if so, 0 if not. int tree_contains(struct node *tree_node, int i) { if (tree_node == NULL) return 0; // base case else if (tree_node->data == i) return 1; else return tree_contains(tree_node->left, i) || tree_contains(tree_node->right, i); } At most two recursive calls will be made for any given call to tree_contains as defined above. // Inorder traversal: void tree_print(struct node *tree_node) { if (tree_node != NULL) { // base case tree_print(tree_node->left); // go left printf(\"%d \", tree_node->data); // print the integer followed by a space tree_print(tree_node->right); // go right } } 5 3 7 2 4 6 8 P(5) P(3) P(2) P(NULL) return printf(2) P(NULL) return printf(3) p(4) P(NULL) return printf(4) P(NULL) return \u5199\u9012\u5f52\u51fd\u6570\u7684\u6838\u5fc3\u5728\u4e8e\u628a\u6211\u9012\u5f52\u7684\u672c\u8d28\uff1a\u81ea\u9876\u5411\u4e0b\uff0c\u53ea\u6709\u5b50\u95ee\u9898\u90fd\u89e3\u4e86\uff0c\u624d\u80fd\u591f\u89e3\u4e0a\u4e00\u5c42\u7684\u95ee\u9898\u3002\u4f7f\u7528\u7cfb\u7edf\u5806\u6808\u6765\u5b9e\u73b0\u8be5\u8fc7\u7a0b\uff0c\u56e0\u4e3a\u7cfb\u7edf\u5806\u6808\u80fd\u591f\u4e0d\u65ad\u5730\u6309\u7167\u9012\u5f52\u7684\u987a\u5e8f\u8fdb\u884c\u5165\u6808\uff0c\u76f4\u5230\u8fbe\u5230\u6700\u5c0f\u7684\u5b50\u95ee\u9898\uff0c\u4ece\u800c\u5c06\u5b50\u95ee\u9898\u89e3\u51b3\uff0c\u7136\u540e\u51fa\u6808\uff0c\u7136\u540e\u89e3\u51b3\u4e0a\u4e00\u5c42\u5b50\u95ee\u9898\uff0c\u76f4\u81f3\u6700\u9876\u5c42\u7684\u95ee\u9898\u89e3\u51b3\u4e86\uff1b \u6bcf\u4e2a\u9012\u5f52\u8c03\u7528\u90fd\u662f\u4e00\u6761\u76f4\u7ebf\uff0c\u5982\u679c\u9012\u5f52\u51fd\u6570\u4e2d\uff0c\u51fa\u73b0\u4e86\u4e24\u6b21\u9012\u5f52\u8c03\u7528\uff0c\u5219\u5c31\u662f\u4e24\u6761\u7ebf\u4e86\uff1b \u5bf9\u4e8e\u6709\u9012\u5f52\u6027\u8d28\u7684\u95ee\u9898\u6216\u8005\u7ed3\u6784\uff0c\u6211\u4eec\u53ef\u4ee5\u6309\u7167\u4e0a\u8ff0\u9012\u5f52\u7684\u601d\u60f3\u53bb\u7f16\u5199\u7a0b\u5e8f\u3002\u5f53\u8c08\u53ca\u9700\u8981\u9a8c\u8bc1\u6216\u8005\u6a21\u62df\u9012\u5f52\u51fd\u6570\u7684\u6267\u884c\u8fc7\u7a0b\u7684\u65f6\u5019\uff0c\u5c31\u9700\u8981\u4ece\u76f8\u53cd\u7684\u65b9\u5411\u6765\u89e3\u51b3\u95ee\u9898\u4e86\uff0c\u6211\u4eec\u9700\u8981\u4ece\u5e95\u5411\u4e0a\u6765\u6267\u884c\u3002 Implementation issues # In actual implementation, rather than a pure recursive function (single check for base case, otherwise recursive step), a number of modifications may be made, for purposes of clarity or efficiency. These include: Wrapper function (at top) Short-circuiting\uff08\u7b80\u5316\uff09 the base case , aka \"Arm's-length recursion\" (at bottom) Hybrid algorithm (at bottom) \u2013 switching to a different algorithm once data is small enough On the basis of elegance, wrapper functions are generally approved, while short-circuiting the base case is frowned upon, particularly in academia. Hybrid algorithms are often used for efficiency, to reduce the overhead of recursion in small cases, and arm's-length recursion is a special case of this. Recursion versus iteration # Recursion and iteration are equally expressive: recursion can be replaced by iteration with an explicit call stack , while iteration can be replaced with tail recursion . Which approach is preferable depends on the problem under consideration and the language used. In imperative programming , iteration is preferred, particularly for simple recursion, as it avoids the overhead of function calls and call stack management, but recursion is generally used for multiple recursion . By contrast, in functional languages recursion is preferred, with tail recursion optimization leading to little overhead. Implementing an algorithm using iteration may not be easily achievable. For example, a factorial function may be implemented iteratively in C by assigning to an loop index variable and accumulator variable, rather than by passing arguments and returning values by recursion: unsigned int factorial(unsigned int n) { unsigned int product = 1; // empty product is 1 while (n) { product *= n; --n; } return product; } Multiply recursive problems # Multiply recursive problems are inherently recursive, because of prior state they need to track. One example is tree traversal as in depth-first search ; though both recursive and iterative methods are used,[ 19] they contrast with list traversal and linear search in a list, which is a singly recursive and thus naturally iterative method. Other examples include divide-and-conquer algorithms such as Quicksort , and functions such as the Ackermann function . All of these algorithms can be implemented iteratively with the help of an explicit stack , but the programmer effort involved in managing the stack, and the complexity of the resulting program, arguably outweigh any advantages of the iterative solution. Refactoring recursion # Recursive algorithms can be replaced with non-recursive counterparts.[ 20] . One method for replacing recursive algorithms is to simulate them using heap memory in place of stack memory .[ 21] An alternative is to develop a replacement algorithm entirely based on non-recursive methods, which can be challenging.[ 22] For example, recursive algorithms for matching wildcards , such as Rich Salz ' wildmat algorithm,[ 23] were once typical. Non-recursive algorithms for the same purpose, such as the Krauss matching wildcards algorithm , have been developed to avoid the drawbacks of recursion[ 24] and have improved only gradually based on techniques such as collecting tests and profiling performance.[ 25] Tail-recursive functions # Tail-recursive functions are functions in which all recursive calls are tail calls and hence do not build up any deferred operations. For example, the gcd function (shown again below) is tail-recursive. In contrast, the factorial function (also below) is not tail-recursive; because its recursive call is not in tail position, it builds up deferred multiplication operations that must be performed after the final recursive call completes. With a compiler or interpreter that treats tail-recursive calls as jumps rather than function calls , a tail-recursive function such as gcd will execute using constant space . Thus the program is essentially iterative, equivalent to using imperative language control structures like the \"for\" and \"while\" loops. Tail recursion : //INPUT: Integers x, y such that x >= y and y >= 0 int gcd(int x, int y) { if (y == 0) return x; else return gcd(y, x % y); } Augmenting recursion: //INPUT: n is an Integer such that n >= 0 int fact(int n) { if (n == 0) return 1; else return n * fact(n - 1); } The significance of tail recursion is that when making a tail-recursive call (or any tail call), the caller's return position need not be saved on the call stack ; when the recursive call returns, it will branch directly on the previously saved return position. Therefore, in languages that recognize this property of tail calls, tail recursion saves both space and time. Time-efficiency of recursive algorithms # The time efficiency of recursive algorithms can be expressed in a recurrence relation of Big O notation . They can (usually) then be simplified into a single Big-O term. Shortcut rule (master theorem) # Main article: Master theorem (analysis of algorithms) If the time-complexity of the function is in the form $ T(n)=a\\cdot T(n/b)+f(n) $ Then the Big O of the time-complexity is thus: If $ f(n)=O(n^{\\log {b}a-\\epsilon }) $ for some constant $ \\epsilon >0 $, then $ T(n)=\\Theta (n^{\\log {b}a}) $ If $ f(n)=\\Theta (n^{\\log {b}a}) $, then $ T(n)=\\Theta (n^{\\log {b}a}\\log n) $ If $ f(n)=\\Omega (n^{\\log _{b}a+\\epsilon }) $ for some constant $ \\epsilon >0 $, and if $ a\\cdot f(n/b)\\leq c\\cdot f(n) $ for some constant c < 1 and all sufficiently large n , then $ T(n)=\\Theta (f(n)) $ where a represents the number of recursive calls at each level of recursion, b represents by what factor smaller the input is for the next level of recursion (i.e. the number of pieces you divide the problem into), and f \u2009( n ) represents the work the function does independent of any recursion (e.g. partitioning, recombining) at each level of recursion.","title":"Recursion(computer-science)"},{"location":"Recursion/Recursion(computer-science)/#recursion_computer_science","text":"Recursion in computer science is a method of solving a problem where the solution depends on solutions to smaller instances of the same problem (as opposed to iteration ).[ 1] The approach can be applied to many types of problems, and recursion is one of the central ideas of computer science.[ 2] SUMMARY : \u5728 Recursion \u4e2d\u662f\u4ece\u5b9a\u4e49\u7684\u89d2\u5ea6\u6765\u63cf\u8ff0\u9012\u5f52\uff0c\u5728\u672c\u7bc7\u4e2d\uff0c\u662f\u4ece\u89e3\u51b3\u95ee\u9898\u7684\u89d2\u5ea6\u6765\u63cf\u8ff0\u9012\u5f52\uff1b The power of recursion evidently lies in the possibility of defining an infinite\uff08\u65e0\u9650\u7684\uff09 set of objects by a finite\uff08\u6709\u9650\u7684\uff09 statement. In the same manner, an infinite number of computations can be described by a finite recursive program, even if this program contains no explicit repetitions. \u2014\u2009 Niklaus Wirth , Algorithms + Data Structures = Programs , 1976[ 3] SUMMARY : \u4ecefinite\uff08\u6709\u9650\u7684\uff09 statement\u5230 infinite\uff08\u65e0\u9650\u7684\uff09\uff0c\u8fd9\u6b63\u662frecursion\u7684\u5f3a\u5927\u6240\u5728\uff1b Most computer programming languages support recursion by allowing a function to call itself from within its own code. Some functional programming languages do not define any looping constructs but rely solely on recursion to repeatedly call code. Computability theory proves that these recursive-only languages are Turing complete ; they are as computationally powerful as Turing complete imperative languages, meaning they can solve the same kinds of problems as imperative languages even without iterative control structures such as while and for . \u5927\u591a\u6570\u8ba1\u7b97\u673a\u7f16\u7a0b\u8bed\u8a00\u901a\u8fc7\u5141\u8bb8\u51fd\u6570\u5728\u81ea\u5df1\u7684\u4ee3\u7801\u4e2d\u8c03\u7528\u81ea\u8eab\u6765\u652f\u6301\u9012\u5f52\u3002 \u4e00\u4e9b\u51fd\u6570\u5f0f\u7f16\u7a0b\u8bed\u8a00\u4e0d\u5b9a\u4e49\u4efb\u4f55\u5faa\u73af\u7ed3\u6784\uff0c\u800c\u4ec5\u4ec5\u4f9d\u8d56\u9012\u5f52\u6765\u91cd\u590d\u8c03\u7528\u4ee3\u7801\u3002 \u53ef\u8ba1\u7b97\u6027\u7406\u8bba\u8bc1\u660e\u4e86\u8fd9\u4e9b\u9012\u5f52\u8bed\u8a00\u662f\u56fe\u7075\u5b8c\u5907\u7684; \u5b83\u4eec\u4e0e\u56fe\u7075\u5b8c\u5168\u547d\u4ee4\u5f0f\u8bed\u8a00\u4e00\u6837\u5177\u6709\u5f3a\u5927\u7684\u8ba1\u7b97\u80fd\u529b\uff0c\u8fd9\u610f\u5473\u7740\u5373\u4f7f\u6ca1\u6709\u8bf8\u5982while\u548cfor\u4e4b\u7c7b\u7684\u8fed\u4ee3\u63a7\u5236\u7ed3\u6784\uff0c\u5b83\u4eec\u4e5f\u53ef\u4ee5\u50cf\u547d\u4ee4\u5f0f\u8bed\u8a00\u4e00\u6837\u89e3\u51b3\u76f8\u540c\u7c7b\u578b\u7684\u95ee\u9898\u3002","title":"Recursion (computer science)"},{"location":"Recursion/Recursion(computer-science)/#recursive_functions_and_algorithms","text":"A common computer programming tactic is to divide a problem into sub-problems of the same type as the original, solve those sub-problems, and combine the results. This is often referred to as the divide-and-conquer method ; when combined with a lookup table that stores the results of solving sub-problems (to avoid solving them repeatedly and incurring extra computation time), it can be referred to as dynamic programming or memoization . SUMMARY : dynamic programming VS Recursion (computer science) A recursive function definition has one or more base cases , meaning input(s) for which the function produces a result trivially (without recurring), and one or more recursive cases , meaning input(s) for which the program recurs\uff08\u9012\u5f52\uff0c\u91cd\u73b0\uff0c\u91cd\u590d\uff09 (calls itself). For example, the factorial function can be defined recursively by the equations 0! = 1 and, for all n > 0, n ! = n ( n \u2212 1)!. Neither equation by itself constitutes a complete definition; the first is the base case, and the second is the recursive case. Because the base case breaks the chain of recursion, it is sometimes also called the \"terminating case\". SUMMARY : \u4e00\u4e2arecursive function\u7684\u6784\u6210 The job of the recursive cases can be seen as breaking down complex inputs into simpler ones. In a properly designed recursive function, with each recursive call, the input problem must be simplified in such a way that eventually the base case must be reached. (Functions that are not intended to terminate under normal circumstances\u2014for example, some system and server processes \u2014are an exception to this.) Neglecting to write a base case, or testing for it incorrectly, can cause an infinite loop . For some functions (such as one that computes the series for e = 1/0! + 1/1! + 1/2! + 1/3! + ...) there is not an obvious base case implied by the input data; for these one may add a parameter (such as the number of terms to be added, in our series example) to provide a 'stopping criterion' that establishes the base case . Such an example is more naturally treated by co-recursion , where successive terms in the output are the partial sums; this can be converted to a recursion by using the indexing parameter to say \"compute the n th term ( n th partial sum)\". SUMMARY : \u5728\u9012\u5f52\u51fd\u6570\u4e2d\u6dfb\u52a0\u4e00\u4e2a\u5165\u53c2\uff0c\u8fd9\u4e2a\u5165\u53c2\u5c31\u8868\u793a\u505c\u6b62\u6761\u4ef6\uff1b","title":"Recursive functions and algorithms"},{"location":"Recursion/Recursion(computer-science)/#recursive_data_types","text":"Many computer programs must process or generate an arbitrarily large quantity of data . Recursion is one technique for representing data whose exact size the programmer does not know: the programmer can specify this data with a self-referential definition. There are two types of self-referential definitions : inductive and coinductive definitions. Further information: Algebraic data type","title":"Recursive data types"},{"location":"Recursion/Recursion(computer-science)/#inductively_defined_data","text":"Main article: Recursive data type Similarly recursive definitions are often used to model the structure of expressions and statements in programming languages. Language designers often express grammars in a syntax such as Backus\u2013Naur form ; here is such a grammar, for a simple language of arithmetic expressions with multiplication and addition: <expr> ::= <number> | (<expr> * <expr>) | (<expr> + <expr>) This says that an expression is either a number, a product of two expressions, or a sum of two expressions. By recursively referring to expressions in the second and third lines, the grammar permits arbitrarily complex arithmetic expressions such as (5 * ((3 * 6) + 8)) , with more than one product or sum operation in a single expression.","title":"Inductively defined data"},{"location":"Recursion/Recursion(computer-science)/#coinductively_defined_data_and_corecursion","text":"Main articles: Coinduction and Corecursion","title":"Coinductively defined data and corecursion"},{"location":"Recursion/Recursion(computer-science)/#types_of_recursion","text":"","title":"Types of recursion"},{"location":"Recursion/Recursion(computer-science)/#single_recursion_and_multiple_recursion","text":"Recursion that only contains a single self-reference is known as single recursion , while recursion that contains multiple self-references is known as multiple recursion . Standard examples of single recursion include list traversal, such as in a linear search, or computing the factorial function, while standard examples of multiple recursion include tree traversal , such as in a depth-first search . Single recursion is often much more efficient than multiple recursion , and can generally be replaced by an iterative computation , running in linear time and requiring constant space. Multiple recursion , by contrast, may require exponential time and space, and is more fundamentally recursive, not being able to be replaced by iteration without an explicit stack . SUMMARY : \u901a\u8fc7Fibonacci\u548ctree traversal \u7684\u4f8b\u5b50\u5c31\u53ef\u4ee5\u9a8c\u8bc1\u4e0a\u9762\u8fd9\u6bb5\u8bdd SUMMARY : Multiple recursion\u7684\u590d\u6742\u6027 Multiple recursion can sometimes be converted to single recursion (and, if desired, thence to iteration). For example, while computing the Fibonacci sequence naively is multiple iteration, as each value requires two previous values , it can be computed by single recursion by passing two successive values as parameters. This is more naturally framed as corecursion , building up from the initial values , tracking at each step two successive values \u2013 see corecursion: examples . A more sophisticated example is using a threaded binary tree , which allows iterative tree traversal, rather than multiple recursion. SUMMARY : Fibonacci \u51fd\u6570\u7684\u8868\u8fbe\u5f0f\u4e2d\u5305\u542b\u4e86\u4e24\u4e2a\u9012\u5f52\u8c03\u7528\uff0c\u4f46\u662f\u6b63\u5982\u4e0a\u9762\u6240\u8bf4\u7684\uff1a it can be computed by single recursion by passing two successive values as parameters\uff1b\u8fd9\u6837\u5c31\u53ef\u4ee5\u5c06\u5b83\u8f6c\u6362\u4e3a\u4e00\u4e2atail recursion\u4e86\uff0c\u8fd9\u6837\u5c31\u53ef\u4ee5\u6d88\u9664\u6389tail recursion\uff0c\u4f7f\u7528iterative\u65b9\u6cd5\u6765\u5b9e\u73b0\u4e86\uff1b","title":"Single recursion and multiple recursion"},{"location":"Recursion/Recursion(computer-science)/#indirect_recursion","text":"Main article: Mutual recursion Most basic examples of recursion , and most of the examples presented here, demonstrate direct recursion , in which a function calls itself. Indirect recursion occurs when a function is called not by itself but by another function that it called (either directly or indirectly). For example, if f calls f, that is direct recursion, but if f calls g which calls f, then that is indirect recursion of f. Chains of three or more functions are possible; for example, function 1 calls function 2, function 2 calls function 3, and function 3 calls function 1 again. Indirect recursion is also called mutual recursion , which is a more symmetric term, though this is simply a difference of emphasis, not a different notion. That is, if f calls g and then g calls f, which in turn calls g again, from the point of view of f alone, f is indirectly recursing, while from the point of view of g alone, it is indirectly recursing, while from the point of view of both, f and g are mutually recursing on each other. Similarly a set of three or more functions that call each other can be called a set of mutually recursive functions.","title":"Indirect recursion"},{"location":"Recursion/Recursion(computer-science)/#anonymous_recursion","text":"Main article: Anonymous recursion Recursion is usually done by explicitly calling a function by name. However, recursion can also be done via implicitly calling a function based on the current context, which is particularly useful for anonymous functions , and is known as anonymous recursion .","title":"Anonymous recursion"},{"location":"Recursion/Recursion(computer-science)/#structural_versus_generative_recursion","text":"See also: Structural recursion Some authors classify recursion as either \" structural \" or \" generative \". The distinction is related to where a recursive procedure gets the data that it works on, and how it processes that data: [Functions that consume structured data ] typically decompose their arguments into their immediate structural components and then process those components. If one of the immediate components belongs to the same class of data as the input, the function is recursive. For that reason, we refer to these functions as (STRUCTURALLY) RECURSIVE FUNCTIONS. \u2014\u2009Felleisen, Findler, Flatt, and Krishnaurthi, How to Design Programs , 2001[ 4] Thus, the defining characteristic of a structurally recursive function is that the argument to each recursive call is the content of a field of the original input. Structural recursion includes nearly all tree traversals , including XML processing , binary tree creation and search , etc. By considering the algebraic structure of the natural numbers (that is, a natural number is either zero or the successor of a natural number), functions such as factorial may also be regarded as structural recursion . Generative recursion is the alternative: Many well-known recursive algorithms generate an entirely new piece of data from the given data and recur on it. HtDP ( How to Design Programs ) refers to this kind as generative recursion . Examples of generative recursion include: gcd , quicksort , binary search , mergesort , Newton's method , fractals , and adaptive integration . \u2014\u2009Matthias Felleisen, Advanced Functional Programming , 2002[ 5] This distinction is important in proving termination of a function. All structurally recursive functions on finite ( inductively defined ) data structures can easily be shown to terminate, via structural induction : intuitively, each recursive call receives a smaller piece of input data, until a base case is reached. Generatively recursive functions , in contrast, do not necessarily feed smaller input to their recursive calls, so proof of their termination is not necessarily as simple, and avoiding infinite loops requires greater care. These generatively recursive functions can often be interpreted as corecursive functions \u2013 each step generates the new data, such as successive approximation in Newton's method \u2013 and terminating this corecursion requires that the data eventually satisfy some condition, which is not necessarily guaranteed. In terms of loop variants , structural recursion is when there is an obvious loop variant, namely size or complexity, which starts off finite and decreases at each recursive step. By contrast, generative recursion is when there is not such an obvious loop variant , and termination depends on a function, such as \"error of approximation\" that does not necessarily decrease to zero, and thus termination is not guaranteed without further analysis.","title":"Structural versus generative recursion"},{"location":"Recursion/Recursion(computer-science)/#recursive_programs","text":"","title":"Recursive programs"},{"location":"Recursion/Recursion(computer-science)/#recursive_procedures","text":"","title":"Recursive procedures"},{"location":"Recursion/Recursion(computer-science)/#factorial","text":"","title":"Factorial"},{"location":"Recursion/Recursion(computer-science)/#greatest_common_divisor","text":"The Euclidean algorithm , which computes the greatest common divisor of two integers, can be written recursively. Function definition : $ \\gcd(x,y)={\\begin{cases}x&{\\mbox{if }}y=0\\\\gcd(y,\\operatorname {remainder} (x,y))&{\\mbox{if }}y>0\\\\end{cases}} $ Pseudocode (recursive): function gcd is: input: integer x, integer y such that x > 0 and y >= 0 1. if y is 0, return x 2. otherwise, return [ gcd( y, (remainder of x/y) ) ] end gcd Recurrence relation for greatest common divisor, where $ x\\%y $ expresses the remainder of $ x/y $: $ \\gcd(x,y)=\\gcd(y,x\\%y) $ if $ y\\neq 0 $ $ \\gcd(x,0)=x $ Computing the recurrence relation for x = 27 and y = 9: gcd(27, 9) = gcd(9, 27% 9) = gcd(9, 0) = 9 The recursive program above is tail-recursive ; it is equivalent to an iterative algorithm , and the computation shown above shows the steps of evaluation that would be performed by a language that eliminates tail calls . Below is a version of the same algorithm using explicit iteration, suitable for a language that does not eliminate tail calls. By maintaining its state entirely in the variables x and y and using a looping construct, the program avoids making recursive calls and growing the call stack. Pseudocode (iterative): function gcd is: input: integer x, integer y such that x >= y and y >= 0 1. create new variable called remainder 2. begin loop 1. if y is zero, exit loop 2. set remainder to the remainder of x/y 3. set x to y 4. set y to remainder 5. repeat loop 3. return x end gcd The iterative algorithm requires a temporary variable, and even given knowledge of the Euclidean algorithm it is more difficult to understand the process by simple inspection, although the two algorithms are very similar in their steps.","title":"Greatest common divisor"},{"location":"Recursion/Recursion(computer-science)/#towers_of_hanoi","text":"Main article: Towers of Hanoi The Towers of Hanoi is a mathematical puzzle whose solution illustrates recursion.[ 6] [ 7] There are three pegs which can hold stacks of disks of different diameters. A larger disk may never be stacked on top of a smaller. Starting with n disks on one peg, they must be moved to another peg one at a time. What is the smallest number of steps to move the stack? Function definition : $ \\operatorname {hanoi} (n)={\\begin{cases}1&{\\mbox{if }}n=1\\2\\cdot \\operatorname {hanoi} (n-1)+1&{\\mbox{if }}n>1\\\\end{cases}} $ Recurrence relation for hanoi : $ h_{n}=2h_{n-1}+1 $ $ h_{1}=1 $ Computing the recurrence relation for n = 4: hanoi(4) = 2*hanoi(3) + 1 = 2*(2*hanoi(2) + 1) + 1 = 2*(2*(2*hanoi(1) + 1) + 1) + 1 = 2*(2*(2*1 + 1) + 1) + 1 = 2*(2*(3) + 1) + 1 = 2*(7) + 1 = 15 Example implementations:","title":"Towers of Hanoi"},{"location":"Recursion/Recursion(computer-science)/#binary_search","text":"The binary search algorithm is a method of searching a sorted array for a single element by cutting the array in half with each recursive pass. The trick is to pick a midpoint near the center of the array, compare the data at that point with the data being searched and then responding to one of three possible conditions: the data is found at the midpoint, the data at the midpoint is greater than the data being searched for, or the data at the midpoint is less than the data being searched for. Recursion is used in this algorithm because with each pass a new array is created by cutting the old one in half. The binary search procedure is then called recursively, this time on the new (and smaller) array. Typically the array's size is adjusted by manipulating a beginning and ending index. The algorithm exhibits a logarithmic order of growth because it essentially divides the problem domain in half with each pass. Example implementation of binary search in C: /* Call binary_search with proper initial conditions. INPUT: data is an array of integers SORTED in ASCENDING order, toFind is the integer to search for, count is the total number of elements in the array OUTPUT: result of binary_search */ int search(int *data, int toFind, int count) { // Start = 0 (beginning index) // End = count - 1 (top index) return binary_search(data, toFind, 0, count-1); } /* Binary Search Algorithm. INPUT: data is a array of integers SORTED in ASCENDING order, toFind is the integer to search for, start is the minimum array index, end is the maximum array index OUTPUT: position of the integer toFind within array data, -1 if not found */ int binary_search(int *data, int toFind, int start, int end) { //Get the midpoint. int mid = start + (end - start)/2; //Integer division //Stop condition. if (start > end) return -1; else if (data[mid] == toFind) //Found? return mid; else if (data[mid] > toFind) //Data is greater than toFind, search lower half return binary_search(data, toFind, start, mid-1); else //Data is less than toFind, search upper half return binary_search(data, toFind, mid+1, end); }","title":"Binary search"},{"location":"Recursion/Recursion(computer-science)/#recursive_data_structures_structural_recursion","text":"Main article: Recursive data type An important application of recursion in computer science is in defining dynamic data structures such as lists and trees . Recursive data structures can dynamically grow to a theoretically infinite size in response to runtime requirements; in contrast, the size of a static array must be set at compile time. \" Recursive algorithms are particularly appropriate when the underlying problem or the data to be treated are defined in recursive terms.\"[ 9] The examples in this section illustrate what is known as \" structural recursion \". This term refers to the fact that the recursive procedures are acting on data that is defined recursively. As long as a programmer derives the template from a data definition, functions employ structural recursion. That is, the recursions in a function's body consume some immediate piece of a given compound value.[ 5]","title":"Recursive data structures (structural recursion)"},{"location":"Recursion/Recursion(computer-science)/#linked_lists","text":"Main article: Linked list Below is a C definition of a linked list node structure. Notice especially how the node is defined in terms of itself. The \"next\" element of struct node is a pointer to another struct node , effectively creating a list type. struct node { int data; // some integer data struct node *next; // pointer to another struct node }; Because the struct node data structure is defined recursively , procedures that operate on it can be implemented naturally as recursive procedures . The list_print procedure defined below walks down the list until the list is empty (i.e., the list pointer has a value of NULL). For each node it prints the data element (an integer). In the C implementation, the list remains unchanged by the list_print procedure. void list_print(struct node *list) { if (list != NULL) // base case { printf (\"%d \", list->data); // print integer data followed by a space list_print (list->next); // recursive call on the next node } }","title":"Linked lists"},{"location":"Recursion/Recursion(computer-science)/#binary_trees","text":"Main article: Binary tree Below is a simple definition for a binary tree node. Like the node for linked lists, it is defined in terms of itself, recursively. There are two self-referential pointers: left (pointing to the left sub-tree) and right (pointing to the right sub-tree). struct node { int data; // some integer data struct node *left; // pointer to the left subtree struct node *right; // point to the right subtree }; Operations on the tree can be implemented using recursion. Note that because there are two self-referencing pointers (left and right), tree operations may require two recursive calls: // Test if tree_node contains i; return 1 if so, 0 if not. int tree_contains(struct node *tree_node, int i) { if (tree_node == NULL) return 0; // base case else if (tree_node->data == i) return 1; else return tree_contains(tree_node->left, i) || tree_contains(tree_node->right, i); } At most two recursive calls will be made for any given call to tree_contains as defined above. // Inorder traversal: void tree_print(struct node *tree_node) { if (tree_node != NULL) { // base case tree_print(tree_node->left); // go left printf(\"%d \", tree_node->data); // print the integer followed by a space tree_print(tree_node->right); // go right } } 5 3 7 2 4 6 8 P(5) P(3) P(2) P(NULL) return printf(2) P(NULL) return printf(3) p(4) P(NULL) return printf(4) P(NULL) return \u5199\u9012\u5f52\u51fd\u6570\u7684\u6838\u5fc3\u5728\u4e8e\u628a\u6211\u9012\u5f52\u7684\u672c\u8d28\uff1a\u81ea\u9876\u5411\u4e0b\uff0c\u53ea\u6709\u5b50\u95ee\u9898\u90fd\u89e3\u4e86\uff0c\u624d\u80fd\u591f\u89e3\u4e0a\u4e00\u5c42\u7684\u95ee\u9898\u3002\u4f7f\u7528\u7cfb\u7edf\u5806\u6808\u6765\u5b9e\u73b0\u8be5\u8fc7\u7a0b\uff0c\u56e0\u4e3a\u7cfb\u7edf\u5806\u6808\u80fd\u591f\u4e0d\u65ad\u5730\u6309\u7167\u9012\u5f52\u7684\u987a\u5e8f\u8fdb\u884c\u5165\u6808\uff0c\u76f4\u5230\u8fbe\u5230\u6700\u5c0f\u7684\u5b50\u95ee\u9898\uff0c\u4ece\u800c\u5c06\u5b50\u95ee\u9898\u89e3\u51b3\uff0c\u7136\u540e\u51fa\u6808\uff0c\u7136\u540e\u89e3\u51b3\u4e0a\u4e00\u5c42\u5b50\u95ee\u9898\uff0c\u76f4\u81f3\u6700\u9876\u5c42\u7684\u95ee\u9898\u89e3\u51b3\u4e86\uff1b \u6bcf\u4e2a\u9012\u5f52\u8c03\u7528\u90fd\u662f\u4e00\u6761\u76f4\u7ebf\uff0c\u5982\u679c\u9012\u5f52\u51fd\u6570\u4e2d\uff0c\u51fa\u73b0\u4e86\u4e24\u6b21\u9012\u5f52\u8c03\u7528\uff0c\u5219\u5c31\u662f\u4e24\u6761\u7ebf\u4e86\uff1b \u5bf9\u4e8e\u6709\u9012\u5f52\u6027\u8d28\u7684\u95ee\u9898\u6216\u8005\u7ed3\u6784\uff0c\u6211\u4eec\u53ef\u4ee5\u6309\u7167\u4e0a\u8ff0\u9012\u5f52\u7684\u601d\u60f3\u53bb\u7f16\u5199\u7a0b\u5e8f\u3002\u5f53\u8c08\u53ca\u9700\u8981\u9a8c\u8bc1\u6216\u8005\u6a21\u62df\u9012\u5f52\u51fd\u6570\u7684\u6267\u884c\u8fc7\u7a0b\u7684\u65f6\u5019\uff0c\u5c31\u9700\u8981\u4ece\u76f8\u53cd\u7684\u65b9\u5411\u6765\u89e3\u51b3\u95ee\u9898\u4e86\uff0c\u6211\u4eec\u9700\u8981\u4ece\u5e95\u5411\u4e0a\u6765\u6267\u884c\u3002","title":"Binary trees"},{"location":"Recursion/Recursion(computer-science)/#implementation_issues","text":"In actual implementation, rather than a pure recursive function (single check for base case, otherwise recursive step), a number of modifications may be made, for purposes of clarity or efficiency. These include: Wrapper function (at top) Short-circuiting\uff08\u7b80\u5316\uff09 the base case , aka \"Arm's-length recursion\" (at bottom) Hybrid algorithm (at bottom) \u2013 switching to a different algorithm once data is small enough On the basis of elegance, wrapper functions are generally approved, while short-circuiting the base case is frowned upon, particularly in academia. Hybrid algorithms are often used for efficiency, to reduce the overhead of recursion in small cases, and arm's-length recursion is a special case of this.","title":"Implementation issues"},{"location":"Recursion/Recursion(computer-science)/#recursion_versus_iteration","text":"Recursion and iteration are equally expressive: recursion can be replaced by iteration with an explicit call stack , while iteration can be replaced with tail recursion . Which approach is preferable depends on the problem under consideration and the language used. In imperative programming , iteration is preferred, particularly for simple recursion, as it avoids the overhead of function calls and call stack management, but recursion is generally used for multiple recursion . By contrast, in functional languages recursion is preferred, with tail recursion optimization leading to little overhead. Implementing an algorithm using iteration may not be easily achievable. For example, a factorial function may be implemented iteratively in C by assigning to an loop index variable and accumulator variable, rather than by passing arguments and returning values by recursion: unsigned int factorial(unsigned int n) { unsigned int product = 1; // empty product is 1 while (n) { product *= n; --n; } return product; }","title":"Recursion versus iteration"},{"location":"Recursion/Recursion(computer-science)/#multiply_recursive_problems","text":"Multiply recursive problems are inherently recursive, because of prior state they need to track. One example is tree traversal as in depth-first search ; though both recursive and iterative methods are used,[ 19] they contrast with list traversal and linear search in a list, which is a singly recursive and thus naturally iterative method. Other examples include divide-and-conquer algorithms such as Quicksort , and functions such as the Ackermann function . All of these algorithms can be implemented iteratively with the help of an explicit stack , but the programmer effort involved in managing the stack, and the complexity of the resulting program, arguably outweigh any advantages of the iterative solution.","title":"Multiply recursive problems"},{"location":"Recursion/Recursion(computer-science)/#refactoring_recursion","text":"Recursive algorithms can be replaced with non-recursive counterparts.[ 20] . One method for replacing recursive algorithms is to simulate them using heap memory in place of stack memory .[ 21] An alternative is to develop a replacement algorithm entirely based on non-recursive methods, which can be challenging.[ 22] For example, recursive algorithms for matching wildcards , such as Rich Salz ' wildmat algorithm,[ 23] were once typical. Non-recursive algorithms for the same purpose, such as the Krauss matching wildcards algorithm , have been developed to avoid the drawbacks of recursion[ 24] and have improved only gradually based on techniques such as collecting tests and profiling performance.[ 25]","title":"Refactoring recursion"},{"location":"Recursion/Recursion(computer-science)/#tail-recursive_functions","text":"Tail-recursive functions are functions in which all recursive calls are tail calls and hence do not build up any deferred operations. For example, the gcd function (shown again below) is tail-recursive. In contrast, the factorial function (also below) is not tail-recursive; because its recursive call is not in tail position, it builds up deferred multiplication operations that must be performed after the final recursive call completes. With a compiler or interpreter that treats tail-recursive calls as jumps rather than function calls , a tail-recursive function such as gcd will execute using constant space . Thus the program is essentially iterative, equivalent to using imperative language control structures like the \"for\" and \"while\" loops. Tail recursion : //INPUT: Integers x, y such that x >= y and y >= 0 int gcd(int x, int y) { if (y == 0) return x; else return gcd(y, x % y); } Augmenting recursion: //INPUT: n is an Integer such that n >= 0 int fact(int n) { if (n == 0) return 1; else return n * fact(n - 1); } The significance of tail recursion is that when making a tail-recursive call (or any tail call), the caller's return position need not be saved on the call stack ; when the recursive call returns, it will branch directly on the previously saved return position. Therefore, in languages that recognize this property of tail calls, tail recursion saves both space and time.","title":"Tail-recursive functions"},{"location":"Recursion/Recursion(computer-science)/#time-efficiency_of_recursive_algorithms","text":"The time efficiency of recursive algorithms can be expressed in a recurrence relation of Big O notation . They can (usually) then be simplified into a single Big-O term.","title":"Time-efficiency of recursive algorithms"},{"location":"Recursion/Recursion(computer-science)/#shortcut_rule_master_theorem","text":"Main article: Master theorem (analysis of algorithms) If the time-complexity of the function is in the form $ T(n)=a\\cdot T(n/b)+f(n) $ Then the Big O of the time-complexity is thus: If $ f(n)=O(n^{\\log {b}a-\\epsilon }) $ for some constant $ \\epsilon >0 $, then $ T(n)=\\Theta (n^{\\log {b}a}) $ If $ f(n)=\\Theta (n^{\\log {b}a}) $, then $ T(n)=\\Theta (n^{\\log {b}a}\\log n) $ If $ f(n)=\\Omega (n^{\\log _{b}a+\\epsilon }) $ for some constant $ \\epsilon >0 $, and if $ a\\cdot f(n/b)\\leq c\\cdot f(n) $ for some constant c < 1 and all sufficiently large n , then $ T(n)=\\Theta (f(n)) $ where a represents the number of recursive calls at each level of recursion, b represents by what factor smaller the input is for the next level of recursion (i.e. the number of pieces you divide the problem into), and f \u2009( n ) represents the work the function does independent of any recursion (e.g. partitioning, recombining) at each level of recursion.","title":"Shortcut rule (master theorem)"},{"location":"Recursion/Recursion-analysis-and-representation/","text":"\u9012\u5f52\u51fd\u6570\u7684\u8868\u793a\u4e0e\u5206\u6790 \u9012\u5f52\u51fd\u6570\u8c03\u7528\u8fc7\u7a0b\u7684\u8868\u793a Write a program to print all permutations of a given string \u6590\u6ce2\u90a3\u5951\u6570 Matrix Chain Multiplication | DP-8 \u9012\u5f52\u51fd\u6570\u7684\u590d\u6742\u6027\u5206\u6790 Analysis of Algorithm | Set 4 (Solving Recurrences) Lecture 20: Recursion Trees and the Master Method \u9012\u5f52\u51fd\u6570\u7684\u8868\u793a\u4e0e\u5206\u6790 # \u9012\u5f52\u51fd\u6570\u8c03\u7528\u8fc7\u7a0b\u7684\u8868\u793a # \u5bf9\u9012\u5f52\u51fd\u6570\u8fdb\u884c\u590d\u6742\u5ea6\u5206\u6790\u662f\u975e\u5e38\u91cd\u8981\u7684\uff0c\u56e0\u4e3a\u6240\u6709\u4f7f\u7528\u9012\u5f52\u7684\u51fd\u6570\u90fd\u6d89\u53ca\u8fd9\u4e2a\u95ee\u9898\uff1b\u5176\u5b9e\u590d\u6742\u5ea6\u5206\u6790\u672c\u8d28\u4e0a\u6765\u8bf4\u662f\u7edf\u8ba1\u9012\u5f52\u51fd\u6570\u7684\u6267\u884c\u6b21\u6570\u3001\u6267\u884c\u6df1\u5ea6\u7b49\u95ee\u9898\uff0c\u6240\u4ee5\u5982\u679c\u5bf9\u9012\u5f52\u51fd\u6570\u7684\u8c03\u7528\u8fc7\u7a0b\u6709\u4e00\u4e2a\u76f4\u89c2\uff0c\u51c6\u786e\u5730\u63cf\u8ff0\u7684\u8bdd\uff0c\u90a3\u4e48\u5206\u6790\u5176\u9012\u5f52\u51fd\u6570\u7684\u590d\u6742\u5ea6\u4e5f\u4f1a\u975e\u5e38\u5bb9\u6613\uff0c\u76ee\u524d\u6d41\u884c\u7684\u8868\u793a\u65b9\u6cd5\u662f\uff1a\u9012\u5f52\u8c03\u7528\u6811\uff0c\u5982\u4e0b\u662f\u4e00\u4e9b\u4f7f\u7528\u9012\u5f52\u8c03\u7528\u6811\u6765\u8868\u793a\uff1a Write a program to print all permutations of a given string # \u6590\u6ce2\u90a3\u5951\u6570 # \u90a3\u5982\u4f55\u6765\u7406\u89e3 \u9012\u5f52\u8c03\u7528\u6811 \u5462\uff1f\u5b9e\u9645\u4e0a\uff0c \u9012\u5f52\u51fd\u6570 \u7684\u6267\u884c\u8fc7\u7a0b\u5e76\u4e0d\u4f1a\u663e\u793a\u7684\u6784\u9020\u51fa\u4e00\u4e2a \u9012\u5f52\u8c03\u7528\u6811 \uff0c\u5b83\u53ea\u662f\u903b\u8f91\u4e0a\u5f62\u6210\u4e86\u4e00\u4e2a\u6811\uff0c\u4e0b\u9762\u5bf9\u6b64\u8fdb\u884c\u8be6\u7ec6\u7684\u5206\u6790\uff1a \u6211\u4eec\u77e5\u9053\uff0c\u51fd\u6570\u7684\u8c03\u7528\u8fc7\u7a0b\u6240\u4f7f\u7528\u7684\u662f Call stack \uff0c\u6bcf\u4e00\u6b21\u7684\u51fd\u6570\u8c03\u7528\u90fd\u4f1a\u5728 Call stack \u4e0apush\u4e00\u4e2a stack frame \uff08\u53c2\u89c1 Call stack \uff09\uff1b\u9012\u5f52\u51fd\u6570\u4e00\u76f4\u6267\u884c\u7684\u662f\u540c\u4e00\u4e2a\u51fd\u6570\uff0c\u6240\u4ee5\u5b83\u7684 Call stack \u4e2d\u7684 stack frame \u7684\u6267\u884c\u903b\u8f91\u662f\u76f8\u540c\u7684\uff08\u5165\u53c2\u53ef\u80fd\u4e0d\u540c\uff09\uff1b\u5728\u9012\u5f52\u51fd\u6570\u6267\u884c\u7684\u8fc7\u7a0b\u4e2d\uff0c\u6bcf\u6267\u884c\u4e00\u6b21\u9012\u5f52\u8c03\u7528\u5c31\u5f80 Call stack \u4e0apush\uff08\u5165\u6808\uff09\u4e00\u4e2a stack frame \uff0c\u76f4\u5230\u67d0\u4e2a\u9012\u5f52\u51fd\u6570\u6267\u884c\u5230\u4e86base case\uff0c\u5219\u5b83\u4f1areturn\uff0c\u8fd9\u5c31\u610f\u5473\u4e2d\u5b83\u7684 stack frame \u4f1apop\uff08\u51fa\u6808\uff09\uff0c\u5219\u63a7\u5236\u4f1a\u8fd4\u56de\u5230\u8c03\u7528\u5b83\u7684\u51fd\u6570\uff1b\u663e\u7136\uff0c\u524d\u9762\u6240\u63cf\u8ff0\u7684\u8fc7\u7a0b\u5bf9\u5e94\u8fd9 \u6811\u7684\u6df1\u5ea6\u4f18\u5148\u904d\u5386 \uff0c\u6240\u4ee5\u6211\u4eec\u8bf4\uff1a\u9012\u5f52\u51fd\u6570\u7684\u6267\u884c\u8fc7\u7a0b\u662f\u5bf9\u9012\u5f52\u8c03\u7528\u6811\u8fdb\u884c\u6df1\u5ea6\u4f18\u5148\u904d\u5386\u3002 SUMMARY : \u4e0a\u9762\u6240\u63cf\u8ff0\u7684\uff1a\u51fd\u6570\u8c03\u7528-\u5165\u6808\uff0c\u51fd\u6570\u8fd4\u56de-\u51fa\u6808\uff0c\u975e\u5e38\u7c7b\u4f3c\u4e8e\u62ec\u53f7\u5339\u914d\u7b97\u6cd5\u4e2d\u7684\u6b63\u62ec\u53f7\u5165\u6808\uff0c\u53cd\u62ec\u53f7\u51fa\u6808\uff1b \u603b\u7684\u6765\u8bf4\uff0c\u9012\u5f52\u8c03\u7528\u6570\u662f\u5bf9\u9012\u5f52\u51fd\u6570\u7684 Call stack \u7684\u53ef\u89c6\u5316\u5206\u6790\uff1b \u5176\u5b9e\u6211\u4eec\u662f\u5b8c\u5168\u53ef\u4ee5\u6839\u636e \u9012\u5f52\u51fd\u6570 \u753b\u51fa\u5bf9\u5e94\u7684 \u9012\u5f52\u8c03\u7528\u6811 \u7684\u3002\u6bd4\u5982 perm \u51fd\u6570\u5c31\u662f\u5178\u578b\u7684 \u6392\u5217\u6811 \uff0c\u4e8c\u5206\u641c\u7d22\u3001quike sort\u7b49\u5c31\u662f\u5178\u578b\u7684 \u4e8c\u53c9\u6811 \u3002 Matrix Chain Multiplication | DP-8 # \u9012\u5f52\u51fd\u6570\u7684\u590d\u6742\u6027\u5206\u6790 # Analysis of Algorithm | Set 4 (Solving Recurrences) # Lecture 20: Recursion Trees and the Master Method #","title":"Recursion-analysis-and-representation"},{"location":"Recursion/Recursion-analysis-and-representation/#_1","text":"","title":"\u9012\u5f52\u51fd\u6570\u7684\u8868\u793a\u4e0e\u5206\u6790"},{"location":"Recursion/Recursion-analysis-and-representation/#_2","text":"\u5bf9\u9012\u5f52\u51fd\u6570\u8fdb\u884c\u590d\u6742\u5ea6\u5206\u6790\u662f\u975e\u5e38\u91cd\u8981\u7684\uff0c\u56e0\u4e3a\u6240\u6709\u4f7f\u7528\u9012\u5f52\u7684\u51fd\u6570\u90fd\u6d89\u53ca\u8fd9\u4e2a\u95ee\u9898\uff1b\u5176\u5b9e\u590d\u6742\u5ea6\u5206\u6790\u672c\u8d28\u4e0a\u6765\u8bf4\u662f\u7edf\u8ba1\u9012\u5f52\u51fd\u6570\u7684\u6267\u884c\u6b21\u6570\u3001\u6267\u884c\u6df1\u5ea6\u7b49\u95ee\u9898\uff0c\u6240\u4ee5\u5982\u679c\u5bf9\u9012\u5f52\u51fd\u6570\u7684\u8c03\u7528\u8fc7\u7a0b\u6709\u4e00\u4e2a\u76f4\u89c2\uff0c\u51c6\u786e\u5730\u63cf\u8ff0\u7684\u8bdd\uff0c\u90a3\u4e48\u5206\u6790\u5176\u9012\u5f52\u51fd\u6570\u7684\u590d\u6742\u5ea6\u4e5f\u4f1a\u975e\u5e38\u5bb9\u6613\uff0c\u76ee\u524d\u6d41\u884c\u7684\u8868\u793a\u65b9\u6cd5\u662f\uff1a\u9012\u5f52\u8c03\u7528\u6811\uff0c\u5982\u4e0b\u662f\u4e00\u4e9b\u4f7f\u7528\u9012\u5f52\u8c03\u7528\u6811\u6765\u8868\u793a\uff1a","title":"\u9012\u5f52\u51fd\u6570\u8c03\u7528\u8fc7\u7a0b\u7684\u8868\u793a"},{"location":"Recursion/Recursion-analysis-and-representation/#write_a_program_to_print_all_permutations_of_a_given_string","text":"","title":"Write a program to print all permutations of a given string"},{"location":"Recursion/Recursion-analysis-and-representation/#_3","text":"\u90a3\u5982\u4f55\u6765\u7406\u89e3 \u9012\u5f52\u8c03\u7528\u6811 \u5462\uff1f\u5b9e\u9645\u4e0a\uff0c \u9012\u5f52\u51fd\u6570 \u7684\u6267\u884c\u8fc7\u7a0b\u5e76\u4e0d\u4f1a\u663e\u793a\u7684\u6784\u9020\u51fa\u4e00\u4e2a \u9012\u5f52\u8c03\u7528\u6811 \uff0c\u5b83\u53ea\u662f\u903b\u8f91\u4e0a\u5f62\u6210\u4e86\u4e00\u4e2a\u6811\uff0c\u4e0b\u9762\u5bf9\u6b64\u8fdb\u884c\u8be6\u7ec6\u7684\u5206\u6790\uff1a \u6211\u4eec\u77e5\u9053\uff0c\u51fd\u6570\u7684\u8c03\u7528\u8fc7\u7a0b\u6240\u4f7f\u7528\u7684\u662f Call stack \uff0c\u6bcf\u4e00\u6b21\u7684\u51fd\u6570\u8c03\u7528\u90fd\u4f1a\u5728 Call stack \u4e0apush\u4e00\u4e2a stack frame \uff08\u53c2\u89c1 Call stack \uff09\uff1b\u9012\u5f52\u51fd\u6570\u4e00\u76f4\u6267\u884c\u7684\u662f\u540c\u4e00\u4e2a\u51fd\u6570\uff0c\u6240\u4ee5\u5b83\u7684 Call stack \u4e2d\u7684 stack frame \u7684\u6267\u884c\u903b\u8f91\u662f\u76f8\u540c\u7684\uff08\u5165\u53c2\u53ef\u80fd\u4e0d\u540c\uff09\uff1b\u5728\u9012\u5f52\u51fd\u6570\u6267\u884c\u7684\u8fc7\u7a0b\u4e2d\uff0c\u6bcf\u6267\u884c\u4e00\u6b21\u9012\u5f52\u8c03\u7528\u5c31\u5f80 Call stack \u4e0apush\uff08\u5165\u6808\uff09\u4e00\u4e2a stack frame \uff0c\u76f4\u5230\u67d0\u4e2a\u9012\u5f52\u51fd\u6570\u6267\u884c\u5230\u4e86base case\uff0c\u5219\u5b83\u4f1areturn\uff0c\u8fd9\u5c31\u610f\u5473\u4e2d\u5b83\u7684 stack frame \u4f1apop\uff08\u51fa\u6808\uff09\uff0c\u5219\u63a7\u5236\u4f1a\u8fd4\u56de\u5230\u8c03\u7528\u5b83\u7684\u51fd\u6570\uff1b\u663e\u7136\uff0c\u524d\u9762\u6240\u63cf\u8ff0\u7684\u8fc7\u7a0b\u5bf9\u5e94\u8fd9 \u6811\u7684\u6df1\u5ea6\u4f18\u5148\u904d\u5386 \uff0c\u6240\u4ee5\u6211\u4eec\u8bf4\uff1a\u9012\u5f52\u51fd\u6570\u7684\u6267\u884c\u8fc7\u7a0b\u662f\u5bf9\u9012\u5f52\u8c03\u7528\u6811\u8fdb\u884c\u6df1\u5ea6\u4f18\u5148\u904d\u5386\u3002 SUMMARY : \u4e0a\u9762\u6240\u63cf\u8ff0\u7684\uff1a\u51fd\u6570\u8c03\u7528-\u5165\u6808\uff0c\u51fd\u6570\u8fd4\u56de-\u51fa\u6808\uff0c\u975e\u5e38\u7c7b\u4f3c\u4e8e\u62ec\u53f7\u5339\u914d\u7b97\u6cd5\u4e2d\u7684\u6b63\u62ec\u53f7\u5165\u6808\uff0c\u53cd\u62ec\u53f7\u51fa\u6808\uff1b \u603b\u7684\u6765\u8bf4\uff0c\u9012\u5f52\u8c03\u7528\u6570\u662f\u5bf9\u9012\u5f52\u51fd\u6570\u7684 Call stack \u7684\u53ef\u89c6\u5316\u5206\u6790\uff1b \u5176\u5b9e\u6211\u4eec\u662f\u5b8c\u5168\u53ef\u4ee5\u6839\u636e \u9012\u5f52\u51fd\u6570 \u753b\u51fa\u5bf9\u5e94\u7684 \u9012\u5f52\u8c03\u7528\u6811 \u7684\u3002\u6bd4\u5982 perm \u51fd\u6570\u5c31\u662f\u5178\u578b\u7684 \u6392\u5217\u6811 \uff0c\u4e8c\u5206\u641c\u7d22\u3001quike sort\u7b49\u5c31\u662f\u5178\u578b\u7684 \u4e8c\u53c9\u6811 \u3002","title":"\u6590\u6ce2\u90a3\u5951\u6570"},{"location":"Recursion/Recursion-analysis-and-representation/#matrix_chain_multiplication_dp-8","text":"","title":"Matrix Chain Multiplication | DP-8"},{"location":"Recursion/Recursion-analysis-and-representation/#_4","text":"","title":"\u9012\u5f52\u51fd\u6570\u7684\u590d\u6742\u6027\u5206\u6790"},{"location":"Recursion/Recursion-analysis-and-representation/#analysis_of_algorithm_set_4_solving_recurrences","text":"","title":"Analysis of Algorithm | Set 4 (Solving Recurrences)"},{"location":"Recursion/Recursion-analysis-and-representation/#lecture_20_recursion_trees_and_the_master_method","text":"","title":"Lecture 20: Recursion Trees and the Master Method"},{"location":"Recursion/Recursion-to-iteration/","text":"\u524d\u8a00 \u5c3e\u9012\u5f52\u6d88\u9664 using user stack to replace the call stack of recursion function recursion\u7684\u5b9e\u73b0 iteration\u7684\u5b9e\u73b0 recursion\u7684\u5b9e\u73b0 VS iteration\u7684\u5b9e\u73b0 \u524d\u8a00 # \u5c06recursion\u8f6c\u6362\u4e3aiteration\uff0c\u672c\u8d28\u4e0a\u6765\u8bf4\u662f\u5c06recursion\u7684\u81ea\u9876\u5411\u4e0b\u7684\u65b9\u5f0f\u8f6c\u6362\u4e3a\u81ea\u5e95\u5411\u4e0a\u5730\u4f7f\u7528\u9012\u5f52\u5173\u7cfb\uff1b \u5c3e\u9012\u5f52\u6d88\u9664 # \u5c3e\u9012\u5f52\u51fd\u6570\u53ef\u4ee5\u65e0\u9700\u501f\u52a9data structure\u5c31\u53ef\u4ee5\u6d88\u9664\uff0c\u5982fibnacci\u3002 using user stack to replace the call stack of recursion function # \u5176\u4ed6\u7684\u9012\u5f52\u51fd\u6570\u5982tree\u904d\u5386\u51fd\u6570\uff0c\u4e0d\u662f\u5c3e\u9012\u5f52\uff0c\u9700\u8981\u501f\u52a9\u4e8edata structure\u624d\u80fd\u591f\u6d88\u9664\u3002 \u5173\u4e8e\u6b64\u7684\u4e00\u4e2a\u5178\u578b\u7684\u4f8b\u5b50\u5c31\u662f Tree traversal \uff0c\u4ee5\u4e0b\u662f\u4ece\u8fd9\u6458\u6284\u7684code\uff1a recursion\u7684\u5b9e\u73b0 # preorder(node) if (node == null) return visit(node) preorder(node.left) preorder(node.right) iteration\u7684\u5b9e\u73b0 # iterativePreorder(node) if (node == null) return s \u2190 empty stack s.push(node) while (not s.isEmpty()) node \u2190 s.pop() visit(node) //right child is pushed first so that left is processed first if (node.right \u2260 null) s.push(node.right) if (node.left \u2260 null) s.push(node.left) recursion\u7684\u5b9e\u73b0 VS iteration\u7684\u5b9e\u73b0 # \u6b63\u5982\u5728\u300a recursion-analysis-and-representation.md \u300b\u4e2d\u6240\u63cf\u8ff0\u7684\uff1a \u51fd\u6570\u7684\u8c03\u7528\u8fc7\u7a0b\u6240\u4f7f\u7528\u7684\u662f Call stack \uff0c\u6bcf\u4e00\u6b21\u7684\u51fd\u6570\u8c03\u7528\u90fd\u4f1a\u5728 Call stack \u4e0apush\u4e00\u4e2a stack frame \uff08\u53c2\u89c1 Call stack \uff09\uff1b\u9012\u5f52\u51fd\u6570\u4e00\u76f4\u6267\u884c\u7684\u662f\u540c\u4e00\u4e2a\u51fd\u6570\uff0c\u6240\u4ee5\u5b83\u7684 Call stack \u4e2d\u7684 stack frame \u7684\u6267\u884c\u903b\u8f91\u662f\u76f8\u540c\u7684\uff08\u5165\u53c2\u53ef\u80fd\u4e0d\u540c\uff09\uff1b\u5728\u9012\u5f52\u51fd\u6570\u6267\u884c\u7684\u8fc7\u7a0b\u4e2d\uff0c\u6bcf\u6267\u884c\u4e00\u6b21\u9012\u5f52\u8c03\u7528\u5c31\u5f80 Call stack \u4e0apush\uff08\u5165\u6808\uff09\u4e00\u4e2a stack frame \uff0c\u76f4\u5230\u67d0\u4e2a\u9012\u5f52\u51fd\u6570\u6267\u884c\u5230\u4e86base case\uff0c\u5219\u5b83\u4f1areturn\uff0c\u8fd9\u5c31\u610f\u5473\u4e2d\u5b83\u7684 stack frame \u4f1apop\uff08\u51fa\u6808\uff09\uff0c\u5219\u63a7\u5236\u4f1a\u8fd4\u56de\u5230\u8c03\u7528\u5b83\u7684\u51fd\u6570\uff1b\u663e\u7136\uff0c\u524d\u9762\u6240\u63cf\u8ff0\u7684\u8fc7\u7a0b\u5bf9\u5e94\u7740 \u6811\u7684\u6df1\u5ea6\u4f18\u5148\u904d\u5386 \uff0c\u6240\u4ee5\u6211\u4eec\u8bf4\uff1a\u9012\u5f52\u51fd\u6570\u7684\u6267\u884c\u8fc7\u7a0b\u662f\u5bf9\u9012\u5f52\u8c03\u7528\u6811\u8fdb\u884c\u6df1\u5ea6\u4f18\u5148\u904d\u5386\u3002 \u5728\u9012\u5f52\u7684\u5b9e\u73b0\u4e2d\uff0c\u51fd\u6570 preorder \u7684\u6267\u884c\u8fc7\u7a0b\u4e2d\uff0c\u4f1a\u4e0d\u65ad\u5730\u5f80 Call stack \u4e2dpush stack frame\uff0c\u8fd9\u4e9bstack frame\u4e2d\u5c31\u5305\u542b\u4e86\u4e00\u4e2a\u4e00\u4e2a\u7684 \u8282\u70b9 \u4fe1\u606f\uff0c\u76f4\u81f3\u9047\u5230base case\uff0c Call stack \u4e2d\u7684stack frame\u624d\u4f1a\u51fa\u6808\uff1b\u73b0\u5728\u53ea\u5206\u6790 preorder \u4e2d\u5173\u4e8e preorder \u7684\u8c03\u7528\uff0c\u5728\u6bcf\u4e00\u6b21\u8c03\u7528\u4e2d\u4f1a\u5148\u6267\u884c preorder(node.left) \uff0c\u8fd9\u8bf4\u660e preorder \u4f1a\u4f18\u5148\u5904\u7406 node.left \uff0c\u800c\u5c06 node.right \u7559\u5728stack frame\u4e2d\uff08\u51fd\u6570\u6682\u65f6\u6ca1\u6709\u6267\u884c\u5230\u8fd9\u91cc\uff0c\u5f85 preorder(node.left) \u8fd4\u56de\u540e\uff0c\u624d\u4f1a\u6267\u884c\u5230\u5b83\uff09\uff0c\u800c\u5f85\u63a7\u5236\u8fd4\u56de\u65f6\u624d\u8fdb\u884c\u5904\u7406\uff1b\u8fd9\u5c31\u662f\u4f7f\u7528call stack\u6765\u4fdd\u5b58deferred node\uff0c\u6b63\u5982 Tree traversal \u4e2d\u6240\u8ff0\uff1a Traversing a tree involves iterating over all nodes in some manner. Because from a given node there is more than one possible next node (it is not a linear data structure), then, assuming sequential computation (not parallel), some nodes must be deferred\u2014stored in some way for later visiting. This is often done via a stack (LIFO) or queue (FIFO). As a tree is a self-referential (recursively defined) data structure, traversal can be defined by recursion or, more subtly, corecursion , in a very natural and clear fashion; in these cases the deferred nodes are stored implicitly in the call stack . \u663e\u7136\uff0c\u5728\u4f7f\u7528iteration\u7684\u65f6\u5019\uff0c\u5c31\u9700\u8981\u7528\u6237\u5b9a\u4e49\u4e00\u4e2astack\u6765\u5145\u5f53calling stack\u5728recursion\u4e2d\u7684\u89d2\u8272\u4e86\uff1a\u5728recursion\u4e2d\uff0c preorder(node.left) \u7684\u6267\u884c\u5728 preorder(node.right) \u4e4b\u524d\uff0c\u8fd9\u5c31\u610f\u5473\u4e2d\uff0c\u4f18\u5148\u5904\u7406 node.left \uff0c\u6240\u4ee5\u5728iteration\u7684\u5b9e\u73b0\u4e2d\uff0c\u8981\u5148\u5165\u6808 node.right \uff0c\u518d\u5165\u6808 node.left \uff08\u56e0\u4e3astack\u662f\u540e\u8fdb\u5148\u51fa\uff09\uff0c\u4ece\u800c\u6a21\u62df\u4e86\u4e0a\u8ff0\u7cfb\u7edf\u8c03\u7528\u4e2d\u5c06 node.right \u4fdd\u5b58\u5728call stack\u4e2d\u3002 \u518d\u6765\u770b\u770b \u4e2d\u5e8f\u904d\u5386 inorder(node) if (node == null) return inorder(node.left) visit(node) inorder(node.right) \u4e0d\u65ad\u5730\u5c06\u5de6\u8282\u70b9\u538b\u5165call stack\u4e2d\uff0c\u77e5\u9053\u5de6\u8282\u70b9\u4e3aNULL\uff0c\u624d\u8fd4\u56de\uff08\u51fa\u6808\uff09\uff0c\u624dvisit\u8282\u70b9\uff1b iterativeInorder(node) s \u2190 empty stack while (not s.isEmpty() or node \u2260 null) if (node \u2260 null) s.push(node) node \u2190 node.left else node \u2190 s.pop() visit(node) node \u2190 node.right \u4e2d\u5e8f\u904d\u5386\u7684recursion\u7248\u672c\u8981\u6bd4\u5148\u5e8f\u904d\u5386\u7684\u590d\u6742\u5730\u591a\u3002\u5176\u5b9e\u5bf9\u4ed6\u7684\u5206\u6790\u8fd8\u662f\u8981\u4ece\u4ececall stack\u5230user stack\u3002\u4e2d\u5e8f\u904d\u5386\u4e2d\uff0c\u5f53\u9012\u5f52\u51fd\u6570\u8fd4\u56de\u5230\u5b83\u7684\u4e3b\u8c03\u51fd\u6570\u7684\u65f6\u5019\uff0c\u8fd8\u9700\u8981\u8bbf\u95ee\u8282\u70b9 node \uff08\u5728call stack\u4e2d\u4fdd\u5b58\u4e86\u8fd9\u4e9b\u4fe1\u606f\uff09\uff0c\u4f46\u662f\u5728\u5148\u5e8f\u904d\u5386\u4e2d\uff0c\u5f53\u9012\u5f52\u51fd\u6570\u8fd4\u56de\u5230\u5b83\u7684\u4e3b\u8c03\u51fd\u6570\u7684\u65f6\u5019\uff0c\u65e0\u9700\u5728\u8bbf\u95ee\u8282\u70b9 node \u4e86\uff1b\u6240\u4ee5\u5728\u5148\u5e8f\u904d\u5386\u7684iteration\u7248\u672c\u4e2d\uff0c node \u8282\u70b9\u5728\u672c\u8f6e\u4f7f\u7528\u4e86\u4e4b\u540e\uff0c\u5c31\u65e0\u9700\u7ee7\u7eed\u7559\u5728user stack\u4e2d\u4e86\uff0c\u6240\u4ee5\u5728\u6bcf\u8f6e\u7684\u5faa\u73af\u5f00\u59cb\u7684\u65f6\u5019\uff0c\u5c31\u5c06\u5b83\u4eceuser stack\u4e2d\u53d6\u51fa\uff1b\u4f46\u662f\u4e2d\u5e8f\u904d\u5386\u7684iteration\u7248\u672c\u4e2d\uff0c\u5c31\u4e0d\u80fd\u591f\u8fd9\u6837\u4e86\uff0c\u53ea\u6709\u5f53node\u6ca1\u6709\u4e86left node\u540e\uff0c\u624d\u80fd\u591f\u5c06\u5b83\u4eceuser stack\u4e2d\u53d6\u51fa\uff1b user stack\u7684\u51fa\u6808\u5bf9\u5e94\u7740call stack\u4e2d\u7684\u4ece\u9012\u5f52\u51fd\u6570\u4e2d\u8fd4\u56de\uff0cuser stack\u4e2d\u8fdb\u6808\u5219\u5bf9\u5e94\u4e2d\u8c03\u7528\u9012\u5f52\u51fd\u6570\uff1b \u5bf9\u6bd4\u4e0a\u8ff0\u4ee3\u7801\uff0c\u53ef\u4ee5\u53d1\u73b0\uff0c\u5982\u4e0b\u4e24\u4e2a\u8bed\u53e5\u662f\u5728\u4e00\u8d77\u7684\uff1a node \u2190 s.pop() visit(node) \u5176\u5b9e\u8fd9\u662f\u4e00\u4e2a\u7406\u89e3\u95ee\u9898\u672c\u8d28\u7684\u6240\u5728\uff0c\u65e0\u8bba\u54ea\u79cd\u65b9\u5f0f\uff0c\u4ece\u6808\u4e2d\u53d6\u51fa\u5143\u7d20\uff0c\u7136\u540e\u8fdb\u884cvisit\uff0c\u4e0d\u540c\u7684\u662f\u6df1\u5ea6\u4f18\u5148\u5148\u5e8f\u904d\u5386\u662f\u5728\u6bcf\u6b21\u5148\u4ece\u6808\u4e2d\u53d6\u51fa\u5143\u7d20\u8fdb\u884cvisit\u3002\u6df1\u5ea6\u4f18\u5148\u7684\u4e2d\u5e8f\u904d\u5386\u5219\u662f\u5728\u5de6\u5b50\u6811\u90fd\u8bbf\u95ee\u5b8c\u4e86\u540e\u624d\u4ece\u6808\u4e2d\u53d6\u51fa\u5143\u7d20\u8fdb\u884cvisit\u3002","title":"Recursion-to-iteration"},{"location":"Recursion/Recursion-to-iteration/#_1","text":"\u5c06recursion\u8f6c\u6362\u4e3aiteration\uff0c\u672c\u8d28\u4e0a\u6765\u8bf4\u662f\u5c06recursion\u7684\u81ea\u9876\u5411\u4e0b\u7684\u65b9\u5f0f\u8f6c\u6362\u4e3a\u81ea\u5e95\u5411\u4e0a\u5730\u4f7f\u7528\u9012\u5f52\u5173\u7cfb\uff1b","title":"\u524d\u8a00"},{"location":"Recursion/Recursion-to-iteration/#_2","text":"\u5c3e\u9012\u5f52\u51fd\u6570\u53ef\u4ee5\u65e0\u9700\u501f\u52a9data structure\u5c31\u53ef\u4ee5\u6d88\u9664\uff0c\u5982fibnacci\u3002","title":"\u5c3e\u9012\u5f52\u6d88\u9664"},{"location":"Recursion/Recursion-to-iteration/#using_user_stack_to_replace_the_call_stack_of_recursion_function","text":"\u5176\u4ed6\u7684\u9012\u5f52\u51fd\u6570\u5982tree\u904d\u5386\u51fd\u6570\uff0c\u4e0d\u662f\u5c3e\u9012\u5f52\uff0c\u9700\u8981\u501f\u52a9\u4e8edata structure\u624d\u80fd\u591f\u6d88\u9664\u3002 \u5173\u4e8e\u6b64\u7684\u4e00\u4e2a\u5178\u578b\u7684\u4f8b\u5b50\u5c31\u662f Tree traversal \uff0c\u4ee5\u4e0b\u662f\u4ece\u8fd9\u6458\u6284\u7684code\uff1a","title":"using user stack to replace the call stack of recursion function"},{"location":"Recursion/Recursion-to-iteration/#recursion","text":"preorder(node) if (node == null) return visit(node) preorder(node.left) preorder(node.right)","title":"recursion\u7684\u5b9e\u73b0"},{"location":"Recursion/Recursion-to-iteration/#iteration","text":"iterativePreorder(node) if (node == null) return s \u2190 empty stack s.push(node) while (not s.isEmpty()) node \u2190 s.pop() visit(node) //right child is pushed first so that left is processed first if (node.right \u2260 null) s.push(node.right) if (node.left \u2260 null) s.push(node.left)","title":"iteration\u7684\u5b9e\u73b0"},{"location":"Recursion/Recursion-to-iteration/#recursion_vs_iteration","text":"\u6b63\u5982\u5728\u300a recursion-analysis-and-representation.md \u300b\u4e2d\u6240\u63cf\u8ff0\u7684\uff1a \u51fd\u6570\u7684\u8c03\u7528\u8fc7\u7a0b\u6240\u4f7f\u7528\u7684\u662f Call stack \uff0c\u6bcf\u4e00\u6b21\u7684\u51fd\u6570\u8c03\u7528\u90fd\u4f1a\u5728 Call stack \u4e0apush\u4e00\u4e2a stack frame \uff08\u53c2\u89c1 Call stack \uff09\uff1b\u9012\u5f52\u51fd\u6570\u4e00\u76f4\u6267\u884c\u7684\u662f\u540c\u4e00\u4e2a\u51fd\u6570\uff0c\u6240\u4ee5\u5b83\u7684 Call stack \u4e2d\u7684 stack frame \u7684\u6267\u884c\u903b\u8f91\u662f\u76f8\u540c\u7684\uff08\u5165\u53c2\u53ef\u80fd\u4e0d\u540c\uff09\uff1b\u5728\u9012\u5f52\u51fd\u6570\u6267\u884c\u7684\u8fc7\u7a0b\u4e2d\uff0c\u6bcf\u6267\u884c\u4e00\u6b21\u9012\u5f52\u8c03\u7528\u5c31\u5f80 Call stack \u4e0apush\uff08\u5165\u6808\uff09\u4e00\u4e2a stack frame \uff0c\u76f4\u5230\u67d0\u4e2a\u9012\u5f52\u51fd\u6570\u6267\u884c\u5230\u4e86base case\uff0c\u5219\u5b83\u4f1areturn\uff0c\u8fd9\u5c31\u610f\u5473\u4e2d\u5b83\u7684 stack frame \u4f1apop\uff08\u51fa\u6808\uff09\uff0c\u5219\u63a7\u5236\u4f1a\u8fd4\u56de\u5230\u8c03\u7528\u5b83\u7684\u51fd\u6570\uff1b\u663e\u7136\uff0c\u524d\u9762\u6240\u63cf\u8ff0\u7684\u8fc7\u7a0b\u5bf9\u5e94\u7740 \u6811\u7684\u6df1\u5ea6\u4f18\u5148\u904d\u5386 \uff0c\u6240\u4ee5\u6211\u4eec\u8bf4\uff1a\u9012\u5f52\u51fd\u6570\u7684\u6267\u884c\u8fc7\u7a0b\u662f\u5bf9\u9012\u5f52\u8c03\u7528\u6811\u8fdb\u884c\u6df1\u5ea6\u4f18\u5148\u904d\u5386\u3002 \u5728\u9012\u5f52\u7684\u5b9e\u73b0\u4e2d\uff0c\u51fd\u6570 preorder \u7684\u6267\u884c\u8fc7\u7a0b\u4e2d\uff0c\u4f1a\u4e0d\u65ad\u5730\u5f80 Call stack \u4e2dpush stack frame\uff0c\u8fd9\u4e9bstack frame\u4e2d\u5c31\u5305\u542b\u4e86\u4e00\u4e2a\u4e00\u4e2a\u7684 \u8282\u70b9 \u4fe1\u606f\uff0c\u76f4\u81f3\u9047\u5230base case\uff0c Call stack \u4e2d\u7684stack frame\u624d\u4f1a\u51fa\u6808\uff1b\u73b0\u5728\u53ea\u5206\u6790 preorder \u4e2d\u5173\u4e8e preorder \u7684\u8c03\u7528\uff0c\u5728\u6bcf\u4e00\u6b21\u8c03\u7528\u4e2d\u4f1a\u5148\u6267\u884c preorder(node.left) \uff0c\u8fd9\u8bf4\u660e preorder \u4f1a\u4f18\u5148\u5904\u7406 node.left \uff0c\u800c\u5c06 node.right \u7559\u5728stack frame\u4e2d\uff08\u51fd\u6570\u6682\u65f6\u6ca1\u6709\u6267\u884c\u5230\u8fd9\u91cc\uff0c\u5f85 preorder(node.left) \u8fd4\u56de\u540e\uff0c\u624d\u4f1a\u6267\u884c\u5230\u5b83\uff09\uff0c\u800c\u5f85\u63a7\u5236\u8fd4\u56de\u65f6\u624d\u8fdb\u884c\u5904\u7406\uff1b\u8fd9\u5c31\u662f\u4f7f\u7528call stack\u6765\u4fdd\u5b58deferred node\uff0c\u6b63\u5982 Tree traversal \u4e2d\u6240\u8ff0\uff1a Traversing a tree involves iterating over all nodes in some manner. Because from a given node there is more than one possible next node (it is not a linear data structure), then, assuming sequential computation (not parallel), some nodes must be deferred\u2014stored in some way for later visiting. This is often done via a stack (LIFO) or queue (FIFO). As a tree is a self-referential (recursively defined) data structure, traversal can be defined by recursion or, more subtly, corecursion , in a very natural and clear fashion; in these cases the deferred nodes are stored implicitly in the call stack . \u663e\u7136\uff0c\u5728\u4f7f\u7528iteration\u7684\u65f6\u5019\uff0c\u5c31\u9700\u8981\u7528\u6237\u5b9a\u4e49\u4e00\u4e2astack\u6765\u5145\u5f53calling stack\u5728recursion\u4e2d\u7684\u89d2\u8272\u4e86\uff1a\u5728recursion\u4e2d\uff0c preorder(node.left) \u7684\u6267\u884c\u5728 preorder(node.right) \u4e4b\u524d\uff0c\u8fd9\u5c31\u610f\u5473\u4e2d\uff0c\u4f18\u5148\u5904\u7406 node.left \uff0c\u6240\u4ee5\u5728iteration\u7684\u5b9e\u73b0\u4e2d\uff0c\u8981\u5148\u5165\u6808 node.right \uff0c\u518d\u5165\u6808 node.left \uff08\u56e0\u4e3astack\u662f\u540e\u8fdb\u5148\u51fa\uff09\uff0c\u4ece\u800c\u6a21\u62df\u4e86\u4e0a\u8ff0\u7cfb\u7edf\u8c03\u7528\u4e2d\u5c06 node.right \u4fdd\u5b58\u5728call stack\u4e2d\u3002 \u518d\u6765\u770b\u770b \u4e2d\u5e8f\u904d\u5386 inorder(node) if (node == null) return inorder(node.left) visit(node) inorder(node.right) \u4e0d\u65ad\u5730\u5c06\u5de6\u8282\u70b9\u538b\u5165call stack\u4e2d\uff0c\u77e5\u9053\u5de6\u8282\u70b9\u4e3aNULL\uff0c\u624d\u8fd4\u56de\uff08\u51fa\u6808\uff09\uff0c\u624dvisit\u8282\u70b9\uff1b iterativeInorder(node) s \u2190 empty stack while (not s.isEmpty() or node \u2260 null) if (node \u2260 null) s.push(node) node \u2190 node.left else node \u2190 s.pop() visit(node) node \u2190 node.right \u4e2d\u5e8f\u904d\u5386\u7684recursion\u7248\u672c\u8981\u6bd4\u5148\u5e8f\u904d\u5386\u7684\u590d\u6742\u5730\u591a\u3002\u5176\u5b9e\u5bf9\u4ed6\u7684\u5206\u6790\u8fd8\u662f\u8981\u4ece\u4ececall stack\u5230user stack\u3002\u4e2d\u5e8f\u904d\u5386\u4e2d\uff0c\u5f53\u9012\u5f52\u51fd\u6570\u8fd4\u56de\u5230\u5b83\u7684\u4e3b\u8c03\u51fd\u6570\u7684\u65f6\u5019\uff0c\u8fd8\u9700\u8981\u8bbf\u95ee\u8282\u70b9 node \uff08\u5728call stack\u4e2d\u4fdd\u5b58\u4e86\u8fd9\u4e9b\u4fe1\u606f\uff09\uff0c\u4f46\u662f\u5728\u5148\u5e8f\u904d\u5386\u4e2d\uff0c\u5f53\u9012\u5f52\u51fd\u6570\u8fd4\u56de\u5230\u5b83\u7684\u4e3b\u8c03\u51fd\u6570\u7684\u65f6\u5019\uff0c\u65e0\u9700\u5728\u8bbf\u95ee\u8282\u70b9 node \u4e86\uff1b\u6240\u4ee5\u5728\u5148\u5e8f\u904d\u5386\u7684iteration\u7248\u672c\u4e2d\uff0c node \u8282\u70b9\u5728\u672c\u8f6e\u4f7f\u7528\u4e86\u4e4b\u540e\uff0c\u5c31\u65e0\u9700\u7ee7\u7eed\u7559\u5728user stack\u4e2d\u4e86\uff0c\u6240\u4ee5\u5728\u6bcf\u8f6e\u7684\u5faa\u73af\u5f00\u59cb\u7684\u65f6\u5019\uff0c\u5c31\u5c06\u5b83\u4eceuser stack\u4e2d\u53d6\u51fa\uff1b\u4f46\u662f\u4e2d\u5e8f\u904d\u5386\u7684iteration\u7248\u672c\u4e2d\uff0c\u5c31\u4e0d\u80fd\u591f\u8fd9\u6837\u4e86\uff0c\u53ea\u6709\u5f53node\u6ca1\u6709\u4e86left node\u540e\uff0c\u624d\u80fd\u591f\u5c06\u5b83\u4eceuser stack\u4e2d\u53d6\u51fa\uff1b user stack\u7684\u51fa\u6808\u5bf9\u5e94\u7740call stack\u4e2d\u7684\u4ece\u9012\u5f52\u51fd\u6570\u4e2d\u8fd4\u56de\uff0cuser stack\u4e2d\u8fdb\u6808\u5219\u5bf9\u5e94\u4e2d\u8c03\u7528\u9012\u5f52\u51fd\u6570\uff1b \u5bf9\u6bd4\u4e0a\u8ff0\u4ee3\u7801\uff0c\u53ef\u4ee5\u53d1\u73b0\uff0c\u5982\u4e0b\u4e24\u4e2a\u8bed\u53e5\u662f\u5728\u4e00\u8d77\u7684\uff1a node \u2190 s.pop() visit(node) \u5176\u5b9e\u8fd9\u662f\u4e00\u4e2a\u7406\u89e3\u95ee\u9898\u672c\u8d28\u7684\u6240\u5728\uff0c\u65e0\u8bba\u54ea\u79cd\u65b9\u5f0f\uff0c\u4ece\u6808\u4e2d\u53d6\u51fa\u5143\u7d20\uff0c\u7136\u540e\u8fdb\u884cvisit\uff0c\u4e0d\u540c\u7684\u662f\u6df1\u5ea6\u4f18\u5148\u5148\u5e8f\u904d\u5386\u662f\u5728\u6bcf\u6b21\u5148\u4ece\u6808\u4e2d\u53d6\u51fa\u5143\u7d20\u8fdb\u884cvisit\u3002\u6df1\u5ea6\u4f18\u5148\u7684\u4e2d\u5e8f\u904d\u5386\u5219\u662f\u5728\u5de6\u5b50\u6811\u90fd\u8bbf\u95ee\u5b8c\u4e86\u540e\u624d\u4ece\u6808\u4e2d\u53d6\u51fa\u5143\u7d20\u8fdb\u884cvisit\u3002","title":"recursion\u7684\u5b9e\u73b0 VS iteration\u7684\u5b9e\u73b0"},{"location":"Recursion/Structural-induction/","text":"Structural induction Structural induction #","title":"Structural-induction"},{"location":"Recursion/Structural-induction/#structural_induction","text":"","title":"Structural induction"},{"location":"Recursion/Tower-of-Hanoi/","text":"Tower of Hanoi Solution Iterative solution Recursive solution Logical analysis of the recursive solution Recursive implementation Tower of Hanoi # The Tower of Hanoi (also called the Tower of Brahma or Lucas' Tower [ 1] and sometimes pluralized as Towers ) is a mathematical game or puzzle . It consists of three rods and a number of disks of different sizes, which can slide onto any rod. The puzzle starts with the disks in a neat stack in ascending order of size on one rod, the smallest at the top, thus making a conical \uff08\u9525\u5f62\uff09 shape. The objective of the puzzle is to move the entire stack to another rod, obeying the following simple rules: Only one disk can be moved at a time. Each move consists of taking the upper disk from one of the stacks and placing it on top of another stack or on an empty rod. No larger disk may be placed on top of a smaller disk. With 3 disks, the puzzle can be solved in 7 moves. The minimal number of moves required to solve a Tower of Hanoi puzzle is $2^n \u2212 1$, where n is the number of disks. Solution # The puzzle can be played with any number of disks, although many toy versions have around 7 to 9 of them. The minimal number of moves required to solve a Tower of Hanoi puzzle is $2^n \u2212 1$, where $n$ is the number of disks.[ 5] This is precisely the n th Mersenne number . SUMMARY : \u6309\u7167\u6211\u7684\u5b9e\u8df5\u6765\u770b\uff0c\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u7684\u786e\u662f\u5b58\u5728\u9012\u5f52\u7684\u8fc7\u7a0b\u7684\uff1a\u6bd4\u5982\u6709A\u3001B\u3001C\u4e09\u4e2arob\uff0c\u8981\u79fb\u52a8A\u4e0a\u7684n\u4e2adisk\u5230B\u4e0a\uff1b\u663e\u7136\u9996\u5148\u9700\u8981\u5c06A\u4e0a\u7684\u524dn-1\u4e2adisk\u79fb\u52a8\u5230C rob\u4e0a\uff1b\u7136\u540e\u5c06A\u4e0a\u7684\u7b2cn\u4e2adisk\u79fb\u52a8\u5230B\u4e0a\uff1b\u7136\u540e\u5c06C\u4e0a\u7684n-1\u4e2adisk\u79fb\u52a8\u5230B\u4e0a\uff1b\u8fd9\u6837\u5c31\u5b8c\u6210\u4e86\u5168\u90e8\u7684\u79fb\u52a8\uff1b\u663e\u7136\u6309\u7167\u8fd9\u4e2a\u63cf\u8ff0\uff0c\u5176\u5b9e\u5c31\u5df2\u7ecf\u80fd\u591f\u5199\u51fa\u9012\u5f52\u516c\u5f0f\u4e86\uff1a $ \\operatorname {hanoi} (n)={\\begin{cases}1&{\\mbox{if }}n=1\\2\\cdot \\operatorname {hanoi} (n-1)+1&{\\mbox{if }}n>1\\\\end{cases}} $ \u4e0a\u5f0f\u4e2d\u76842\u662f\u56e0\u4e3a\u524dn-1\u4e2adisk\u88ab\u79fb\u52a8\u4e24\u6b21\u30021\u662f\u56e0\u4e3a\u9700\u8981\u5c06disk n\u79fb\u52a8\u5230\u76ee\u6807rob\uff1b \u6bd4\u5982n=1\uff0c\u5219\u76f4\u63a5\u79fb\u52a8\u5230B\u4e0a\u5373\u53ef\uff1b \u6bd4\u5982n=2\uff0c\u5219\u5148\u5c06disk 1\u79fb\u52a8\u5230C\uff0c\u7136\u540e\u5c06disk 2\u79fb\u52a8\u5230B\uff0c\u7136\u540e\u5c06disk 1\u79fb\u52a8\u5230B\uff1b \u6bd4\u5982n=3\uff0c\u5219\u5148\u5c06disk 1\u548cdisk 2\u79fb\u52a8\u5230C\uff0c\u7136\u540e\u518d\u5c06disk 3\u79fb\u52a8\u5230B\uff0c\u7136\u540e\u5c06disk 1\u548cdisk 2\u4eceC\u79fb\u52a8\u5230B\uff1b Iterative solution # A simple solution for the toy puzzle is to alternate moves between the smallest piece and a non-smallest piece. When moving the smallest piece, always move it to the next position in the same direction (to the right if the starting number of pieces is even, to the left if the starting number of pieces is odd). If there is no tower position in the chosen direction, move the piece to the opposite end, but then continue to move in the correct direction. For example, if you started with three pieces, you would move the smallest piece to the opposite end, then continue in the left direction after that. When the turn is to move the non-smallest piece, there is only one legal move. Doing this will complete the puzzle in the fewest moves.[ 6] Recursive solution # The key to solving a problem recursively is to recognize that it can be broken down into a collection of smaller sub-problems, to each of which that same general solving procedure that we are seeking applies, and the total solution is then found in some simple way from those sub-problems' solutions. Each of thus created sub-problems being \"smaller\" guarantees that the base case(s) will eventually be reached. Thence, for the Towers of Hanoi: label the pegs A, B, C, let n be the total number of disks, number the disks from 1 (smallest, topmost) to n (largest, bottom-most). Assuming all n disks are distributed in valid arrangements among the pegs; assuming there are m top disks on a source peg, and all the rest of the disks are larger than m , so they can be safely ignored; to move m disks from a source peg to a target peg using a spare peg, without violating the rules: Move m \u2212 1 disks from the source to the spare peg, by the same general solving procedure . Rules are not violated, by assumption. This leaves the disk m as a top disk on the source peg. Move the disk m from the source to the target peg, which is guaranteed to be a valid move, by the assumptions \u2014 a simple step . Move the m \u2212 1 disks that we have just placed on the spare, from the spare to the target peg by the same general solving procedure , so they are placed on top of the disk m without violating the rules. The base case being to move 0 disks (in steps 1 and 3), that is, do nothing \u2013 which obviously doesn't violate the rules. The full Tower of Hanoi solution then consists of moving n disks from the source peg A to the target peg C, using B as the spare peg. This approach can be given a rigorous mathematical proof with mathematical induction and is often used as an example of recursion when teaching programming. Logical analysis of the recursive solution # As in many mathematical puzzles, finding a solution is made easier by solving a slightly more general problem: how to move a tower of h (height) disks from a starting peg f = A (from) onto a destination peg t = C (to), B being the remaining third peg and assuming t \u2260 f . First, observe that the problem is symmetric for permutations of the names of the pegs ( symmetric group S 3 ). If a solution is known moving from peg A to peg C , then, by renaming the pegs, the same solution can be used for every other choice of starting and destination peg. If there is only one disk (or even none at all), the problem is trivial. If h = 1, then simply move the disk from peg A to peg C . If h > 1, then somewhere along the sequence of moves, the largest disk must be moved from peg A to another peg, preferably to peg C . The only situation that allows this move is when all smaller h \u2212 1 disks are on peg B . Hence, first all h \u2212 1 smaller disks must go from A to B . Then move the largest disk and finally move the h \u2212 1 smaller disks from peg B to peg C . The presence of the largest disk does not impede any move of the h \u2212 1 smaller disks and can be temporarily ignored. Now the problem is reduced to moving h \u2212 1 disks from one peg to another one, first from A to B and subsequently from B to C , but the same method can be used both times by renaming the pegs. The same strategy can be used to reduce the h \u2212 1 problem to h \u2212 2, h \u2212 3, and so on until only one disk is left. This is called recursion. This algorithm can be schematized as follows. Identify the disks in order of increasing size by the natural numbers from 0 up to but not including h . Hence disk 0 is the smallest one, and disk h \u2212 1 the largest one. The following is a procedure for moving a tower of h disks from a peg A onto a peg C , with B being the remaining third peg: If h > 1, then first use this procedure to move the h \u2212 1 smaller disks from peg A to peg B . Now the largest disk, i.e. disk h can be moved from peg A to peg C . If h > 1, then again use this procedure to move the h \u2212 1 smaller disks from peg B to peg C . By means of mathematical induction , it is easily proven that the above procedure requires the minimal number of moves possible, and that the produced solution is the only one with this minimal number of moves. Using recurrence relations , the exact number of moves that this solution requires can be calculated by: $ 2^{h}-1 $. This result is obtained by noting that steps 1 and 3 take $ T_{h-1} $ moves, and step 2 takes one move, giving $ T_{h}=2T_{h-1}+1 $. Recursive implementation # The following highlights an essential function of the recursive solution, which may be otherwise misunderstood or overlooked. That is, with every level of recursion, the first recursive call inverts the target and auxiliary stacks, while in the second recursive call the source and auxiliary stacks are inverted. A = [3, 2, 1] B = [] C = [] def move(n, source, target, auxiliary): if n > 0: # move n - 1 disks from source to auxiliary, so they are out of the way move(n - 1, source, auxiliary, target) # move the nth disk from source to target target.append(source.pop()) # Display our progress print(A, B, C, '##############', sep = '\\n') # move the n - 1 disks that we left on auxiliary onto target move(n - 1, auxiliary, target, source) # initiate call from source A to target C with auxiliary B move(3, A, C, B) The following code implements more recursive functions for a text-based animation: import time A = [i for i in range(5, 0, -1)] height = len(A)-1 #stable height value for animation B = [] C = [] def move(n, source, target, auxiliary): if n > 0: # move n - 1 disks from source to auxiliary, so they are out of the way move(n - 1, source, auxiliary, target) # move the nth disk from source to target target.append(source.pop()) # Display our progress using a recursive function to draw it out drawDisks(A, B, C, height) print(\"\") #provide spacing time.sleep(0.3) #pause for a moment to animate # move the n - 1 disks that we left on auxiliary onto target move(n - 1, auxiliary, target, source) def drawDisks(A, B, C, position, width = 2 * int(max(A))): #width parameter defaults to double of the largest sized disk in the initial tower. if position >= 0: #if A has a value in the list at the given position, create a disk at its position (height) valueInA = \" \" if position >= len(A) else createDisk(A[position]) #same for B and C valueInB = \" \" if position >= len(B) else createDisk(B[position]) valueInC = \" \" if position >= len(C) else createDisk(C[position]) #print each row print(\"{0:^{width}}{1:^{width}}{2:^{width}}\".format(valueInA, valueInB, valueInC, width=width)) #recursivly call this method again to the next position (height) drawDisks(A, B, C, position-1, width) else: #when done with recurisve, print column labels print(\"{0:^{width}}{1:^{width}}{2:^{width}}\".format(\"A\", \"B\", \"C\", width=width)) def createDisk(size): '''simple recursive method to create a slanted disk.''' if size==1: return \"/\\\\\" else: return \"/\" + createDisk(size-1) + \"\\\\\" # initiate call from source A to target C with auxiliary B move(len(A), A, B, C)","title":"Tower-of-Hanoi"},{"location":"Recursion/Tower-of-Hanoi/#tower_of_hanoi","text":"The Tower of Hanoi (also called the Tower of Brahma or Lucas' Tower [ 1] and sometimes pluralized as Towers ) is a mathematical game or puzzle . It consists of three rods and a number of disks of different sizes, which can slide onto any rod. The puzzle starts with the disks in a neat stack in ascending order of size on one rod, the smallest at the top, thus making a conical \uff08\u9525\u5f62\uff09 shape. The objective of the puzzle is to move the entire stack to another rod, obeying the following simple rules: Only one disk can be moved at a time. Each move consists of taking the upper disk from one of the stacks and placing it on top of another stack or on an empty rod. No larger disk may be placed on top of a smaller disk. With 3 disks, the puzzle can be solved in 7 moves. The minimal number of moves required to solve a Tower of Hanoi puzzle is $2^n \u2212 1$, where n is the number of disks.","title":"Tower of Hanoi"},{"location":"Recursion/Tower-of-Hanoi/#solution","text":"The puzzle can be played with any number of disks, although many toy versions have around 7 to 9 of them. The minimal number of moves required to solve a Tower of Hanoi puzzle is $2^n \u2212 1$, where $n$ is the number of disks.[ 5] This is precisely the n th Mersenne number . SUMMARY : \u6309\u7167\u6211\u7684\u5b9e\u8df5\u6765\u770b\uff0c\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u7684\u786e\u662f\u5b58\u5728\u9012\u5f52\u7684\u8fc7\u7a0b\u7684\uff1a\u6bd4\u5982\u6709A\u3001B\u3001C\u4e09\u4e2arob\uff0c\u8981\u79fb\u52a8A\u4e0a\u7684n\u4e2adisk\u5230B\u4e0a\uff1b\u663e\u7136\u9996\u5148\u9700\u8981\u5c06A\u4e0a\u7684\u524dn-1\u4e2adisk\u79fb\u52a8\u5230C rob\u4e0a\uff1b\u7136\u540e\u5c06A\u4e0a\u7684\u7b2cn\u4e2adisk\u79fb\u52a8\u5230B\u4e0a\uff1b\u7136\u540e\u5c06C\u4e0a\u7684n-1\u4e2adisk\u79fb\u52a8\u5230B\u4e0a\uff1b\u8fd9\u6837\u5c31\u5b8c\u6210\u4e86\u5168\u90e8\u7684\u79fb\u52a8\uff1b\u663e\u7136\u6309\u7167\u8fd9\u4e2a\u63cf\u8ff0\uff0c\u5176\u5b9e\u5c31\u5df2\u7ecf\u80fd\u591f\u5199\u51fa\u9012\u5f52\u516c\u5f0f\u4e86\uff1a $ \\operatorname {hanoi} (n)={\\begin{cases}1&{\\mbox{if }}n=1\\2\\cdot \\operatorname {hanoi} (n-1)+1&{\\mbox{if }}n>1\\\\end{cases}} $ \u4e0a\u5f0f\u4e2d\u76842\u662f\u56e0\u4e3a\u524dn-1\u4e2adisk\u88ab\u79fb\u52a8\u4e24\u6b21\u30021\u662f\u56e0\u4e3a\u9700\u8981\u5c06disk n\u79fb\u52a8\u5230\u76ee\u6807rob\uff1b \u6bd4\u5982n=1\uff0c\u5219\u76f4\u63a5\u79fb\u52a8\u5230B\u4e0a\u5373\u53ef\uff1b \u6bd4\u5982n=2\uff0c\u5219\u5148\u5c06disk 1\u79fb\u52a8\u5230C\uff0c\u7136\u540e\u5c06disk 2\u79fb\u52a8\u5230B\uff0c\u7136\u540e\u5c06disk 1\u79fb\u52a8\u5230B\uff1b \u6bd4\u5982n=3\uff0c\u5219\u5148\u5c06disk 1\u548cdisk 2\u79fb\u52a8\u5230C\uff0c\u7136\u540e\u518d\u5c06disk 3\u79fb\u52a8\u5230B\uff0c\u7136\u540e\u5c06disk 1\u548cdisk 2\u4eceC\u79fb\u52a8\u5230B\uff1b","title":"Solution"},{"location":"Recursion/Tower-of-Hanoi/#iterative_solution","text":"A simple solution for the toy puzzle is to alternate moves between the smallest piece and a non-smallest piece. When moving the smallest piece, always move it to the next position in the same direction (to the right if the starting number of pieces is even, to the left if the starting number of pieces is odd). If there is no tower position in the chosen direction, move the piece to the opposite end, but then continue to move in the correct direction. For example, if you started with three pieces, you would move the smallest piece to the opposite end, then continue in the left direction after that. When the turn is to move the non-smallest piece, there is only one legal move. Doing this will complete the puzzle in the fewest moves.[ 6]","title":"Iterative solution"},{"location":"Recursion/Tower-of-Hanoi/#recursive_solution","text":"The key to solving a problem recursively is to recognize that it can be broken down into a collection of smaller sub-problems, to each of which that same general solving procedure that we are seeking applies, and the total solution is then found in some simple way from those sub-problems' solutions. Each of thus created sub-problems being \"smaller\" guarantees that the base case(s) will eventually be reached. Thence, for the Towers of Hanoi: label the pegs A, B, C, let n be the total number of disks, number the disks from 1 (smallest, topmost) to n (largest, bottom-most). Assuming all n disks are distributed in valid arrangements among the pegs; assuming there are m top disks on a source peg, and all the rest of the disks are larger than m , so they can be safely ignored; to move m disks from a source peg to a target peg using a spare peg, without violating the rules: Move m \u2212 1 disks from the source to the spare peg, by the same general solving procedure . Rules are not violated, by assumption. This leaves the disk m as a top disk on the source peg. Move the disk m from the source to the target peg, which is guaranteed to be a valid move, by the assumptions \u2014 a simple step . Move the m \u2212 1 disks that we have just placed on the spare, from the spare to the target peg by the same general solving procedure , so they are placed on top of the disk m without violating the rules. The base case being to move 0 disks (in steps 1 and 3), that is, do nothing \u2013 which obviously doesn't violate the rules. The full Tower of Hanoi solution then consists of moving n disks from the source peg A to the target peg C, using B as the spare peg. This approach can be given a rigorous mathematical proof with mathematical induction and is often used as an example of recursion when teaching programming.","title":"Recursive solution"},{"location":"Recursion/Tower-of-Hanoi/#logical_analysis_of_the_recursive_solution","text":"As in many mathematical puzzles, finding a solution is made easier by solving a slightly more general problem: how to move a tower of h (height) disks from a starting peg f = A (from) onto a destination peg t = C (to), B being the remaining third peg and assuming t \u2260 f . First, observe that the problem is symmetric for permutations of the names of the pegs ( symmetric group S 3 ). If a solution is known moving from peg A to peg C , then, by renaming the pegs, the same solution can be used for every other choice of starting and destination peg. If there is only one disk (or even none at all), the problem is trivial. If h = 1, then simply move the disk from peg A to peg C . If h > 1, then somewhere along the sequence of moves, the largest disk must be moved from peg A to another peg, preferably to peg C . The only situation that allows this move is when all smaller h \u2212 1 disks are on peg B . Hence, first all h \u2212 1 smaller disks must go from A to B . Then move the largest disk and finally move the h \u2212 1 smaller disks from peg B to peg C . The presence of the largest disk does not impede any move of the h \u2212 1 smaller disks and can be temporarily ignored. Now the problem is reduced to moving h \u2212 1 disks from one peg to another one, first from A to B and subsequently from B to C , but the same method can be used both times by renaming the pegs. The same strategy can be used to reduce the h \u2212 1 problem to h \u2212 2, h \u2212 3, and so on until only one disk is left. This is called recursion. This algorithm can be schematized as follows. Identify the disks in order of increasing size by the natural numbers from 0 up to but not including h . Hence disk 0 is the smallest one, and disk h \u2212 1 the largest one. The following is a procedure for moving a tower of h disks from a peg A onto a peg C , with B being the remaining third peg: If h > 1, then first use this procedure to move the h \u2212 1 smaller disks from peg A to peg B . Now the largest disk, i.e. disk h can be moved from peg A to peg C . If h > 1, then again use this procedure to move the h \u2212 1 smaller disks from peg B to peg C . By means of mathematical induction , it is easily proven that the above procedure requires the minimal number of moves possible, and that the produced solution is the only one with this minimal number of moves. Using recurrence relations , the exact number of moves that this solution requires can be calculated by: $ 2^{h}-1 $. This result is obtained by noting that steps 1 and 3 take $ T_{h-1} $ moves, and step 2 takes one move, giving $ T_{h}=2T_{h-1}+1 $.","title":"Logical analysis of the recursive solution"},{"location":"Recursion/Tower-of-Hanoi/#recursive_implementation","text":"The following highlights an essential function of the recursive solution, which may be otherwise misunderstood or overlooked. That is, with every level of recursion, the first recursive call inverts the target and auxiliary stacks, while in the second recursive call the source and auxiliary stacks are inverted. A = [3, 2, 1] B = [] C = [] def move(n, source, target, auxiliary): if n > 0: # move n - 1 disks from source to auxiliary, so they are out of the way move(n - 1, source, auxiliary, target) # move the nth disk from source to target target.append(source.pop()) # Display our progress print(A, B, C, '##############', sep = '\\n') # move the n - 1 disks that we left on auxiliary onto target move(n - 1, auxiliary, target, source) # initiate call from source A to target C with auxiliary B move(3, A, C, B) The following code implements more recursive functions for a text-based animation: import time A = [i for i in range(5, 0, -1)] height = len(A)-1 #stable height value for animation B = [] C = [] def move(n, source, target, auxiliary): if n > 0: # move n - 1 disks from source to auxiliary, so they are out of the way move(n - 1, source, auxiliary, target) # move the nth disk from source to target target.append(source.pop()) # Display our progress using a recursive function to draw it out drawDisks(A, B, C, height) print(\"\") #provide spacing time.sleep(0.3) #pause for a moment to animate # move the n - 1 disks that we left on auxiliary onto target move(n - 1, auxiliary, target, source) def drawDisks(A, B, C, position, width = 2 * int(max(A))): #width parameter defaults to double of the largest sized disk in the initial tower. if position >= 0: #if A has a value in the list at the given position, create a disk at its position (height) valueInA = \" \" if position >= len(A) else createDisk(A[position]) #same for B and C valueInB = \" \" if position >= len(B) else createDisk(B[position]) valueInC = \" \" if position >= len(C) else createDisk(C[position]) #print each row print(\"{0:^{width}}{1:^{width}}{2:^{width}}\".format(valueInA, valueInB, valueInC, width=width)) #recursivly call this method again to the next position (height) drawDisks(A, B, C, position-1, width) else: #when done with recurisve, print column labels print(\"{0:^{width}}{1:^{width}}{2:^{width}}\".format(\"A\", \"B\", \"C\", width=width)) def createDisk(size): '''simple recursive method to create a slanted disk.''' if size==1: return \"/\\\\\" else: return \"/\" + createDisk(size-1) + \"\\\\\" # initiate call from source A to target C with auxiliary B move(len(A), A, B, C)","title":"Recursive implementation"},{"location":"Recursion/VS-recursion-VS-corecursion/","text":"recursion \u548c corecursion \u7684\u8ba1\u7b97\u65b9\u5411\u662f\u76f8\u53cd\uff1a\u5bf9\u4e8e\u4e00\u4e2a recurrence relations \uff0c\u5982 n! := n \u00d7 (n - 1)! .\uff0crecursion\u662f\u4ece\u5de6\u81f3\u53f3\uff0c\u4f46\u662fcorecursion\u662f\u4ece\u53f3\u81f3\u5de6\uff0c\u4f46\u662f\u80fd\u591f\u6b8a\u9014\u540c\u5f52 recursion works analytically VS corecursion works synthetically recursion top-down VS corecursion bottom-up recursion reduce VS corecursion produce \u5728 Tree traversal \u4e2d\u6709\u5982\u4e0b\u63cf\u8ff0\uff1a Depth-first search is easily implemented via a stack , including recursively (via the call stack ), while breadth-first search is easily implemented via a queue , including corecursively .","title":"VS-recursion-VS-corecursion"},{"location":"Recursion/VS-structural-recursion-VS-generative-recursion/","text":"Structural recursion versus generative recursion How does structural recursion differ from generative recursion? A Structural recursion versus generative recursion # \u5728 Recursion (computer science) \u4e2d\u6709\u4e13\u95e8\u7684\u7ae0\u8282\u6765\u63cf\u8ff0Structural versus generative recursion\u3002 How does structural recursion differ from generative recursion? # The description of generative recursion in Wikipedia is clear to me, but I'm confused about the concept of structural recursion. Can someone explain if a function calculating nth Fibonacci number and a function calculating factorial from 1 to N will be structural or generative? COMMENTS My two pennies: Fib is generative recursive using that definition because the data is \"generated\" as it goes along. Whereas, according to the article, structural recursion is about traversing an [existing] graph. The article goes on to state that an crucial distinction is that structural recursion can be proven to terminate through structural induction .. \u2013 user166390 A # The key difference between structural and generative recursion is where a recursive procedure gets the data that it works on and how it processes that data\uff08 recursive procedure gets the data that it works on\u5176\u5b9e\u5c31\u662frecursion function\u7684 \u5165\u53c2 \uff09. Specifically, for structural recursion , a recursive call is made on a subset of the original input data. Whereas for generative recursion , a recursive call is made on data that was constructed/calculated from the original input data. SUMMARY : \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u603b\u7ed3\u5730\u662f\u975e\u5e38\u597d\u7684\uff1b For example, if you wanted to count the number of elements in a linked list, you could do the following: int NumberOfNodes(ListNode* node) { if (node == nullptr) return 0; return 1 + NumberOfNodes(node->next); } Here, the recursive call to NumberOfNodes is being made on node->next , which is a piece of the original input which already existed. In this case, the recursion works by breaking down the input into smaller pieces, then recursing on the smaller pieces. Similarly, this code to search a BST for a value would be structural recursion, because the recursive calls are to subparts of the original input: TreeNode* Find(TreeNode* root, DataType value) { if (root == nullptr) return nullptr; if (value < root->value) return Find(root->left, value); else return Find(root->right, value); The term \"structural recursion\" comes from the fact that these structures (lists, BSTs, etc.) can be defined recursively: A list is either nothing, or a cell followed by a list. A binary tree is either nothing, or a node with two binary trees as children. When doing structural recursion, you are \"undoing\" the operation from which these structures are built out of one another. For example, the NumberOfNodes function \"undoes\" the construction of taking a node and prepending it to an existing list. The Find operator \"undoes\" the operation of gluing a node to two other trees. Therefore, it's easy to see why these functions have to terminate - eventually, you \"undo\" all of the operations that went in to building up the object in the first place, and the recursion stops. On the other hand, consider Quicksort , which does the following: Pick a pivot. Create three new lists: one of all elements less than the pivot, one of all elements greater than the pivot, and one of all elements equal to the pivot. Recursively sort the first and second of these lists. Concatenate the list of smaller, equal, and larger values. Here, the recursive calls are being made on smaller arrays that weren't part of the original input - the lists had to be created from the data. (Typically, an implementation would reuse space for these lists, but those sublists weren't guaranteed to exist directly within the input). This distinction is blurry\uff08\u6a21\u7cca\u7684\uff09 when it comes to natural numbers . Usually, natural numbers are recursively defined as follows: 0 is a natural number. If n is a natural number, n + 1 is a natural number. Nothing else is a natural number. Under this definition, the number n is a \"part\" of n + 1. Therefore, this recursive code to compute n! is structural recursion: int Factorial(int n) { if (n == 0) return 1; return n * Factorial(n - 1); } This is structural recursion, because the argument n - 1 was a \"part\" of the original input n. Similarly, by this definition, computing the nth Fibonacci number recursively counts as structural recursion: int Fibonacci(int n) { if (n <= 1) return n; return Fibonacci(n - 1) + Fibonacci(n - 2); } This is considered structural recursion because n - 1 is a part of n (formed by \"undoing\" the +1) and n - 2 is a part of n - 1 (again formed by \"undoing\" the +1). On the other hand, this code to compute gcd would be considered generative recursion, rather than structural recursion: int gcd(int a, int b) { if (b == 0) return a; return gcd(b, a % b); } The reasoning is that since a % b is \"computed\" from a and b , rather than formed by \"undoing\" some number of +1 operations, the data is generated. The reason that generative recursion is different from structural recursion is that there's no guarantee that it terminates. For example, think about this function: int BadTimes(int a, int b) { if (a == 0 && b == 0) return 0; return BadTimes(a * 2, b - 1); } This generative recursive function never terminates: a keeps getting bigger even though b keeps getting smaller. Honestly, I've never heard of this distinction before and I teach courses in discrete math and programming. I wouldn't worry too much about it unless someone is requiring you to know the difference. Hope this helps!","title":"VS-structural-recursion-VS-generative-recursion"},{"location":"Recursion/VS-structural-recursion-VS-generative-recursion/#structural_recursion_versus_generative_recursion","text":"\u5728 Recursion (computer science) \u4e2d\u6709\u4e13\u95e8\u7684\u7ae0\u8282\u6765\u63cf\u8ff0Structural versus generative recursion\u3002","title":"Structural recursion versus generative recursion"},{"location":"Recursion/VS-structural-recursion-VS-generative-recursion/#how_does_structural_recursion_differ_from_generative_recursion","text":"The description of generative recursion in Wikipedia is clear to me, but I'm confused about the concept of structural recursion. Can someone explain if a function calculating nth Fibonacci number and a function calculating factorial from 1 to N will be structural or generative? COMMENTS My two pennies: Fib is generative recursive using that definition because the data is \"generated\" as it goes along. Whereas, according to the article, structural recursion is about traversing an [existing] graph. The article goes on to state that an crucial distinction is that structural recursion can be proven to terminate through structural induction .. \u2013 user166390","title":"How does structural recursion differ from generative recursion?"},{"location":"Recursion/VS-structural-recursion-VS-generative-recursion/#a","text":"The key difference between structural and generative recursion is where a recursive procedure gets the data that it works on and how it processes that data\uff08 recursive procedure gets the data that it works on\u5176\u5b9e\u5c31\u662frecursion function\u7684 \u5165\u53c2 \uff09. Specifically, for structural recursion , a recursive call is made on a subset of the original input data. Whereas for generative recursion , a recursive call is made on data that was constructed/calculated from the original input data. SUMMARY : \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u603b\u7ed3\u5730\u662f\u975e\u5e38\u597d\u7684\uff1b For example, if you wanted to count the number of elements in a linked list, you could do the following: int NumberOfNodes(ListNode* node) { if (node == nullptr) return 0; return 1 + NumberOfNodes(node->next); } Here, the recursive call to NumberOfNodes is being made on node->next , which is a piece of the original input which already existed. In this case, the recursion works by breaking down the input into smaller pieces, then recursing on the smaller pieces. Similarly, this code to search a BST for a value would be structural recursion, because the recursive calls are to subparts of the original input: TreeNode* Find(TreeNode* root, DataType value) { if (root == nullptr) return nullptr; if (value < root->value) return Find(root->left, value); else return Find(root->right, value); The term \"structural recursion\" comes from the fact that these structures (lists, BSTs, etc.) can be defined recursively: A list is either nothing, or a cell followed by a list. A binary tree is either nothing, or a node with two binary trees as children. When doing structural recursion, you are \"undoing\" the operation from which these structures are built out of one another. For example, the NumberOfNodes function \"undoes\" the construction of taking a node and prepending it to an existing list. The Find operator \"undoes\" the operation of gluing a node to two other trees. Therefore, it's easy to see why these functions have to terminate - eventually, you \"undo\" all of the operations that went in to building up the object in the first place, and the recursion stops. On the other hand, consider Quicksort , which does the following: Pick a pivot. Create three new lists: one of all elements less than the pivot, one of all elements greater than the pivot, and one of all elements equal to the pivot. Recursively sort the first and second of these lists. Concatenate the list of smaller, equal, and larger values. Here, the recursive calls are being made on smaller arrays that weren't part of the original input - the lists had to be created from the data. (Typically, an implementation would reuse space for these lists, but those sublists weren't guaranteed to exist directly within the input). This distinction is blurry\uff08\u6a21\u7cca\u7684\uff09 when it comes to natural numbers . Usually, natural numbers are recursively defined as follows: 0 is a natural number. If n is a natural number, n + 1 is a natural number. Nothing else is a natural number. Under this definition, the number n is a \"part\" of n + 1. Therefore, this recursive code to compute n! is structural recursion: int Factorial(int n) { if (n == 0) return 1; return n * Factorial(n - 1); } This is structural recursion, because the argument n - 1 was a \"part\" of the original input n. Similarly, by this definition, computing the nth Fibonacci number recursively counts as structural recursion: int Fibonacci(int n) { if (n <= 1) return n; return Fibonacci(n - 1) + Fibonacci(n - 2); } This is considered structural recursion because n - 1 is a part of n (formed by \"undoing\" the +1) and n - 2 is a part of n - 1 (again formed by \"undoing\" the +1). On the other hand, this code to compute gcd would be considered generative recursion, rather than structural recursion: int gcd(int a, int b) { if (b == 0) return a; return gcd(b, a % b); } The reasoning is that since a % b is \"computed\" from a and b , rather than formed by \"undoing\" some number of +1 operations, the data is generated. The reason that generative recursion is different from structural recursion is that there's no guarantee that it terminates. For example, think about this function: int BadTimes(int a, int b) { if (a == 0 && b == 0) return 0; return BadTimes(a * 2, b - 1); } This generative recursive function never terminates: a keeps getting bigger even though b keeps getting smaller. Honestly, I've never heard of this distinction before and I teach courses in discrete math and programming. I wouldn't worry too much about it unless someone is requiring you to know the difference. Hope this helps!","title":"A"}]}